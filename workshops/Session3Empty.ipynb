{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "\n",
    "- **3.0. Data Preparation**\n",
    "  - 3.0.1. *Vocabulary*\n",
    "  - 3.0.2. *Dataset*\n",
    "<br><br>  \n",
    "\n",
    "- **3.1. Word2Vec from Scratch**\n",
    "  - 3.1.1. *CBOW*\n",
    "  - 3.1.2. *Skipgram*\n",
    "  - 3.1.3. *Word2Vec Dataset*\n",
    "  - 3.1.4. *Train a CBOW model*\n",
    "  - 3.1.5. *Train a Skipgram model*\n",
    "  - 3.1.6. *Loading Pre-trained Embeddings*\n",
    "  \n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /home/kenny/anaconda3/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: torch in /home/kenny/anaconda3/lib/python3.6/site-packages (1.0.0)\n",
      "Requirement already satisfied: tqdm in /home/kenny/anaconda3/lib/python3.6/site-packages (4.29.1)\n",
      "Requirement already satisfied: nltk in /home/kenny/anaconda3/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: lazyme in /home/kenny/anaconda3/lib/python3.6/site-packages (0.0.22)\n",
      "Requirement already satisfied: requests in /home/kenny/anaconda3/lib/python3.6/site-packages (2.18.4)\n",
      "Collecting gensim\n",
      "  Using cached https://files.pythonhosted.org/packages/a0/8e/719275cb952330a14752655141ceda5f6ec4c065eb3f54993943f89faa4e/gensim-3.7.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: scikit-learn in /home/kenny/anaconda3/lib/python3.6/site-packages (from sklearn) (0.19.1)\n",
      "Requirement already satisfied: six in /home/kenny/anaconda3/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/kenny/anaconda3/lib/python3.6/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/kenny/anaconda3/lib/python3.6/site-packages (from requests) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/kenny/anaconda3/lib/python3.6/site-packages (from requests) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kenny/anaconda3/lib/python3.6/site-packages (from requests) (2018.10.15)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/kenny/anaconda3/lib/python3.6/site-packages (from gensim) (1.14.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/kenny/anaconda3/lib/python3.6/site-packages (from gensim) (1.0.0)\n",
      "Collecting smart-open>=1.7.0 (from gensim)\n",
      "Collecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/8f/35/6da51b44a808b5caf05957e9b38f4c4827302120cd1cde2ca3fd712af412/boto3-1.9.83-py2.py3-none-any.whl\n",
      "Requirement already satisfied: boto>=2.32 in /home/kenny/anaconda3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.48.0)\n",
      "Collecting bz2file (from smart-open>=1.7.0->gensim)\n",
      "Collecting botocore<1.13.0,>=1.12.83 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/f4/745026b1f20d687b14b5ad26b3087121596a81ae055ecb28b6187273978b/botocore-1.12.83-py2.py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl\n",
      "Requirement already satisfied: docutils>=0.10 in /home/kenny/anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.83->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/kenny/anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.83->boto3->smart-open>=1.7.0->gensim) (2.6.1)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, bz2file, smart-open, gensim\n",
      "Successfully installed boto3-1.9.83 botocore-1.12.83 bz2file-0.98 gensim-3.7.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.8.0\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "/home/kenny/anaconda3/envs/pytorch/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/kenny/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sklearn torch tqdm nltk lazyme requests gensim\n",
    "!python -m nltk.downloader movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0. Data Preparation\n",
    "\n",
    "Before we train our own embeddings, lets first understand how to read text data into pytorch.\n",
    "The native pytorch way to load datasets is to use the `torch.utils.data.Dataset` object.\n",
    "\n",
    "There are already several other libraries that help with loading text datasets, e.g. \n",
    "\n",
    " - FastAI https://docs.fast.ai/text.data.html\n",
    " - AllenNLP https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset.html\n",
    " - Torch Text https://github.com/pytorch/text#data\n",
    " - Texar https://texar.readthedocs.io/en/latest/code/data.html#id4 \n",
    " - SpaCy https://github.com/explosion/thinc\n",
    " \n",
    "\n",
    "But to truly understand and use it for the custom datasets you'll see at work, lets learn it the native way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0.1  Vocabulary\n",
    "\n",
    "Given a text, the first thing to do is to build a vocabulary (i.e. a dictionary of unique words) and assign an index to each unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "text = \"\"\"Language users never choose words randomly, and language is essentially\n",
    "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
    "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
    "the null hypothesis will never be true. Moreover, where there is enough\n",
    "data, we shall (almost) always be able to establish that it is not true. In\n",
    "corpus studies, we frequently do have enough data, so the fact that a relation \n",
    "between two phenomena is demonstrably non-random, does not support the inference \n",
    "that it is not arbitrary. We present experimental evidence\n",
    "of how arbitrary associations between word frequencies and corpora are\n",
    "systematically non-random. We review literature in which hypothesis testing \n",
    "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
    "\n",
    "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "\n",
    "uniq_tokens = set(chain(*tokenized_text))\n",
    "\n",
    "vocab = {}   # Assign indices to every word.\n",
    "idx2tok = {} # Also keep an dict of index to words.\n",
    "for i, token in enumerate(uniq_tokens):\n",
    "    vocab[token] = i\n",
    "    idx2tok[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['language',\n",
       "  'users',\n",
       "  'never',\n",
       "  'choose',\n",
       "  'words',\n",
       "  'randomly',\n",
       "  ',',\n",
       "  'and',\n",
       "  'language',\n",
       "  'is',\n",
       "  'essentially',\n",
       "  'non-random',\n",
       "  '.'],\n",
       " ['statistical',\n",
       "  'hypothesis',\n",
       "  'testing',\n",
       "  'uses',\n",
       "  'a',\n",
       "  'null',\n",
       "  'hypothesis',\n",
       "  ',',\n",
       "  'which',\n",
       "  'posits',\n",
       "  'randomness',\n",
       "  '.'],\n",
       " ['hence',\n",
       "  ',',\n",
       "  'when',\n",
       "  'we',\n",
       "  'look',\n",
       "  'at',\n",
       "  'linguistic',\n",
       "  'phenomena',\n",
       "  'in',\n",
       "  'corpora',\n",
       "  ',',\n",
       "  'the',\n",
       "  'null',\n",
       "  'hypothesis',\n",
       "  'will',\n",
       "  'never',\n",
       "  'be',\n",
       "  'true',\n",
       "  '.'],\n",
       " ['moreover',\n",
       "  ',',\n",
       "  'where',\n",
       "  'there',\n",
       "  'is',\n",
       "  'enough',\n",
       "  'data',\n",
       "  ',',\n",
       "  'we',\n",
       "  'shall',\n",
       "  '(',\n",
       "  'almost',\n",
       "  ')',\n",
       "  'always',\n",
       "  'be',\n",
       "  'able',\n",
       "  'to',\n",
       "  'establish',\n",
       "  'that',\n",
       "  'it',\n",
       "  'is',\n",
       "  'not',\n",
       "  'true',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'corpus',\n",
       "  'studies',\n",
       "  ',',\n",
       "  'we',\n",
       "  'frequently',\n",
       "  'do',\n",
       "  'have',\n",
       "  'enough',\n",
       "  'data',\n",
       "  ',',\n",
       "  'so',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'a',\n",
       "  'relation',\n",
       "  'between',\n",
       "  'two',\n",
       "  'phenomena',\n",
       "  'is',\n",
       "  'demonstrably',\n",
       "  'non-random',\n",
       "  ',',\n",
       "  'does',\n",
       "  'not',\n",
       "  'support',\n",
       "  'the',\n",
       "  'inference',\n",
       "  'that',\n",
       "  'it',\n",
       "  'is',\n",
       "  'not',\n",
       "  'arbitrary',\n",
       "  '.'],\n",
       " ['we',\n",
       "  'present',\n",
       "  'experimental',\n",
       "  'evidence',\n",
       "  'of',\n",
       "  'how',\n",
       "  'arbitrary',\n",
       "  'associations',\n",
       "  'between',\n",
       "  'word',\n",
       "  'frequencies',\n",
       "  'and',\n",
       "  'corpora',\n",
       "  'are',\n",
       "  'systematically',\n",
       "  'non-random',\n",
       "  '.'],\n",
       " ['we',\n",
       "  'review',\n",
       "  'literature',\n",
       "  'in',\n",
       "  'which',\n",
       "  'hypothesis',\n",
       "  'testing',\n",
       "  'has',\n",
       "  'been',\n",
       "  'used',\n",
       "  ',',\n",
       "  'and',\n",
       "  'show',\n",
       "  'how',\n",
       "  'it',\n",
       "  'has',\n",
       "  'often',\n",
       "  'led',\n",
       "  'to',\n",
       "  'unhelpful',\n",
       "  'or',\n",
       "  'misleading',\n",
       "  'results',\n",
       "  '.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the index of the word 'corpora'\n",
    "vocab['corpora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57, 60, 33, 17, 65, 79, 63, 21, 57, 1, 84, 13, 9]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The indexed representation of the first sentence.\n",
    "\n",
    "sent0 = tokenized_text[0]\n",
    "\n",
    "[vocab[token] for token in sent0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pet Peeve\n",
    "\n",
    "I (Liling) don't really like to write my own vectorizer the `gensim` has functions that are optimized for such operations. In fact, I've written a [whole preprocessing pipeline library for me to use for language modelling and machine translation purposes](https://github.com/alvations/komorebi/blob/master/komorebi/text.py) =)\n",
    "\n",
    "Using `gensim`, I would have written the above as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "vocab = Dictionary(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ',',\n",
       " 1: '.',\n",
       " 2: 'and',\n",
       " 3: 'choose',\n",
       " 4: 'essentially',\n",
       " 5: 'is',\n",
       " 6: 'language',\n",
       " 7: 'never',\n",
       " 8: 'non-random',\n",
       " 9: 'randomly',\n",
       " 10: 'users',\n",
       " 11: 'words',\n",
       " 12: 'a',\n",
       " 13: 'hypothesis',\n",
       " 14: 'null',\n",
       " 15: 'posits',\n",
       " 16: 'randomness',\n",
       " 17: 'statistical',\n",
       " 18: 'testing',\n",
       " 19: 'uses',\n",
       " 20: 'which',\n",
       " 21: 'at',\n",
       " 22: 'be',\n",
       " 23: 'corpora',\n",
       " 24: 'hence',\n",
       " 25: 'in',\n",
       " 26: 'linguistic',\n",
       " 27: 'look',\n",
       " 28: 'phenomena',\n",
       " 29: 'the',\n",
       " 30: 'true',\n",
       " 31: 'we',\n",
       " 32: 'when',\n",
       " 33: 'will',\n",
       " 34: '(',\n",
       " 35: ')',\n",
       " 36: 'able',\n",
       " 37: 'almost',\n",
       " 38: 'always',\n",
       " 39: 'data',\n",
       " 40: 'enough',\n",
       " 41: 'establish',\n",
       " 42: 'it',\n",
       " 43: 'moreover',\n",
       " 44: 'not',\n",
       " 45: 'shall',\n",
       " 46: 'that',\n",
       " 47: 'there',\n",
       " 48: 'to',\n",
       " 49: 'where',\n",
       " 50: 'arbitrary',\n",
       " 51: 'between',\n",
       " 52: 'corpus',\n",
       " 53: 'demonstrably',\n",
       " 54: 'do',\n",
       " 55: 'does',\n",
       " 56: 'fact',\n",
       " 57: 'frequently',\n",
       " 58: 'have',\n",
       " 59: 'inference',\n",
       " 60: 'relation',\n",
       " 61: 'so',\n",
       " 62: 'studies',\n",
       " 63: 'support',\n",
       " 64: 'two',\n",
       " 65: 'are',\n",
       " 66: 'associations',\n",
       " 67: 'evidence',\n",
       " 68: 'experimental',\n",
       " 69: 'frequencies',\n",
       " 70: 'how',\n",
       " 71: 'of',\n",
       " 72: 'present',\n",
       " 73: 'systematically',\n",
       " 74: 'word',\n",
       " 75: 'been',\n",
       " 76: 'has',\n",
       " 77: 'led',\n",
       " 78: 'literature',\n",
       " 79: 'misleading',\n",
       " 80: 'often',\n",
       " 81: 'or',\n",
       " 82: 'results',\n",
       " 83: 'review',\n",
       " 84: 'show',\n",
       " 85: 'unhelpful',\n",
       " 86: 'used'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the key-value order is different of gensim from the native Python's\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token2id['corpora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.doc2idx(sent0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"indexed form\" of the tokens in the sentence forms the ***vectorized*** input to the `nn.Embedding` layer in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0.2 Dataset\n",
    "\n",
    "Lets try creating a `torch.utils.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Text(Dataset):\n",
    "    def __init__(self, tokenized_texts):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self.vocab = Dictionary(tokenized_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        return self.vectorize(self.sents[index])\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return {'x': self.vocab.doc2idx(tokens)}\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_dataset = Text(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': [6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset[0] # First sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return `dict` in `__getitem__()`\n",
    "\n",
    "This is nice if we're just representing sentences/documents by their indices but when we're doing machine learning, we usually have `X` and `Y`. \n",
    "\n",
    "If we have labels for the each sentence, we can also put it into to `__getitem__()` by having it return a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LabeledText(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self.labels = labels # Sentence level labels.\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "\n",
    "Python\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        return {'X': self.vectorize(self.sents[index]), 'Y': self.labels[index]}\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try the `LabeledDataset` on a movie review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:17<00:00, 114.56it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "for fileid in tqdm(movie_reviews.fileids()):\n",
    "    label = fileid.split('/')[0]\n",
    "    doc = word_tokenize(movie_reviews.open(fileid).read())\n",
    "    documents.append(doc)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " ':',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'church',\n",
       " 'party',\n",
       " ',',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'then',\n",
       " 'drive',\n",
       " '.',\n",
       " 'they',\n",
       " 'get',\n",
       " 'into',\n",
       " 'an',\n",
       " 'accident',\n",
       " '.',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'guys',\n",
       " 'dies',\n",
       " ',',\n",
       " 'but',\n",
       " 'his',\n",
       " 'girlfriend',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'see',\n",
       " 'him',\n",
       " 'in',\n",
       " 'her',\n",
       " 'life',\n",
       " ',',\n",
       " 'and',\n",
       " 'has',\n",
       " 'nightmares',\n",
       " '.',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'deal',\n",
       " '?',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'and',\n",
       " '``',\n",
       " 'sorta',\n",
       " '``',\n",
       " 'find',\n",
       " 'out',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'critique',\n",
       " ':',\n",
       " 'a',\n",
       " 'mind-fuck',\n",
       " 'movie',\n",
       " 'for',\n",
       " 'the',\n",
       " 'teen',\n",
       " 'generation',\n",
       " 'that',\n",
       " 'touches',\n",
       " 'on',\n",
       " 'a',\n",
       " 'very',\n",
       " 'cool',\n",
       " 'idea',\n",
       " ',',\n",
       " 'but',\n",
       " 'presents',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'bad',\n",
       " 'package',\n",
       " '.',\n",
       " 'which',\n",
       " 'is',\n",
       " 'what',\n",
       " 'makes',\n",
       " 'this',\n",
       " 'review',\n",
       " 'an',\n",
       " 'even',\n",
       " 'harder',\n",
       " 'one',\n",
       " 'to',\n",
       " 'write',\n",
       " ',',\n",
       " 'since',\n",
       " 'i',\n",
       " 'generally',\n",
       " 'applaud',\n",
       " 'films',\n",
       " 'which',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'break',\n",
       " 'the',\n",
       " 'mold',\n",
       " ',',\n",
       " 'mess',\n",
       " 'with',\n",
       " 'your',\n",
       " 'head',\n",
       " 'and',\n",
       " 'such',\n",
       " '(',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '&',\n",
       " 'memento',\n",
       " ')',\n",
       " ',',\n",
       " 'but',\n",
       " 'there',\n",
       " 'are',\n",
       " 'good',\n",
       " 'and',\n",
       " 'bad',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'making',\n",
       " 'all',\n",
       " 'types',\n",
       " 'of',\n",
       " 'films',\n",
       " ',',\n",
       " 'and',\n",
       " 'these',\n",
       " 'folks',\n",
       " 'just',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'snag',\n",
       " 'this',\n",
       " 'one',\n",
       " 'correctly',\n",
       " '.',\n",
       " 'they',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'this',\n",
       " 'pretty',\n",
       " 'neat',\n",
       " 'concept',\n",
       " ',',\n",
       " 'but',\n",
       " 'executed',\n",
       " 'it',\n",
       " 'terribly',\n",
       " '.',\n",
       " 'so',\n",
       " 'what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'problems',\n",
       " 'with',\n",
       " 'the',\n",
       " 'movie',\n",
       " '?',\n",
       " 'well',\n",
       " ',',\n",
       " 'its',\n",
       " 'main',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'simply',\n",
       " 'too',\n",
       " 'jumbled',\n",
       " '.',\n",
       " 'it',\n",
       " 'starts',\n",
       " 'off',\n",
       " '``',\n",
       " 'normal',\n",
       " '``',\n",
       " 'but',\n",
       " 'then',\n",
       " 'downshifts',\n",
       " 'into',\n",
       " 'this',\n",
       " '``',\n",
       " 'fantasy',\n",
       " '``',\n",
       " 'world',\n",
       " 'in',\n",
       " 'which',\n",
       " 'you',\n",
       " ',',\n",
       " 'as',\n",
       " 'an',\n",
       " 'audience',\n",
       " 'member',\n",
       " ',',\n",
       " 'have',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'going',\n",
       " 'on',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'dreams',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'characters',\n",
       " 'coming',\n",
       " 'back',\n",
       " 'from',\n",
       " 'the',\n",
       " 'dead',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'others',\n",
       " 'who',\n",
       " 'look',\n",
       " 'like',\n",
       " 'the',\n",
       " 'dead',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'strange',\n",
       " 'apparitions',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'disappearances',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'a',\n",
       " 'looooot',\n",
       " 'of',\n",
       " 'chase',\n",
       " 'scenes',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tons',\n",
       " 'of',\n",
       " 'weird',\n",
       " 'things',\n",
       " 'that',\n",
       " 'happen',\n",
       " ',',\n",
       " 'and',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'not',\n",
       " 'explained',\n",
       " '.',\n",
       " 'now',\n",
       " 'i',\n",
       " 'personally',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'mind',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'unravel',\n",
       " 'a',\n",
       " 'film',\n",
       " 'every',\n",
       " 'now',\n",
       " 'and',\n",
       " 'then',\n",
       " ',',\n",
       " 'but',\n",
       " 'when',\n",
       " 'all',\n",
       " 'it',\n",
       " 'does',\n",
       " 'is',\n",
       " 'give',\n",
       " 'me',\n",
       " 'the',\n",
       " 'same',\n",
       " 'clue',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " ',',\n",
       " 'i',\n",
       " 'get',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'fed',\n",
       " 'up',\n",
       " 'after',\n",
       " 'a',\n",
       " 'while',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'this',\n",
       " 'film',\n",
       " \"'s\",\n",
       " 'biggest',\n",
       " 'problem',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'obviously',\n",
       " 'got',\n",
       " 'this',\n",
       " 'big',\n",
       " 'secret',\n",
       " 'to',\n",
       " 'hide',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'want',\n",
       " 'to',\n",
       " 'hide',\n",
       " 'it',\n",
       " 'completely',\n",
       " 'until',\n",
       " 'its',\n",
       " 'final',\n",
       " 'five',\n",
       " 'minutes',\n",
       " '.',\n",
       " 'and',\n",
       " 'do',\n",
       " 'they',\n",
       " 'make',\n",
       " 'things',\n",
       " 'entertaining',\n",
       " ',',\n",
       " 'thrilling',\n",
       " 'or',\n",
       " 'even',\n",
       " 'engaging',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'meantime',\n",
       " '?',\n",
       " 'not',\n",
       " 'really',\n",
       " '.',\n",
       " 'the',\n",
       " 'sad',\n",
       " 'part',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'arrow',\n",
       " 'and',\n",
       " 'i',\n",
       " 'both',\n",
       " 'dig',\n",
       " 'on',\n",
       " 'flicks',\n",
       " 'like',\n",
       " 'this',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " 'actually',\n",
       " 'figured',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'out',\n",
       " 'by',\n",
       " 'the',\n",
       " 'half-way',\n",
       " 'point',\n",
       " ',',\n",
       " 'so',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'after',\n",
       " 'that',\n",
       " 'did',\n",
       " 'start',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'sense',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'still',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'the',\n",
       " 'make',\n",
       " 'the',\n",
       " 'film',\n",
       " 'all',\n",
       " 'that',\n",
       " 'more',\n",
       " 'entertaining',\n",
       " '.',\n",
       " 'i',\n",
       " 'guess',\n",
       " 'the',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'with',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'this',\n",
       " 'is',\n",
       " 'that',\n",
       " 'you',\n",
       " 'should',\n",
       " 'always',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'the',\n",
       " 'audience',\n",
       " 'is',\n",
       " '``',\n",
       " 'into',\n",
       " 'it',\n",
       " '``',\n",
       " 'even',\n",
       " 'before',\n",
       " 'they',\n",
       " 'are',\n",
       " 'given',\n",
       " 'the',\n",
       " 'secret',\n",
       " 'password',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'your',\n",
       " 'world',\n",
       " 'of',\n",
       " 'understanding',\n",
       " '.',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'showing',\n",
       " 'melissa',\n",
       " 'sagemiller',\n",
       " 'running',\n",
       " 'away',\n",
       " 'from',\n",
       " 'visions',\n",
       " 'for',\n",
       " 'about',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'just',\n",
       " 'plain',\n",
       " 'lazy',\n",
       " '!',\n",
       " '!',\n",
       " 'okay',\n",
       " ',',\n",
       " 'we',\n",
       " 'get',\n",
       " 'it',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'people',\n",
       " 'chasing',\n",
       " 'her',\n",
       " 'and',\n",
       " 'we',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'who',\n",
       " 'they',\n",
       " 'are',\n",
       " '.',\n",
       " 'do',\n",
       " 'we',\n",
       " 'really',\n",
       " 'need',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " '?',\n",
       " 'how',\n",
       " 'about',\n",
       " 'giving',\n",
       " 'us',\n",
       " 'different',\n",
       " 'scenes',\n",
       " 'offering',\n",
       " 'further',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'going',\n",
       " 'down',\n",
       " 'in',\n",
       " 'the',\n",
       " 'movie',\n",
       " '?',\n",
       " 'apparently',\n",
       " ',',\n",
       " 'the',\n",
       " 'studio',\n",
       " 'took',\n",
       " 'this',\n",
       " 'film',\n",
       " 'away',\n",
       " 'from',\n",
       " 'its',\n",
       " 'director',\n",
       " 'and',\n",
       " 'chopped',\n",
       " 'it',\n",
       " 'up',\n",
       " 'themselves',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'shows',\n",
       " '.',\n",
       " 'there',\n",
       " 'might',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'decent',\n",
       " 'teen',\n",
       " 'mind-fuck',\n",
       " 'movie',\n",
       " 'in',\n",
       " 'here',\n",
       " 'somewhere',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'guess',\n",
       " '``',\n",
       " 'the',\n",
       " 'suits',\n",
       " '``',\n",
       " 'decided',\n",
       " 'that',\n",
       " 'turning',\n",
       " 'it',\n",
       " 'into',\n",
       " 'a',\n",
       " 'music',\n",
       " 'video',\n",
       " 'with',\n",
       " 'little',\n",
       " 'edge',\n",
       " ',',\n",
       " 'would',\n",
       " 'make',\n",
       " 'more',\n",
       " 'sense',\n",
       " '.',\n",
       " 'the',\n",
       " 'actors',\n",
       " 'are',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'for',\n",
       " 'the',\n",
       " 'most',\n",
       " 'part',\n",
       " ',',\n",
       " 'although',\n",
       " 'wes',\n",
       " 'bentley',\n",
       " 'just',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'playing',\n",
       " 'the',\n",
       " 'exact',\n",
       " 'same',\n",
       " 'character',\n",
       " 'that',\n",
       " 'he',\n",
       " 'did',\n",
       " 'in',\n",
       " 'american',\n",
       " 'beauty',\n",
       " ',',\n",
       " 'only',\n",
       " 'in',\n",
       " 'a',\n",
       " 'new',\n",
       " 'neighborhood',\n",
       " '.',\n",
       " 'but',\n",
       " 'my',\n",
       " 'biggest',\n",
       " 'kudos',\n",
       " 'go',\n",
       " 'out',\n",
       " 'to',\n",
       " 'sagemiller',\n",
       " ',',\n",
       " 'who',\n",
       " 'holds',\n",
       " 'her',\n",
       " 'own',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'film',\n",
       " ',',\n",
       " 'and',\n",
       " 'actually',\n",
       " 'has',\n",
       " 'you',\n",
       " 'feeling',\n",
       " 'her',\n",
       " 'character',\n",
       " \"'s\",\n",
       " 'unraveling',\n",
       " '.',\n",
       " 'overall',\n",
       " ',',\n",
       " 'the',\n",
       " 'film',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'stick',\n",
       " 'because',\n",
       " 'it',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'entertain',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'confusing',\n",
       " ',',\n",
       " 'it',\n",
       " 'rarely',\n",
       " 'excites',\n",
       " 'and',\n",
       " 'it',\n",
       " 'feels',\n",
       " 'pretty',\n",
       " 'redundant',\n",
       " 'for',\n",
       " 'most',\n",
       " 'of',\n",
       " 'its',\n",
       " 'runtime',\n",
       " ',',\n",
       " 'despite',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'ending',\n",
       " 'and',\n",
       " 'explanation',\n",
       " 'to',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'craziness',\n",
       " 'that',\n",
       " 'came',\n",
       " 'before',\n",
       " 'it',\n",
       " '.',\n",
       " 'oh',\n",
       " ',',\n",
       " 'and',\n",
       " 'by',\n",
       " 'the',\n",
       " 'way',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'horror',\n",
       " 'or',\n",
       " 'teen',\n",
       " 'slasher',\n",
       " 'flick',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'just',\n",
       " 'packaged',\n",
       " 'to',\n",
       " 'look',\n",
       " 'that',\n",
       " 'way',\n",
       " 'because',\n",
       " 'someone',\n",
       " 'is',\n",
       " 'apparently',\n",
       " 'assuming',\n",
       " 'that',\n",
       " 'the',\n",
       " 'genre',\n",
       " 'is',\n",
       " 'still',\n",
       " 'hot',\n",
       " 'with',\n",
       " 'the',\n",
       " 'kids',\n",
       " '.',\n",
       " 'it',\n",
       " 'also',\n",
       " 'wrapped',\n",
       " 'production',\n",
       " 'two',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'has',\n",
       " 'been',\n",
       " 'sitting',\n",
       " 'on',\n",
       " 'the',\n",
       " 'shelves',\n",
       " 'ever',\n",
       " 'since',\n",
       " '.',\n",
       " 'whatever',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'skip',\n",
       " 'it',\n",
       " '!',\n",
       " 'where',\n",
       " \"'s\",\n",
       " 'joblo',\n",
       " 'coming',\n",
       " 'from',\n",
       " '?',\n",
       " 'a',\n",
       " 'nightmare',\n",
       " 'of',\n",
       " 'elm',\n",
       " 'street',\n",
       " '3',\n",
       " '(',\n",
       " '7/10',\n",
       " ')',\n",
       " '-',\n",
       " 'blair',\n",
       " 'witch',\n",
       " '2',\n",
       " '(',\n",
       " '7/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'crow',\n",
       " '(',\n",
       " '9/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'crow',\n",
       " ':',\n",
       " 'salvation',\n",
       " '(',\n",
       " '4/10',\n",
       " ')',\n",
       " '-',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '(',\n",
       " '10/10',\n",
       " ')',\n",
       " '-',\n",
       " 'memento',\n",
       " '(',\n",
       " '10/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'others',\n",
       " '(',\n",
       " '9/10',\n",
       " ')',\n",
       " '-',\n",
       " 'stir',\n",
       " 'of',\n",
       " 'echoes',\n",
       " '(',\n",
       " '8/10',\n",
       " ')']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_dataset = LabeledText(documents, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[243,\n",
       " 17,\n",
       " 314,\n",
       " 294,\n",
       " 77,\n",
       " 140,\n",
       " 307,\n",
       " 20,\n",
       " 68,\n",
       " 237,\n",
       " 6,\n",
       " 97,\n",
       " 34,\n",
       " 299,\n",
       " 98,\n",
       " 8,\n",
       " 302,\n",
       " 135,\n",
       " 167,\n",
       " 33,\n",
       " 22,\n",
       " 8,\n",
       " 226,\n",
       " 220,\n",
       " 297,\n",
       " 145,\n",
       " 87,\n",
       " 6,\n",
       " 60,\n",
       " 158,\n",
       " 136,\n",
       " 74,\n",
       " 307,\n",
       " 262,\n",
       " 157,\n",
       " 165,\n",
       " 153,\n",
       " 179,\n",
       " 6,\n",
       " 34,\n",
       " 149,\n",
       " 214,\n",
       " 8,\n",
       " 333,\n",
       " 2,\n",
       " 297,\n",
       " 82,\n",
       " 18,\n",
       " 326,\n",
       " 297,\n",
       " 204,\n",
       " 34,\n",
       " 19,\n",
       " 280,\n",
       " 19,\n",
       " 124,\n",
       " 230,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 79,\n",
       " 17,\n",
       " 20,\n",
       " 199,\n",
       " 204,\n",
       " 129,\n",
       " 297,\n",
       " 294,\n",
       " 133,\n",
       " 296,\n",
       " 311,\n",
       " 225,\n",
       " 20,\n",
       " 322,\n",
       " 75,\n",
       " 164,\n",
       " 6,\n",
       " 60,\n",
       " 245,\n",
       " 169,\n",
       " 165,\n",
       " 20,\n",
       " 322,\n",
       " 46,\n",
       " 234,\n",
       " 8,\n",
       " 337,\n",
       " 168,\n",
       " 333,\n",
       " 188,\n",
       " 304,\n",
       " 253,\n",
       " 33,\n",
       " 108,\n",
       " 148,\n",
       " 226,\n",
       " 307,\n",
       " 345,\n",
       " 6,\n",
       " 272,\n",
       " 163,\n",
       " 132,\n",
       " 37,\n",
       " 122,\n",
       " 337,\n",
       " 42,\n",
       " 307,\n",
       " 59,\n",
       " 297,\n",
       " 201,\n",
       " 6,\n",
       " 196,\n",
       " 341,\n",
       " 348,\n",
       " 152,\n",
       " 34,\n",
       " 290,\n",
       " 4,\n",
       " 185,\n",
       " 156,\n",
       " 1,\n",
       " 195,\n",
       " 5,\n",
       " 6,\n",
       " 60,\n",
       " 300,\n",
       " 38,\n",
       " 142,\n",
       " 34,\n",
       " 46,\n",
       " 328,\n",
       " 220,\n",
       " 189,\n",
       " 28,\n",
       " 315,\n",
       " 220,\n",
       " 122,\n",
       " 6,\n",
       " 34,\n",
       " 301,\n",
       " 128,\n",
       " 173,\n",
       " 86,\n",
       " 208,\n",
       " 276,\n",
       " 304,\n",
       " 226,\n",
       " 76,\n",
       " 8,\n",
       " 302,\n",
       " 263,\n",
       " 307,\n",
       " 150,\n",
       " 293,\n",
       " 304,\n",
       " 246,\n",
       " 209,\n",
       " 72,\n",
       " 6,\n",
       " 60,\n",
       " 113,\n",
       " 169,\n",
       " 295,\n",
       " 8,\n",
       " 277,\n",
       " 333,\n",
       " 38,\n",
       " 297,\n",
       " 248,\n",
       " 341,\n",
       " 297,\n",
       " 204,\n",
       " 18,\n",
       " 331,\n",
       " 6,\n",
       " 170,\n",
       " 186,\n",
       " 247,\n",
       " 168,\n",
       " 296,\n",
       " 169,\n",
       " 2,\n",
       " 271,\n",
       " 309,\n",
       " 172,\n",
       " 8,\n",
       " 169,\n",
       " 282,\n",
       " 221,\n",
       " 19,\n",
       " 216,\n",
       " 19,\n",
       " 60,\n",
       " 299,\n",
       " 95,\n",
       " 167,\n",
       " 304,\n",
       " 19,\n",
       " 116,\n",
       " 19,\n",
       " 342,\n",
       " 165,\n",
       " 337,\n",
       " 347,\n",
       " 6,\n",
       " 40,\n",
       " 33,\n",
       " 43,\n",
       " 194,\n",
       " 6,\n",
       " 150,\n",
       " 215,\n",
       " 164,\n",
       " 333,\n",
       " 2,\n",
       " 141,\n",
       " 225,\n",
       " 8,\n",
       " 300,\n",
       " 38,\n",
       " 96,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 64,\n",
       " 70,\n",
       " 45,\n",
       " 130,\n",
       " 297,\n",
       " 81,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 229,\n",
       " 339,\n",
       " 183,\n",
       " 180,\n",
       " 297,\n",
       " 81,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 286,\n",
       " 36,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 91,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 20,\n",
       " 184,\n",
       " 220,\n",
       " 65,\n",
       " 260,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 308,\n",
       " 220,\n",
       " 330,\n",
       " 303,\n",
       " 296,\n",
       " 147,\n",
       " 6,\n",
       " 34,\n",
       " 203,\n",
       " 220,\n",
       " 169,\n",
       " 168,\n",
       " 271,\n",
       " 217,\n",
       " 114,\n",
       " 8,\n",
       " 218,\n",
       " 163,\n",
       " 240,\n",
       " 92,\n",
       " 208,\n",
       " 198,\n",
       " 312,\n",
       " 307,\n",
       " 317,\n",
       " 20,\n",
       " 121,\n",
       " 110,\n",
       " 218,\n",
       " 34,\n",
       " 299,\n",
       " 6,\n",
       " 60,\n",
       " 335,\n",
       " 28,\n",
       " 169,\n",
       " 93,\n",
       " 168,\n",
       " 137,\n",
       " 190,\n",
       " 297,\n",
       " 259,\n",
       " 69,\n",
       " 231,\n",
       " 34,\n",
       " 231,\n",
       " 26,\n",
       " 6,\n",
       " 163,\n",
       " 135,\n",
       " 175,\n",
       " 220,\n",
       " 117,\n",
       " 320,\n",
       " 25,\n",
       " 20,\n",
       " 338,\n",
       " 6,\n",
       " 337,\n",
       " 168,\n",
       " 304,\n",
       " 121,\n",
       " 2,\n",
       " 54,\n",
       " 247,\n",
       " 8,\n",
       " 169,\n",
       " 2,\n",
       " 219,\n",
       " 143,\n",
       " 304,\n",
       " 53,\n",
       " 261,\n",
       " 307,\n",
       " 155,\n",
       " 6,\n",
       " 60,\n",
       " 169,\n",
       " 265,\n",
       " 307,\n",
       " 325,\n",
       " 307,\n",
       " 155,\n",
       " 169,\n",
       " 71,\n",
       " 319,\n",
       " 170,\n",
       " 123,\n",
       " 125,\n",
       " 200,\n",
       " 8,\n",
       " 34,\n",
       " 92,\n",
       " 302,\n",
       " 187,\n",
       " 303,\n",
       " 106,\n",
       " 6,\n",
       " 305,\n",
       " 228,\n",
       " 108,\n",
       " 103,\n",
       " 6,\n",
       " 165,\n",
       " 297,\n",
       " 192,\n",
       " 18,\n",
       " 217,\n",
       " 251,\n",
       " 8,\n",
       " 297,\n",
       " 256,\n",
       " 236,\n",
       " 168,\n",
       " 296,\n",
       " 297,\n",
       " 39,\n",
       " 34,\n",
       " 163,\n",
       " 57,\n",
       " 89,\n",
       " 225,\n",
       " 127,\n",
       " 180,\n",
       " 304,\n",
       " 6,\n",
       " 277,\n",
       " 329,\n",
       " 24,\n",
       " 120,\n",
       " 203,\n",
       " 220,\n",
       " 169,\n",
       " 230,\n",
       " 61,\n",
       " 297,\n",
       " 146,\n",
       " 244,\n",
       " 6,\n",
       " 277,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 287,\n",
       " 25,\n",
       " 296,\n",
       " 86,\n",
       " 281,\n",
       " 307,\n",
       " 187,\n",
       " 20,\n",
       " 182,\n",
       " 55,\n",
       " 220,\n",
       " 266,\n",
       " 6,\n",
       " 60,\n",
       " 169,\n",
       " 284,\n",
       " 86,\n",
       " 208,\n",
       " 297,\n",
       " 187,\n",
       " 297,\n",
       " 121,\n",
       " 28,\n",
       " 296,\n",
       " 202,\n",
       " 106,\n",
       " 8,\n",
       " 163,\n",
       " 144,\n",
       " 297,\n",
       " 58,\n",
       " 181,\n",
       " 341,\n",
       " 205,\n",
       " 180,\n",
       " 304,\n",
       " 168,\n",
       " 296,\n",
       " 347,\n",
       " 268,\n",
       " 31,\n",
       " 187,\n",
       " 292,\n",
       " 296,\n",
       " 297,\n",
       " 43,\n",
       " 168,\n",
       " 19,\n",
       " 167,\n",
       " 169,\n",
       " 19,\n",
       " 108,\n",
       " 51,\n",
       " 302,\n",
       " 38,\n",
       " 138,\n",
       " 297,\n",
       " 261,\n",
       " 238,\n",
       " 307,\n",
       " 104,\n",
       " 348,\n",
       " 342,\n",
       " 220,\n",
       " 316,\n",
       " 8,\n",
       " 163,\n",
       " 191,\n",
       " 6,\n",
       " 269,\n",
       " 193,\n",
       " 257,\n",
       " 254,\n",
       " 44,\n",
       " 130,\n",
       " 324,\n",
       " 129,\n",
       " 21,\n",
       " 11,\n",
       " 200,\n",
       " 306,\n",
       " 297,\n",
       " 204,\n",
       " 168,\n",
       " 173,\n",
       " 241,\n",
       " 178,\n",
       " 0,\n",
       " 0,\n",
       " 224,\n",
       " 6,\n",
       " 329,\n",
       " 135,\n",
       " 169,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 300,\n",
       " 38,\n",
       " 239,\n",
       " 66,\n",
       " 153,\n",
       " 34,\n",
       " 329,\n",
       " 92,\n",
       " 208,\n",
       " 176,\n",
       " 339,\n",
       " 302,\n",
       " 38,\n",
       " 8,\n",
       " 92,\n",
       " 329,\n",
       " 251,\n",
       " 210,\n",
       " 307,\n",
       " 262,\n",
       " 169,\n",
       " 231,\n",
       " 34,\n",
       " 231,\n",
       " 26,\n",
       " 18,\n",
       " 162,\n",
       " 21,\n",
       " 139,\n",
       " 321,\n",
       " 88,\n",
       " 260,\n",
       " 222,\n",
       " 131,\n",
       " 166,\n",
       " 167,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 287,\n",
       " 141,\n",
       " 94,\n",
       " 165,\n",
       " 297,\n",
       " 204,\n",
       " 18,\n",
       " 35,\n",
       " 6,\n",
       " 297,\n",
       " 289,\n",
       " 310,\n",
       " 304,\n",
       " 121,\n",
       " 44,\n",
       " 130,\n",
       " 170,\n",
       " 90,\n",
       " 34,\n",
       " 67,\n",
       " 169,\n",
       " 320,\n",
       " 298,\n",
       " 6,\n",
       " 34,\n",
       " 169,\n",
       " 270,\n",
       " 8,\n",
       " 300,\n",
       " 197,\n",
       " 3,\n",
       " 50,\n",
       " 20,\n",
       " 246,\n",
       " 83,\n",
       " 294,\n",
       " 199,\n",
       " 204,\n",
       " 165,\n",
       " 154,\n",
       " 279,\n",
       " 6,\n",
       " 60,\n",
       " 163,\n",
       " 144,\n",
       " 19,\n",
       " 297,\n",
       " 291,\n",
       " 19,\n",
       " 84,\n",
       " 296,\n",
       " 313,\n",
       " 169,\n",
       " 167,\n",
       " 20,\n",
       " 206,\n",
       " 323,\n",
       " 341,\n",
       " 182,\n",
       " 100,\n",
       " 6,\n",
       " 343,\n",
       " 187,\n",
       " 202,\n",
       " 266,\n",
       " 8,\n",
       " 297,\n",
       " 23,\n",
       " 38,\n",
       " 246,\n",
       " 142,\n",
       " 129,\n",
       " 297,\n",
       " 203,\n",
       " 236,\n",
       " 6,\n",
       " 30,\n",
       " 332,\n",
       " 52,\n",
       " 173,\n",
       " 264,\n",
       " 307,\n",
       " 47,\n",
       " 242,\n",
       " 297,\n",
       " 111,\n",
       " 259,\n",
       " 63,\n",
       " 296,\n",
       " 151,\n",
       " 86,\n",
       " 165,\n",
       " 32,\n",
       " 48,\n",
       " 6,\n",
       " 227,\n",
       " 165,\n",
       " 20,\n",
       " 212,\n",
       " 211,\n",
       " 8,\n",
       " 60,\n",
       " 207,\n",
       " 54,\n",
       " 177,\n",
       " 140,\n",
       " 230,\n",
       " 307,\n",
       " 257,\n",
       " 6,\n",
       " 339,\n",
       " 159,\n",
       " 153,\n",
       " 233,\n",
       " 306,\n",
       " 297,\n",
       " 107,\n",
       " 121,\n",
       " 6,\n",
       " 34,\n",
       " 24,\n",
       " 149,\n",
       " 347,\n",
       " 118,\n",
       " 153,\n",
       " 63,\n",
       " 2,\n",
       " 318,\n",
       " 8,\n",
       " 232,\n",
       " 6,\n",
       " 297,\n",
       " 121,\n",
       " 93,\n",
       " 208,\n",
       " 283,\n",
       " 49,\n",
       " 169,\n",
       " 93,\n",
       " 208,\n",
       " 105,\n",
       " 6,\n",
       " 169,\n",
       " 2,\n",
       " 73,\n",
       " 6,\n",
       " 169,\n",
       " 250,\n",
       " 112,\n",
       " 34,\n",
       " 169,\n",
       " 119,\n",
       " 246,\n",
       " 252,\n",
       " 129,\n",
       " 203,\n",
       " 220,\n",
       " 170,\n",
       " 255,\n",
       " 6,\n",
       " 85,\n",
       " 20,\n",
       " 246,\n",
       " 75,\n",
       " 102,\n",
       " 34,\n",
       " 115,\n",
       " 307,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 78,\n",
       " 296,\n",
       " 62,\n",
       " 51,\n",
       " 169,\n",
       " 8,\n",
       " 223,\n",
       " 6,\n",
       " 34,\n",
       " 61,\n",
       " 297,\n",
       " 327,\n",
       " 6,\n",
       " 304,\n",
       " 168,\n",
       " 217,\n",
       " 20,\n",
       " 160,\n",
       " 228,\n",
       " 294,\n",
       " 275,\n",
       " 126,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 169,\n",
       " 2,\n",
       " 173,\n",
       " 235,\n",
       " 307,\n",
       " 183,\n",
       " 296,\n",
       " 327,\n",
       " 49,\n",
       " 278,\n",
       " 168,\n",
       " 35,\n",
       " 41,\n",
       " 296,\n",
       " 297,\n",
       " 134,\n",
       " 168,\n",
       " 284,\n",
       " 161,\n",
       " 341,\n",
       " 297,\n",
       " 174,\n",
       " 8,\n",
       " 169,\n",
       " 29,\n",
       " 344,\n",
       " 249,\n",
       " 314,\n",
       " 346,\n",
       " 27,\n",
       " 34,\n",
       " 149,\n",
       " 50,\n",
       " 273,\n",
       " 225,\n",
       " 297,\n",
       " 267,\n",
       " 109,\n",
       " 272,\n",
       " 8,\n",
       " 334,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 274,\n",
       " 169,\n",
       " 0,\n",
       " 336,\n",
       " 2,\n",
       " 171,\n",
       " 70,\n",
       " 130,\n",
       " 18,\n",
       " 20,\n",
       " 213,\n",
       " 220,\n",
       " 101,\n",
       " 288,\n",
       " 12,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 56,\n",
       " 340,\n",
       " 10,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 80,\n",
       " 4,\n",
       " 16,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 80,\n",
       " 17,\n",
       " 258,\n",
       " 4,\n",
       " 13,\n",
       " 5,\n",
       " 7,\n",
       " 185,\n",
       " 156,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 195,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 229,\n",
       " 4,\n",
       " 16,\n",
       " 5,\n",
       " 7,\n",
       " 285,\n",
       " 220,\n",
       " 99,\n",
       " 4,\n",
       " 15,\n",
       " 5]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset[0]['X']  # First review in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset[0]['Y']  # Label of the first review in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Word2Vec Training\n",
    "\n",
    "Word2Vec has two training variants:\n",
    "\n",
    " - **Continuous Bag of Words (CBOW)**: Predict center word from (bag of) context words.\n",
    " - **Skip-grams**: Predict context words given center word.\n",
    "  \n",
    "Visually, they look like this:\n",
    "\n",
    "<img src=\"https://ibin.co/4UIznsOEyH7t.png\" width=\"500\">\n",
    "\n",
    "## 3.1.1. CBOW\n",
    "\n",
    "CBOW windows through the sentence and picks out the center word as the `Y` and the surrounding context words as the inputs `X`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def per_window(sequence, n=1):\n",
    "    \"\"\"\n",
    "    From http://stackoverflow.com/q/42220614/610569\n",
    "        >>> list(per_window([1,2,3,4], n=2))\n",
    "        [(1, 2), (2, 3), (3, 4)]\n",
    "        >>> list(per_window([1,2,3,4], n=3))\n",
    "        [(1, 2, 3), (2, 3, 4)]\n",
    "    \"\"\"\n",
    "    start, stop = 0, n\n",
    "    seq = list(sequence)\n",
    "    while stop <= len(seq):\n",
    "        yield seq[start:stop]\n",
    "        start += 1\n",
    "        stop += 1\n",
    "\n",
    "def cbow_iterator(tokens, window_size):\n",
    "    n = window_size * 2 + 1\n",
    "    for window in per_window(tokens, n):\n",
    "        target = window.pop(window_size)\n",
    "        yield window, target   # X = window ; Y = target. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent0 = ['language', 'users', 'never', 'choose', 'words', 'randomly', ',', \n",
    "         'and', 'language', 'is', 'essentially', 'non-random', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['language', 'users', 'choose', 'words'], 'never'),\n",
       " (['users', 'never', 'words', 'randomly'], 'choose'),\n",
       " (['never', 'choose', 'randomly', ','], 'words'),\n",
       " (['choose', 'words', ',', 'and'], 'randomly'),\n",
       " (['words', 'randomly', 'and', 'language'], ','),\n",
       " (['randomly', ',', 'language', 'is'], 'and'),\n",
       " ([',', 'and', 'is', 'essentially'], 'language'),\n",
       " (['and', 'language', 'essentially', 'non-random'], 'is'),\n",
       " (['language', 'is', 'non-random', '.'], 'essentially')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_iterator(sent0, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['language', 'users', 'never', 'words', 'randomly', ','], 'choose'),\n",
       " (['users', 'never', 'choose', 'randomly', ',', 'and'], 'words'),\n",
       " (['never', 'choose', 'words', ',', 'and', 'language'], 'randomly'),\n",
       " (['choose', 'words', 'randomly', 'and', 'language', 'is'], ','),\n",
       " (['words', 'randomly', ',', 'language', 'is', 'essentially'], 'and'),\n",
       " (['randomly', ',', 'and', 'is', 'essentially', 'non-random'], 'language'),\n",
       " ([',', 'and', 'language', 'essentially', 'non-random', '.'], 'is')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_iterator(sent0, 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2. Skipgram\n",
    "\n",
    "Skipgram training windows through the sentence and pictures out the center word as the input `X` and the context words as the outputs `Y`, additionally, it will randommly sample words not in the window as **negative samples**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skipgram_iterator(tokens, window_size):\n",
    "    n = window_size * 2 + 1 \n",
    "    for i, window in enumerate(per_window(tokens, n)):\n",
    "        target = window.pop(window_size)\n",
    "        # Generate positive samples.\n",
    "        for context_word in window:\n",
    "            yield target, context_word, 1\n",
    "        # Generate negative samples.\n",
    "        for _ in range(n-1):\n",
    "            leftovers = tokens[:i] + tokens[i+n:]\n",
    "            yield target, random.choice(leftovers), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language users never choose words randomly , and language is essentially non-random .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('never', 'language', 1),\n",
       " ('never', 'users', 1),\n",
       " ('never', 'choose', 1),\n",
       " ('never', 'words', 1),\n",
       " ('never', '.', 0),\n",
       " ('never', 'randomly', 0),\n",
       " ('never', 'language', 0),\n",
       " ('never', 'and', 0),\n",
       " ('choose', 'users', 1),\n",
       " ('choose', 'never', 1),\n",
       " ('choose', 'words', 1),\n",
       " ('choose', 'randomly', 1),\n",
       " ('choose', '.', 0),\n",
       " ('choose', 'non-random', 0),\n",
       " ('choose', 'and', 0),\n",
       " ('choose', 'non-random', 0),\n",
       " ('words', 'never', 1),\n",
       " ('words', 'choose', 1),\n",
       " ('words', 'randomly', 1),\n",
       " ('words', ',', 1),\n",
       " ('words', 'is', 0),\n",
       " ('words', 'non-random', 0),\n",
       " ('words', 'users', 0),\n",
       " ('words', 'users', 0),\n",
       " ('randomly', 'choose', 1),\n",
       " ('randomly', 'words', 1),\n",
       " ('randomly', ',', 1),\n",
       " ('randomly', 'and', 1),\n",
       " ('randomly', 'never', 0),\n",
       " ('randomly', '.', 0),\n",
       " ('randomly', 'never', 0),\n",
       " ('randomly', 'language', 0),\n",
       " (',', 'words', 1),\n",
       " (',', 'randomly', 1),\n",
       " (',', 'and', 1),\n",
       " (',', 'language', 1),\n",
       " (',', 'essentially', 0),\n",
       " (',', 'language', 0),\n",
       " (',', 'never', 0),\n",
       " (',', 'users', 0),\n",
       " ('and', 'randomly', 1),\n",
       " ('and', ',', 1),\n",
       " ('and', 'language', 1),\n",
       " ('and', 'is', 1),\n",
       " ('and', 'language', 0),\n",
       " ('and', 'language', 0),\n",
       " ('and', '.', 0),\n",
       " ('and', 'language', 0),\n",
       " ('language', ',', 1),\n",
       " ('language', 'and', 1),\n",
       " ('language', 'is', 1),\n",
       " ('language', 'essentially', 1),\n",
       " ('language', 'choose', 0),\n",
       " ('language', '.', 0),\n",
       " ('language', '.', 0),\n",
       " ('language', 'non-random', 0),\n",
       " ('is', 'and', 1),\n",
       " ('is', 'language', 1),\n",
       " ('is', 'essentially', 1),\n",
       " ('is', 'non-random', 1),\n",
       " ('is', '.', 0),\n",
       " ('is', 'randomly', 0),\n",
       " ('is', 'users', 0),\n",
       " ('is', 'never', 0),\n",
       " ('essentially', 'language', 1),\n",
       " ('essentially', 'is', 1),\n",
       " ('essentially', 'non-random', 1),\n",
       " ('essentially', '.', 1),\n",
       " ('essentially', 'randomly', 0),\n",
       " ('essentially', 'and', 0),\n",
       " ('essentially', ',', 0),\n",
       " ('essentially', ',', 0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(' '.join(sent0))\n",
    "list(skipgram_iterator(sent0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3 Word2Vec Dataset\n",
    "\n",
    "Now that we know what are the inputs `X` and outputs `Y` of the Word2Vec task. \n",
    "\n",
    "Lets put everything together and modify the `Dataset` so that `__getitem__` retrieves CBOW or Skipgram formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import functional as F\n",
    "\n",
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        \"\"\"\n",
    "            >>> tokens = ['language', 'users', 'never', 'choose', 'words', 'randomly', \n",
    "            ...           ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n",
    "            >>> cbow_iterator(tokens, 2)\n",
    "            [(['language', 'users', 'choose', 'words'], 'never'),\n",
    "            (['users', 'never', 'words', 'randomly'], 'choose'),\n",
    "            (['never', 'choose', 'randomly', ','], 'words'),\n",
    "            (['choose', 'words', ',', 'and'], 'randomly'),\n",
    "            (['words', 'randomly', 'and', 'language'], ','),\n",
    "            (['randomly', ',', 'language', 'is'], 'and'),\n",
    "            ([',', 'and', 'is', 'essentially'], 'language'),\n",
    "            (['and', 'language', 'essentially', 'non-random'], 'is'),\n",
    "            (['language', 'is', 'non-random', '.'], 'essentially')]\n",
    "        \"\"\"\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        \"\"\"\n",
    "            >>> tokens = ['language', 'users', 'never', 'choose', 'words', 'randomly', \n",
    "            ...           ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n",
    "            >>> list(skipgram_iterator(tokens, 2))[:10]\n",
    "            [('never', 'language', 1),\n",
    "             ('never', 'users', 1),\n",
    "             ('never', 'choose', 1),\n",
    "             ('never', 'words', 1),\n",
    "             ('never', 'non-random', 0),\n",
    "             ('never', 'is', 0),\n",
    "             ('never', 'and', 0),\n",
    "             ('never', 'and', 0),\n",
    "             ('choose', 'users', 1),\n",
    "             ('choose', 'never', 1)]\n",
    "        \"\"\"\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.5. Train a CBOW model\n",
    "\n",
    "### Lets Get Some Data\n",
    "\n",
    "Lets take Kilgarriff (2005) , \"Language is never ever, ever random\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
    "if os.path.isfile('language-never-random.txt'):\n",
    "    with open('language-never-random.txt') as fin:\n",
    "        text = fin.read()\n",
    "else:\n",
    "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
    "    text = requests.get(url).content.decode('utf8')\n",
    "    with open('language-never-random.txt', 'w') as fout:\n",
    "        fout.write(text)\n",
    "\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
    "window_size = 2\n",
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check, lets take a look at the data.\n",
    "print(tokenized_text[0])\n",
    "w2v_dataset.unvectorize([35, 188, 6, 232, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bulk of linguistic questions concern the dis- tinction between a and m. a linguistic account of a phenomenon gen- erally gives us reason to view the relation between , for example , a verb ’ s syntax and its semantics , as motivated rather than arbitrary .\n",
      "\u001b[92mof\u001b[0m \t\t the bulk \u001b[91m______\u001b[0m linguistic questions\n",
      "\u001b[92mlinguistic\u001b[0m \t bulk of \u001b[91m______\u001b[0m questions concern\n",
      "\u001b[92mquestions\u001b[0m \t of linguistic \u001b[91m______\u001b[0m concern the\n",
      "\u001b[92mconcern\u001b[0m \t linguistic questions \u001b[91m______\u001b[0m the dis-\n",
      "\u001b[92mthe\u001b[0m \t\t questions concern \u001b[91m______\u001b[0m dis- tinction\n",
      "\u001b[92mdis-\u001b[0m \t\t concern the \u001b[91m______\u001b[0m tinction between\n",
      "\u001b[92mtinction\u001b[0m \t the dis- \u001b[91m______\u001b[0m between a\n",
      "\u001b[92mbetween\u001b[0m \t dis- tinction \u001b[91m______\u001b[0m a and\n",
      "\u001b[92ma\u001b[0m \t\t tinction between \u001b[91m______\u001b[0m and m.\n",
      "\u001b[92mand\u001b[0m \t\t between a \u001b[91m______\u001b[0m m. a\n",
      "\u001b[92mm.\u001b[0m \t\t a and \u001b[91m______\u001b[0m a linguistic\n",
      "\u001b[92ma\u001b[0m \t\t and m. \u001b[91m______\u001b[0m linguistic account\n",
      "\u001b[92mlinguistic\u001b[0m \t m. a \u001b[91m______\u001b[0m account of\n",
      "\u001b[92maccount\u001b[0m \t a linguistic \u001b[91m______\u001b[0m of a\n",
      "\u001b[92mof\u001b[0m \t\t linguistic account \u001b[91m______\u001b[0m a phenomenon\n",
      "\u001b[92ma\u001b[0m \t\t account of \u001b[91m______\u001b[0m phenomenon gen-\n",
      "\u001b[92mphenomenon\u001b[0m \t of a \u001b[91m______\u001b[0m gen- erally\n",
      "\u001b[92mgen-\u001b[0m \t\t a phenomenon \u001b[91m______\u001b[0m erally gives\n",
      "\u001b[92merally\u001b[0m \t\t phenomenon gen- \u001b[91m______\u001b[0m gives us\n",
      "\u001b[92mgives\u001b[0m \t\t gen- erally \u001b[91m______\u001b[0m us reason\n",
      "\u001b[92mus\u001b[0m \t\t erally gives \u001b[91m______\u001b[0m reason to\n",
      "\u001b[92mreason\u001b[0m \t\t gives us \u001b[91m______\u001b[0m to view\n",
      "\u001b[92mto\u001b[0m \t\t us reason \u001b[91m______\u001b[0m view the\n",
      "\u001b[92mview\u001b[0m \t\t reason to \u001b[91m______\u001b[0m the relation\n",
      "\u001b[92mthe\u001b[0m \t\t to view \u001b[91m______\u001b[0m relation between\n",
      "\u001b[92mrelation\u001b[0m \t view the \u001b[91m______\u001b[0m between ,\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91m______\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91m______\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91m______\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91m______\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91m______\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91m______\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[91m______\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[91m______\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
      "\u001b[92mand\u001b[0m \t\t s syntax \u001b[91m______\u001b[0m its semantics\n",
      "\u001b[92mits\u001b[0m \t\t syntax and \u001b[91m______\u001b[0m semantics ,\n",
      "\u001b[92msemantics\u001b[0m \t and its \u001b[91m______\u001b[0m , as\n",
      "\u001b[92m,\u001b[0m \t\t its semantics \u001b[91m______\u001b[0m as motivated\n",
      "\u001b[92mas\u001b[0m \t\t semantics , \u001b[91m______\u001b[0m motivated rather\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91m______\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91m______\u001b[0m arbitrary .\n"
     ]
    }
   ],
   "source": [
    "from lazyme import color_str\n",
    "\n",
    "def visualize_predictions(x, y, prediction, vocab, window_size, unk='<unk>'):\n",
    "    left = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[:window_size]])\n",
    "    right = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[window_size:]])\n",
    "    target = vocab.get(int(y), '<unk>')\n",
    "\n",
    "    if not prediction:\n",
    "        predicted_word = '______'\n",
    "    else:\n",
    "        predicted_word = vocab.get(int(prediction), '<unk>') \n",
    "    print(color_str(target, 'green'), '\\t' if len(target) > 6 else '\\t\\t', \n",
    "          left, color_str(predicted_word, 'green' if target == predicted_word else 'red'), right)\n",
    "    \n",
    "\n",
    "sent_idx = 10\n",
    "window_size = 2\n",
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')\n",
    "print(' '.join(w2v_dataset.sents[sent_idx]))\n",
    "for w2v_io in w2v_dataset[sent_idx]:\n",
    "    context = x = tensor(w2v_io['x'])\n",
    "    target = y = tensor(w2v_io['y'])\n",
    "    visualize_predictions(context, target, None, w2v_dataset.vocab, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill-in the code for the CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Flatten the embeddings from multiple context words\n",
    "        # into a single vector.\n",
    "        embedded = self.embeddings(inputs).view((1, -1))\n",
    "        # Apply the first linear layer and activation.\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        # Apply a second linear and predict\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        # Return the weights\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211, 24)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenized_text_train, tokenized_text_test = train_test_split(tokenized_text, test_size=0.1, random_state=42)\n",
    "len(tokenized_text_train), len(tokenized_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [11:47<00:00,  6.24s/it]\n"
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.003\n",
    "hidden_size = 100\n",
    "window_size = 2\n",
    "\n",
    "# Initialize the dataset.\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "vocab_size = len(w2v_dataset.vocab)+1\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, embd_size, window_size, hidden_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epoch_loss = []\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x = tensor(w2v_io['x'])\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.long))\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = model(x)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(float(loss))\n",
    "    # Save model after every epoch.\n",
    "    torch.save(model.state_dict(), 'cbow_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArkAAAHSCAYAAADohdOwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2LMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply and Evaluate the CBOW Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 173, 696, 35] 28\n",
      "0\n",
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[91m______\u001b[0m essentially this\n",
      "[173, 28, 35, 217] 696\n",
      "59\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mto\u001b[0m this :\n",
      "[28, 696, 217, 161] 35\n",
      "59\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mto\u001b[0m : if\n",
      "[696, 35, 161, 95] 217\n",
      "39\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91mas\u001b[0m if a\n",
      "[35, 217, 95, 480] 161\n",
      "39\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mas\u001b[0m a word\n",
      "[217, 161, 480, 78] 95\n",
      "95\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[92ma\u001b[0m word (\n",
      "[161, 95, 78, 71] 480\n",
      "114\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91mi\u001b[0m ( or\n",
      "[95, 480, 71, 659] 78\n",
      "78\n",
      "\u001b[92m(\u001b[0m \t\t a word \u001b[92m(\u001b[0m or bigram\n",
      "[480, 78, 659, 0] 71\n",
      "91\n",
      "\u001b[92mor\u001b[0m \t\t word ( \u001b[91m1993\u001b[0m bigram ,\n",
      "[78, 71, 0, 71] 659\n",
      "79\n",
      "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
      "[0, 71, 0, 71] -1\n",
      "863\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mplan\u001b[0m , or\n",
      "[0, 71, 383, 18] -1\n",
      "944\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91msunday\u001b[0m etc .\n",
      "[34, 89, 95, 683] 28\n",
      "78\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[91m(\u001b[0m a vast\n",
      "[89, 28, 683, 897] 95\n",
      "70\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91mnot\u001b[0m vast re-\n",
      "[28, 95, 897, 1244] 683\n",
      "416\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mtheoretical\u001b[0m re- source\n",
      "[95, 683, 1244, 111] 897\n",
      "4\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mand\u001b[0m source for\n",
      "[683, 897, 111, 178] 1244\n",
      "0\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91m______\u001b[0m for many\n",
      "[28, 135, 748, 28] 34\n",
      "0\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[91m______\u001b[0m association is\n",
      "[135, 34, 28, 134] 748\n",
      "565\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91mprobability\u001b[0m is random\n",
      "[34, 748, 134, 0] 28\n",
      "30\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mof\u001b[0m random ,\n",
      "[748, 28, 0, 476] 134\n",
      "677\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91minappropriate\u001b[0m , arbitrary\n",
      "[28, 134, 476, 0] 0\n",
      "0\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91m______\u001b[0m arbitrary ,\n",
      "[134, 0, 0, 331] 476\n",
      "71\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[91mor\u001b[0m , motivated\n",
      "[0, 476, 331, 71] 0\n",
      "71\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91mor\u001b[0m motivated or\n",
      "[476, 0, 71, 252] 331\n",
      "331\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[92mmotivated\u001b[0m or pre-\n",
      "[71, 252, 78, 558] -1\n",
      "79\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91m)\u001b[0m ( r\n",
      "[78, 558, 95, 0] 0\n",
      "79\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m)\u001b[0m a ,\n",
      "[95, 0, 0, 129] -1\n",
      "262\n",
      "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mfirstly\u001b[0m , p\n",
      "[813, 0, 474, 64] 300\n",
      "4\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mand\u001b[0m methods are\n",
      "[0, 111, 0, 24] 630\n",
      "630\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
      "[111, 630, 24, 877] 0\n",
      "234\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mverbs\u001b[0m from just\n",
      "[630, 0, 877, 875] 24\n",
      "111\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mfor\u001b[0m just those\n",
      "[877, 875, 176, 135] -1\n",
      "0\n",
      "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91m______\u001b[0m errors that\n",
      "[0, 4, 8, 70] 14\n",
      "95\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[91ma\u001b[0m do not\n",
      "[4, 14, 70, 876] 8\n",
      "64\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mare\u001b[0m not wish\n",
      "[14, 8, 876, 59] 70\n",
      "34\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mthe\u001b[0m wish to\n",
      "[876, 59, 358, 87] -1\n",
      "25\n",
      "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mhas\u001b[0m any scf\n",
      "[358, 87, 142, 57] 111\n",
      "27\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91min\u001b[0m which there\n",
      "[87, 111, 57, 28] 142\n",
      "34\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
      "[111, 142, 28, 358] 57\n",
      "117\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mlanguage\u001b[0m is any\n",
      "[142, 57, 358, 157] 28\n",
      "28\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
      "[57, 28, 157, 39] 358\n",
      "460\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91mvery\u001b[0m evidence as\n",
      "[28, 358, 39, 95] 157\n",
      "198\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91mby\u001b[0m as a\n",
      "[358, 157, 95, 194] 39\n",
      "30\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mof\u001b[0m a true\n",
      "[157, 39, 194, 87] 95\n",
      "119\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mlinguistics\u001b[0m true scf\n",
      "[39, 95, 87, 111] 194\n",
      "342\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91mhere\u001b[0m scf for\n",
      "[95, 194, 111, 34] 87\n",
      "134\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mrandom\u001b[0m for the\n",
      "[194, 87, 34, 256] 111\n",
      "711\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mgiven\u001b[0m the verb\n",
      "[87, 111, 256, 18] 34\n",
      "34\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
      "[31, 88, 800, 59] -1\n",
      "30\n",
      "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mof\u001b[0m out to\n",
      "[58, 135, 1056, 24] 212\n",
      "117\n",
      "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mlanguage\u001b[0m indistinguishable from\n",
      "[135, 212, 24, 128] 1056\n",
      "45\n",
      "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mbe\u001b[0m from one\n",
      "[212, 1056, 128, 61] 24\n",
      "27\n",
      "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91min\u001b[0m one where\n",
      "[1056, 24, 61, 34] 128\n",
      "1073\n",
      "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mmight\u001b[0m where the\n",
      "[24, 128, 34, 1292] 61\n",
      "280\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mon\u001b[0m the individual\n",
      "[128, 61, 1292, 77] 34\n",
      "4\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mand\u001b[0m individual words\n",
      "[61, 34, 77, 78] 1292\n",
      "30\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mof\u001b[0m words (\n",
      "[34, 1292, 78, 39] 77\n",
      "480\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mword\u001b[0m ( as\n",
      "[1292, 77, 39, 928] 78\n",
      "78\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[92m(\u001b[0m as opposed\n",
      "[77, 78, 928, 59] 39\n",
      "39\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
      "[78, 39, 59, 34] 928\n",
      "79\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
      "[39, 928, 34, 368] 59\n",
      "4\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91mand\u001b[0m the texts\n",
      "[928, 59, 368, 79] 34\n",
      "951\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91mal\u001b[0m texts )\n",
      "[59, 34, 79, 894] 368\n",
      "60\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mtwo\u001b[0m ) had\n",
      "[34, 368, 894, 20] 79\n",
      "375\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mwere\u001b[0m had been\n",
      "[368, 79, 20, 73] 894\n",
      "25\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
      "[79, 894, 73, 1302] 20\n",
      "565\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91mprobability\u001b[0m randomly selected\n",
      "[894, 20, 1302, 0] 73\n",
      "30\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mof\u001b[0m selected ,\n",
      "[20, 73, 0, 35] 1302\n",
      "422\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mgenerated\u001b[0m , this\n",
      "[0, 35, 800, 70] -1\n",
      "71\n",
      "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91mor\u001b[0m out not\n",
      "[800, 70, 45, 34] 59\n",
      "100\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[91mcan\u001b[0m be the\n",
      "[70, 59, 34, 418] 45\n",
      "62\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mwhether\u001b[0m the case\n",
      "[59, 45, 418, 18] 34\n",
      "188\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mχ2\u001b[0m case .\n",
      "[1201, 4, 220, 215] -1\n",
      "220\n",
      "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mcarroll\u001b[0m carroll 1997\n",
      "[215, 843, 30, 32] -1\n",
      "19\n",
      "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91macquisition\u001b[0m of subcategorization\n",
      "[30, 32, 6, 18] 24\n",
      "24\n",
      "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[92mfrom\u001b[0m corpora .\n",
      "[34, 794, 1245, 140] 375\n",
      "348\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mtable\u001b[0m tested using\n",
      "[794, 375, 140, 34] 1245\n",
      "331\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mmotivated\u001b[0m using the\n",
      "[140, 34, 217, 28] -1\n",
      "554\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mmechanism\u001b[0m : is\n",
      "[150, 616, 150, 610] -1\n",
      "34\n",
      "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91mthe\u001b[0m ⫺ 0.5\n",
      "[79, 92, 721, 349] -1\n",
      "1236\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mfar\u001b[0m greater than\n",
      "[721, 349, 720, 725] 34\n",
      "610\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91m0.5\u001b[0m critical value\n",
      "[349, 34, 725, 2] 720\n",
      "60\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91mtwo\u001b[0m value ?\n",
      "[753, 481, 30, 536] -1\n",
      "592\n",
      "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mterms\u001b[0m of statistical\n",
      "[30, 536, 117, 902] 888\n",
      "888\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[92mnatural\u001b[0m language processing\n",
      "[536, 888, 902, 18] 117\n",
      "4\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mand\u001b[0m processing .\n",
      "[161, 34, 28, 458] 118\n",
      "165\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mrelation\u001b[0m is low\n",
      "[34, 118, 458, 0] 28\n",
      "30\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
      "[118, 28, 0, 42] 458\n",
      "1285\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mstood\u001b[0m , we\n",
      "[28, 458, 42, 609] 0\n",
      "0\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91m______\u001b[0m we reject\n",
      "[458, 0, 609, 160] 42\n",
      "143\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mwith\u001b[0m reject h0\n",
      "[0, 42, 160, 18] 609\n",
      "327\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
      "[813, 61, 184, 1013] 34\n",
      "34\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
      "[184, 1013, 198, 43] -1\n",
      "0\n",
      "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91m______\u001b[0m by an\n",
      "[442, 30, 0, 71] -1\n",
      "134\n",
      "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mrandom\u001b[0m , or\n",
      "[0, 71, 116, 28] 61\n",
      "135\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mthat\u001b[0m it is\n",
      "[71, 61, 28, 1009] 116\n",
      "57\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91mthere\u001b[0m is enormous\n",
      "[61, 116, 1009, 0] 28\n",
      "100\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[91mcan\u001b[0m enormous ,\n",
      "[116, 28, 0, 116] 1009\n",
      "108\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91mever\u001b[0m , it\n",
      "[28, 1009, 116, 28] 0\n",
      "135\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91mthat\u001b[0m it is\n",
      "[1009, 0, 28, 1127] 116\n",
      "57\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mthere\u001b[0m is wrong\n",
      "[0, 116, 1127, 59] 28\n",
      "42\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91mwe\u001b[0m wrong to\n",
      "[116, 28, 59, 527] 1127\n",
      "70\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mnot\u001b[0m to identify\n",
      "[28, 1127, 527, 34] 59\n",
      "0\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[91m______\u001b[0m identify the\n",
      "[527, 34, 329, 143] -1\n",
      "4\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mand\u001b[0m distinction with\n",
      "[143, 34, 128, 18] -1\n",
      "1249\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mfollowing\u001b[0m one .\n",
      "[281, 30, 783, 30] 34\n",
      "34\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
      "[30, 34, 30, 34] 783\n",
      "338\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91mcells\u001b[0m of the\n",
      "[159, 462, 723, 43] 28\n",
      "24\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mfrom\u001b[0m often an\n",
      "[723, 43, 88, 59] -1\n",
      "34\n",
      "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mthe\u001b[0m way to\n",
      "[88, 59, 94, 34] -1\n",
      "107\n",
      "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91mestimate\u001b[0m ; the\n",
      "[34, 173, 61, 34] -1\n",
      "0\n",
      "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91m______\u001b[0m where the\n",
      "[30, 34, 28, 486] -1\n",
      "134\n",
      "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mrandom\u001b[0m is overlooked\n",
      "[1196, 1197, 428, 78] 119\n",
      "119\n",
      "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[92mlinguistics\u001b[0m 16 (\n",
      "[1197, 119, 78, 333] 428\n",
      "912\n",
      "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m19\u001b[0m ( 1\n",
      "[119, 428, 333, 79] 78\n",
      "217\n",
      "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[91m:\u001b[0m 1 )\n",
      "[428, 78, 79, 0] 333\n",
      "114\n",
      "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[91mi\u001b[0m ) ,\n",
      "[508, 117, 172, 4] 28\n",
      "28\n",
      "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[92mis\u001b[0m non-random and\n",
      "[117, 28, 4, 698] 172\n",
      "0\n",
      "\u001b[92mnon-random\u001b[0m \t language is \u001b[91m______\u001b[0m and hence\n",
      "[28, 172, 698, 0] 4\n",
      "0\n",
      "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91m______\u001b[0m hence ,\n",
      "[172, 4, 0, 452] 698\n",
      "644\n",
      "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mshoe-polish\u001b[0m , when\n",
      "[4, 698, 452, 42] 0\n",
      "64\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mare\u001b[0m when we\n",
      "[698, 0, 42, 700] 452\n",
      "452\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
      "[0, 452, 700, 290] 42\n",
      "42\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
      "[452, 42, 290, 699] 700\n",
      "700\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[92mlook\u001b[0m at linguistic\n",
      "[27, 6, 34, 192] 0\n",
      "140\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91musing\u001b[0m the null\n",
      "[6, 0, 192, 26] 34\n",
      "34\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
      "[0, 34, 26, 370] 192\n",
      "192\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
      "[34, 192, 370, 125] 26\n",
      "26\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
      "[192, 26, 125, 45] 370\n",
      "370\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
      "[26, 370, 45, 194] 125\n",
      "125\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
      "[370, 125, 194, 18] 45\n",
      "45\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
      "[42, 8, 325, 263] 70\n",
      "463\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[91mfrequently\u001b[0m always have\n",
      "[8, 70, 263, 155] 325\n",
      "45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mbe\u001b[0m have enough\n",
      "[70, 325, 155, 170] 263\n",
      "35\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[91mthis\u001b[0m enough data\n",
      "[325, 263, 170, 59] 155\n",
      "155\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[92menough\u001b[0m data to\n",
      "[263, 155, 59, 609] 170\n",
      "170\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
      "[155, 170, 609, 34] 59\n",
      "59\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
      "[170, 59, 34, 192] 609\n",
      "609\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
      "[59, 609, 192, 26] 34\n",
      "34\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
      "[609, 34, 26, 0] 192\n",
      "192\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
      "[34, 192, 0, 169] 26\n",
      "26\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
      "[192, 26, 169, 135] 0\n",
      "59\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91mto\u001b[0m but that\n",
      "[26, 0, 135, 28] 169\n",
      "169\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[92mbut\u001b[0m that is\n",
      "[0, 169, 28, 95] 135\n",
      "117\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mlanguage\u001b[0m is a\n",
      "[28, 95, 264, 217] -1\n",
      "46\n",
      "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mbetween\u001b[0m issue :\n",
      "[264, 217, 57, 28] 1116\n",
      "0\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91m______\u001b[0m there is\n",
      "[217, 1116, 28, 155] 57\n",
      "57\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
      "[1116, 57, 155, 170] 28\n",
      "28\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
      "[57, 28, 170, 0] 155\n",
      "155\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
      "[28, 155, 0, 116] 170\n",
      "170\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
      "[155, 170, 116, 28] 0\n",
      "0\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
      "[170, 0, 28, 873] 116\n",
      "160\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[91mh0\u001b[0m is rejected\n",
      "[0, 116, 873, 18] 28\n",
      "30\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91mof\u001b[0m rejected .\n",
      "[591, 77, 95, 593] 27\n",
      "61\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[91mwhere\u001b[0m a text\n",
      "[77, 27, 593, 64] 95\n",
      "291\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[91mcommon\u001b[0m text are\n",
      "[27, 95, 64, 70] 593\n",
      "14\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mthey\u001b[0m are not\n",
      "[95, 593, 70, 134] 64\n",
      "0\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91m______\u001b[0m not random\n",
      "[593, 64, 134, 0] 70\n",
      "70\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[92mnot\u001b[0m random ,\n",
      "[64, 70, 0, 42] 134\n",
      "134\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[92mrandom\u001b[0m , we\n",
      "[70, 134, 42, 779] 0\n",
      "258\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91mbecause\u001b[0m we know\n",
      "[134, 0, 779, 135] 42\n",
      "4\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
      "[0, 42, 135, 426] 779\n",
      "0\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91m______\u001b[0m that our\n",
      "[42, 779, 426, 6] 135\n",
      "290\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91mat\u001b[0m our corpora\n",
      "[779, 135, 6, 64] 426\n",
      "30\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91mof\u001b[0m corpora are\n",
      "[135, 426, 64, 70] 6\n",
      "68\n",
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mhelp\u001b[0m are not\n",
      "[426, 6, 70, 73] 64\n",
      "287\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mdoes\u001b[0m not randomly\n",
      "[6, 64, 73, 422] 70\n",
      "35\n",
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mthis\u001b[0m randomly generated\n",
      "[64, 70, 422, 0] 73\n",
      "426\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91mour\u001b[0m generated ,\n",
      "[70, 73, 0, 4] 422\n",
      "134\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mrandom\u001b[0m , and\n",
      "[73, 422, 4, 34] 0\n",
      "0\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91m______\u001b[0m and the\n",
      "[422, 0, 34, 26] 4\n",
      "64\n",
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mare\u001b[0m the hypothesis\n",
      "[0, 4, 26, 806] 34\n",
      "787\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[91mbinomial\u001b[0m hypothesis test\n",
      "[4, 34, 806, 1079] 26\n",
      "26\n",
      "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n",
      "[806, 1079, 34, 158] -1\n",
      "30\n",
      "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mof\u001b[0m the fact\n",
      "[614, 64, 27, 601] -1\n",
      "0\n",
      "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91m______\u001b[0m in section\n",
      "[30, 699, 47, 34] -1\n",
      "565\n",
      "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mprobability\u001b[0m concern the\n",
      "[34, 457, 46, 95] -1\n",
      "488\n",
      "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91mrelationship\u001b[0m between a\n",
      "[95, 4, 95, 699] -1\n",
      "134\n",
      "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mrandom\u001b[0m a linguistic\n",
      "[95, 699, 30, 95] -1\n",
      "411\n",
      "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91mnumber\u001b[0m of a\n",
      "[584, 59, 34, 165] -1\n",
      "156\n",
      "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mestablish\u001b[0m the relation\n",
      "[34, 165, 0, 111] 46\n",
      "186\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mstatistic\u001b[0m , for\n",
      "[165, 46, 111, 630] 0\n",
      "101\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mcorpus\u001b[0m for example\n",
      "[46, 0, 630, 0] 111\n",
      "111\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[92mfor\u001b[0m example ,\n",
      "[0, 111, 0, 95] 630\n",
      "630\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , a\n",
      "[111, 630, 95, 256] 0\n",
      "290\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mat\u001b[0m a verb\n",
      "[630, 0, 256, 63] 95\n",
      "71\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91mor\u001b[0m verb ’\n",
      "[0, 95, 63, 55] 256\n",
      "28\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mis\u001b[0m ’ s\n",
      "[95, 256, 55, 1150] 63\n",
      "63\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[92m’\u001b[0m s syntax\n",
      "[256, 63, 1150, 4] 55\n",
      "55\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
      "[63, 55, 4, 543] 1150\n",
      "341\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91mdefinition\u001b[0m and its\n",
      "[4, 543, 0, 39] -1\n",
      "397\n",
      "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91mrandomness\u001b[0m , as\n",
      "[0, 39, 346, 349] 331\n",
      "0\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
      "[39, 331, 349, 476] 346\n",
      "64\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mare\u001b[0m than arbitrary\n",
      "[331, 346, 476, 18] 349\n",
      "349\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[92mthan\u001b[0m arbitrary .\n",
      "[34, 820, 30, 34] 725\n",
      "725\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
      "[820, 725, 34, 401] 30\n",
      "30\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
      "[725, 30, 401, 407] 34\n",
      "34\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
      "[30, 34, 407, 0] 401\n",
      "401\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[92merror\u001b[0m term ,\n",
      "[34, 401, 0, 117] 407\n",
      "407\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[92mterm\u001b[0m , language\n",
      "[401, 407, 117, 28] 0\n",
      "78\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91m(\u001b[0m language is\n",
      "[407, 0, 28, 125] 117\n",
      "57\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mthere\u001b[0m is never\n",
      "[0, 117, 125, 0] 28\n",
      "28\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
      "[117, 28, 0, 108] 125\n",
      "125\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
      "[28, 125, 108, 0] 0\n",
      "0\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
      "[125, 0, 0, 108] 108\n",
      "108\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
      "[0, 108, 108, 0] 0\n",
      "0\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
      "[108, 0, 0, 134] 108\n",
      "108\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
      "[79, 92, 28, 136] -1\n",
      "0\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91m______\u001b[0m is then\n",
      "[28, 136, 34, 26] -1\n",
      "64\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mare\u001b[0m the hypothesis\n",
      "[100, 0, 0, 45] -1\n",
      "397\n",
      "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91mrandomness\u001b[0m , be\n",
      "[0, 45, 39, 217] -1\n",
      "134\n",
      "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mrandom\u001b[0m as :\n",
      "[39, 217, 34, 401] 64\n",
      "143\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91mwith\u001b[0m the error\n",
      "[217, 64, 401, 592] 34\n",
      "34\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[92mthe\u001b[0m error terms\n",
      "[64, 34, 592, 479] 401\n",
      "413\n",
      "\u001b[92merror\u001b[0m \t\t are the \u001b[91msame\u001b[0m terms systematically\n",
      "[34, 401, 479, 721] 592\n",
      "407\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mterm\u001b[0m systematically greater\n",
      "[401, 592, 721, 349] 479\n",
      "59\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mto\u001b[0m greater than\n",
      "[592, 479, 349, 610] 721\n",
      "721\n",
      "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[92mgreater\u001b[0m than 0.5\n",
      "[479, 721, 610, 2] 349\n",
      "349\n",
      "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
      "[143, 877, 923, 30] 333\n",
      "0\n",
      "\u001b[92m1\u001b[0m \t\t with just \u001b[91m______\u001b[0m % of\n",
      "[877, 333, 30, 465] 923\n",
      "411\n",
      "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91mnumber\u001b[0m of them\n",
      "[333, 923, 465, 0] 30\n",
      "685\n",
      "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91mconfidence\u001b[0m them ,\n",
      "[923, 30, 0, 829] 465\n",
      "256\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mverb\u001b[0m , devastate\n",
      "[0, 829, 128, 30] -1\n",
      "135\n",
      "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mthat\u001b[0m one of\n",
      "[128, 30, 234, 111] 34\n",
      "301\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[91mthese\u001b[0m verbs for\n",
      "[30, 34, 111, 142] 234\n",
      "748\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91massociation\u001b[0m for which\n",
      "[34, 234, 142, 42] 111\n",
      "0\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m______\u001b[0m which we\n",
      "[234, 111, 42, 263] 142\n",
      "0\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91m______\u001b[0m we have\n",
      "[42, 263, 30, 170] -1\n",
      "35\n",
      "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91mthis\u001b[0m of data\n",
      "[0, 4, 674, 474] -1\n",
      "95\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91ma\u001b[0m thresholding methods\n",
      "[674, 474, 1146, 99] 370\n",
      "42\n",
      "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mwe\u001b[0m distinguish associated\n",
      "[474, 370, 99, 254] 1146\n",
      "27\n",
      "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91min\u001b[0m associated scfs\n",
      "[370, 1146, 254, 24] 99\n",
      "116\n",
      "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mit\u001b[0m scfs from\n",
      "[1146, 99, 24, 797] 254\n",
      "217\n",
      "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91m:\u001b[0m from noise\n",
      "[99, 254, 797, 18] 24\n",
      "240\n",
      "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91mfrequency\u001b[0m noise .\n"
     ]
    }
   ],
   "source": [
    "from lazyme import color_str\n",
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            print(w2v_io['x'],w2v_io['y'])\n",
    "            _, prediction =  torch.max(model(x), 1)\n",
    "            print(int(prediction))\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25957446808510637\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go back to the 90th Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CBOW(\n",
       "    (embeddings): Embedding(1304, 100)\n",
       "    (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=1304, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_90 = CBOW(vocab_size, embd_size, window_size, hidden_size)\n",
    "model_90 = torch.nn.DataParallel(model_90)\n",
    "model_90.load_state_dict(torch.load('cbow_checkpoint_80.pt'))\n",
    "model_90.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[91m______\u001b[0m essentially this\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mto\u001b[0m this :\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mto\u001b[0m : if\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91mas\u001b[0m if a\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mas\u001b[0m a word\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[91mand\u001b[0m word (\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91mi\u001b[0m ( or\n",
      "\u001b[92m(\u001b[0m \t\t a word \u001b[92m(\u001b[0m or bigram\n",
      "\u001b[92mor\u001b[0m \t\t word ( \u001b[91m1993\u001b[0m bigram ,\n",
      "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mplan\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91msunday\u001b[0m etc .\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[91m(\u001b[0m a vast\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91mnot\u001b[0m vast re-\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mtheoretical\u001b[0m re- source\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mand\u001b[0m source for\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91m______\u001b[0m for many\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[91m______\u001b[0m association is\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91mprobability\u001b[0m is random\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mof\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91minappropriate\u001b[0m , arbitrary\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91m______\u001b[0m arbitrary ,\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[91mor\u001b[0m , motivated\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91mor\u001b[0m motivated or\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[92mmotivated\u001b[0m or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91m)\u001b[0m ( r\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m)\u001b[0m a ,\n",
      "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mfirstly\u001b[0m , p\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mand\u001b[0m methods are\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mverbs\u001b[0m from just\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mfor\u001b[0m just those\n",
      "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91m______\u001b[0m errors that\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[91ma\u001b[0m do not\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mare\u001b[0m not wish\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mthe\u001b[0m wish to\n",
      "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mhas\u001b[0m any scf\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91min\u001b[0m which there\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mlanguage\u001b[0m is any\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91mvery\u001b[0m evidence as\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91mby\u001b[0m as a\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mof\u001b[0m a true\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mlinguistics\u001b[0m true scf\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91mhere\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mrandom\u001b[0m for the\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mgiven\u001b[0m the verb\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
      "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mof\u001b[0m out to\n",
      "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mlanguage\u001b[0m indistinguishable from\n",
      "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mbe\u001b[0m from one\n",
      "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91min\u001b[0m one where\n",
      "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mand\u001b[0m where the\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mon\u001b[0m the individual\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mand\u001b[0m individual words\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mof\u001b[0m words (\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mword\u001b[0m ( as\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[92m(\u001b[0m as opposed\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91mword\u001b[0m the texts\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91mal\u001b[0m texts )\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mtwo\u001b[0m ) had\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mwere\u001b[0m had been\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91mprobability\u001b[0m randomly selected\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mused\u001b[0m selected ,\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mgenerated\u001b[0m , this\n",
      "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91mor\u001b[0m out not\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[91mcan\u001b[0m be the\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mwhether\u001b[0m the case\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mχ2\u001b[0m case .\n",
      "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mcarroll\u001b[0m carroll 1997\n",
      "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91macquisition\u001b[0m of subcategorization\n",
      "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[92mfrom\u001b[0m corpora .\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mtable\u001b[0m tested using\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mmotivated\u001b[0m using the\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mmechanism\u001b[0m : is\n",
      "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91mthe\u001b[0m ⫺ 0.5\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mfar\u001b[0m greater than\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91m0.5\u001b[0m critical value\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91mtwo\u001b[0m value ?\n",
      "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mterms\u001b[0m of statistical\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[92mnatural\u001b[0m language processing\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mand\u001b[0m processing .\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mrelation\u001b[0m is low\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mstood\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91m______\u001b[0m we reject\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mwith\u001b[0m reject h0\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
      "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91m______\u001b[0m by an\n",
      "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mrandom\u001b[0m , or\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mthat\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91mthere\u001b[0m is enormous\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[91mcan\u001b[0m enormous ,\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91mever\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91mthat\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mthere\u001b[0m is wrong\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91mwe\u001b[0m wrong to\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mnot\u001b[0m to identify\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[91m______\u001b[0m identify the\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mand\u001b[0m distinction with\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mtheoretical\u001b[0m one .\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91mcells\u001b[0m of the\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mfrom\u001b[0m often an\n",
      "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mthe\u001b[0m way to\n",
      "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91mestimate\u001b[0m ; the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91m______\u001b[0m where the\n",
      "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mrandom\u001b[0m is overlooked\n",
      "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[92mlinguistics\u001b[0m 16 (\n",
      "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m19\u001b[0m ( 1\n",
      "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[91m:\u001b[0m 1 )\n",
      "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[91mi\u001b[0m ) ,\n",
      "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[92mis\u001b[0m non-random and\n",
      "\u001b[92mnon-random\u001b[0m \t language is \u001b[91m______\u001b[0m and hence\n",
      "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91m______\u001b[0m hence ,\n",
      "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mshoe-polish\u001b[0m , when\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mare\u001b[0m when we\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[92mlook\u001b[0m at linguistic\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91musing\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[91mfrequently\u001b[0m always have\n",
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mbe\u001b[0m have enough\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[91mthis\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[92menough\u001b[0m data to\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91mto\u001b[0m but that\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[92mbut\u001b[0m that is\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mlanguage\u001b[0m is a\n",
      "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mbetween\u001b[0m issue :\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91m______\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[91mh0\u001b[0m is rejected\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91mof\u001b[0m rejected .\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[91mwhere\u001b[0m a text\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[91mcommon\u001b[0m text are\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mthey\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91m______\u001b[0m not random\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[92mnot\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[92mrandom\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91mbecause\u001b[0m we know\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91m______\u001b[0m that our\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91mat\u001b[0m our corpora\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91mof\u001b[0m corpora are\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mhelp\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mdoes\u001b[0m not randomly\n",
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91msome\u001b[0m randomly generated\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91mour\u001b[0m generated ,\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mrandom\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91m______\u001b[0m and the\n",
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mare\u001b[0m the hypothesis\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[91mdiana\u001b[0m hypothesis test\n",
      "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n",
      "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mof\u001b[0m the fact\n",
      "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mfrequently\u001b[0m in section\n",
      "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mprobability\u001b[0m concern the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91mrelationship\u001b[0m between a\n",
      "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mrandom\u001b[0m a linguistic\n",
      "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91mnumber\u001b[0m of a\n",
      "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mestablish\u001b[0m the relation\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mstatistic\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mcorpus\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[92mfor\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mat\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91mor\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mis\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[92m’\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91mdefinition\u001b[0m and its\n",
      "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91mrandomness\u001b[0m , as\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mare\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[92mthan\u001b[0m arbitrary .\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[91mlog-likelihood\u001b[0m term ,\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[92mterm\u001b[0m , language\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91m(\u001b[0m language is\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mthere\u001b[0m is never\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91m______\u001b[0m is then\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mare\u001b[0m the hypothesis\n",
      "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91mrandomness\u001b[0m , be\n",
      "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mrandom\u001b[0m as :\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91mwith\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[92mthe\u001b[0m error terms\n",
      "\u001b[92merror\u001b[0m \t\t are the \u001b[91msame\u001b[0m terms systematically\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mterm\u001b[0m systematically greater\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mto\u001b[0m greater than\n",
      "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[92mgreater\u001b[0m than 0.5\n",
      "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
      "\u001b[92m1\u001b[0m \t\t with just \u001b[91m______\u001b[0m % of\n",
      "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91mnumber\u001b[0m of them\n",
      "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91mconfidence\u001b[0m them ,\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mverb\u001b[0m , devastate\n",
      "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mthat\u001b[0m one of\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[91mthese\u001b[0m verbs for\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91massociation\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m______\u001b[0m which we\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91m______\u001b[0m we have\n",
      "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91mthis\u001b[0m of data\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91ma\u001b[0m thresholding methods\n",
      "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mwe\u001b[0m distinguish associated\n",
      "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91min\u001b[0m associated scfs\n",
      "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mit\u001b[0m scfs from\n",
      "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91m:\u001b[0m from noise\n",
      "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91mfrequency\u001b[0m noise .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(model_90(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.251063829787234\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Handle Unknown Words? \n",
    "\n",
    "This is not the best way to handle unknown words, but we can simply assign an index for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a', 1: 'bar', 2: 'foo', 3: 'is', 4: 'sentence', 5: 'this'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: '<unk>',\n",
       " 2: 'foo',\n",
       " 3: 'is',\n",
       " 4: 'sentence',\n",
       " 5: 'this',\n",
       " 6: 'a',\n",
       " 7: 'bar'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.patch_with_special_tokens\n",
    "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
    "special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "vocab.patch_with_special_tokens(special_tokens)\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Rewrite the `Word2VecText` Object\n",
    "\n",
    "Now with the (i) unknown word patch in the vocabulary as well as (ii) `skipgram_iterator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        \n",
    "        # Add the unknown word patch here.\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "        special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "        self.vocab.patch_with_special_tokens(special_tokens)\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens, unknown_word_index=1)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                yield {'x': (focus, random.choice(leftovers)), 'y':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_context = self.embeddings(context).view((1, -1))\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "        score = torch.mm(embed_focus, torch.t(embed_context))\n",
    "        log_probs = F.logsigmoid(score)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/211 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 1/211 [00:00<01:28,  2.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 2/211 [00:00<01:29,  2.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 5/211 [00:01<00:55,  3.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 6/211 [00:02<01:11,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 7/211 [00:02<01:24,  2.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 8/211 [00:07<03:00,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 9/211 [00:07<02:54,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 10/211 [00:08<02:41,  1.25it/s]\u001b[A\u001b[A\n",
      "Exception in thread Thread-43:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 148, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "\n",
      "  6%|▌         | 12/211 [00:08<02:24,  1.37it/s]\u001b[A\n",
      "  7%|▋         | 14/211 [00:09<02:08,  1.54it/s]\u001b[A\n",
      "  7%|▋         | 15/211 [00:09<02:07,  1.54it/s]\u001b[A\n",
      "  8%|▊         | 16/211 [00:10<02:05,  1.55it/s]\u001b[A\n",
      "  9%|▊         | 18/211 [00:10<01:52,  1.71it/s]\u001b[A\n",
      "  9%|▉         | 19/211 [00:10<01:50,  1.73it/s]\u001b[A\n",
      "  9%|▉         | 20/211 [00:11<01:52,  1.70it/s]\u001b[A\n",
      " 10%|█         | 22/211 [00:12<01:44,  1.81it/s]\u001b[A\n",
      " 11%|█         | 23/211 [00:12<01:43,  1.82it/s]\u001b[A\n",
      " 11%|█▏        | 24/211 [00:13<01:46,  1.75it/s]\u001b[A\n",
      " 12%|█▏        | 25/211 [00:14<01:45,  1.77it/s]\u001b[A\n",
      " 12%|█▏        | 26/211 [00:14<01:42,  1.80it/s]\u001b[A\n",
      " 13%|█▎        | 27/211 [00:14<01:40,  1.84it/s]\u001b[A\n",
      " 13%|█▎        | 28/211 [00:14<01:37,  1.88it/s]\u001b[A\n",
      " 14%|█▎        | 29/211 [00:15<01:38,  1.86it/s]\u001b[A\n",
      " 15%|█▍        | 31/211 [00:16<01:34,  1.91it/s]\u001b[A\n",
      " 15%|█▌        | 32/211 [00:16<01:31,  1.95it/s]\u001b[A\n",
      " 16%|█▌        | 34/211 [00:16<01:27,  2.03it/s]\u001b[A\n",
      " 17%|█▋        | 35/211 [00:17<01:25,  2.06it/s]\u001b[A\n",
      " 17%|█▋        | 36/211 [00:17<01:24,  2.08it/s]\u001b[A\n",
      " 18%|█▊        | 37/211 [00:17<01:23,  2.07it/s]\u001b[A\n",
      " 18%|█▊        | 39/211 [00:18<01:22,  2.10it/s]\u001b[A\n",
      " 19%|█▉        | 41/211 [00:19<01:18,  2.15it/s]\u001b[A\n",
      " 21%|██        | 44/211 [00:19<01:13,  2.28it/s]\u001b[A\n",
      " 21%|██▏       | 45/211 [00:19<01:12,  2.28it/s]\u001b[A\n",
      " 22%|██▏       | 46/211 [00:19<01:11,  2.31it/s]\u001b[A\n",
      " 23%|██▎       | 48/211 [00:20<01:08,  2.37it/s]\u001b[A\n",
      " 23%|██▎       | 49/211 [00:20<01:07,  2.41it/s]\u001b[A\n",
      " 24%|██▎       | 50/211 [00:20<01:06,  2.42it/s]\u001b[A\n",
      " 24%|██▍       | 51/211 [00:21<01:06,  2.41it/s]\u001b[A\n",
      " 25%|██▍       | 52/211 [00:21<01:05,  2.41it/s]\u001b[A\n",
      " 26%|██▌       | 54/211 [00:21<01:03,  2.46it/s]\u001b[A\n",
      " 26%|██▌       | 55/211 [00:22<01:04,  2.43it/s]\u001b[A\n",
      " 27%|██▋       | 56/211 [00:22<01:03,  2.44it/s]\u001b[A\n",
      " 27%|██▋       | 57/211 [00:23<01:03,  2.44it/s]\u001b[A\n",
      " 27%|██▋       | 58/211 [00:23<01:03,  2.42it/s]\u001b[A\n",
      " 28%|██▊       | 60/211 [00:24<01:01,  2.44it/s]\u001b[A\n",
      " 29%|██▉       | 61/211 [00:26<01:04,  2.33it/s]\u001b[A\n",
      " 29%|██▉       | 62/211 [00:26<01:03,  2.34it/s]\u001b[A\n",
      " 30%|██▉       | 63/211 [00:26<01:02,  2.36it/s]\u001b[A\n",
      " 30%|███       | 64/211 [00:27<01:02,  2.36it/s]\u001b[A\n",
      " 31%|███       | 65/211 [00:27<01:01,  2.36it/s]\u001b[A\n",
      " 32%|███▏      | 67/211 [00:27<00:59,  2.42it/s]\u001b[A\n",
      " 32%|███▏      | 68/211 [00:28<01:00,  2.38it/s]\u001b[A\n",
      " 33%|███▎      | 70/211 [00:29<00:58,  2.39it/s]\u001b[A\n",
      " 34%|███▎      | 71/211 [00:29<00:58,  2.38it/s]\u001b[A\n",
      " 34%|███▍      | 72/211 [00:30<00:58,  2.38it/s]\u001b[A\n",
      " 35%|███▌      | 74/211 [00:30<00:57,  2.40it/s]\u001b[A\n",
      " 36%|███▌      | 75/211 [00:31<00:56,  2.39it/s]\u001b[A\n",
      " 36%|███▋      | 77/211 [00:31<00:55,  2.43it/s]\u001b[A\n",
      " 37%|███▋      | 78/211 [00:31<00:54,  2.46it/s]\u001b[A\n",
      " 37%|███▋      | 79/211 [00:32<00:53,  2.46it/s]\u001b[A\n",
      " 38%|███▊      | 80/211 [00:32<00:53,  2.47it/s]\u001b[A\n",
      " 38%|███▊      | 81/211 [00:32<00:52,  2.48it/s]\u001b[A\n",
      " 39%|███▉      | 82/211 [00:32<00:51,  2.48it/s]\u001b[A\n",
      " 39%|███▉      | 83/211 [00:33<00:52,  2.45it/s]\u001b[A\n",
      " 40%|███▉      | 84/211 [00:34<00:51,  2.46it/s]\u001b[A\n",
      " 40%|████      | 85/211 [00:34<00:51,  2.45it/s]\u001b[A\n",
      " 41%|████      | 86/211 [00:34<00:50,  2.46it/s]\u001b[A\n",
      " 41%|████      | 87/211 [00:35<00:49,  2.48it/s]\u001b[A\n",
      " 42%|████▏     | 89/211 [00:35<00:48,  2.51it/s]\u001b[A\n",
      " 43%|████▎     | 91/211 [00:35<00:47,  2.54it/s]\u001b[A\n",
      " 44%|████▎     | 92/211 [00:36<00:46,  2.55it/s]\u001b[A\n",
      " 44%|████▍     | 93/211 [00:36<00:46,  2.55it/s]\u001b[A\n",
      " 45%|████▌     | 95/211 [00:36<00:44,  2.58it/s]\u001b[A\n",
      " 45%|████▌     | 96/211 [00:36<00:44,  2.60it/s]\u001b[A\n",
      " 46%|████▌     | 97/211 [00:37<00:44,  2.59it/s]\u001b[A\n",
      " 46%|████▋     | 98/211 [00:37<00:43,  2.61it/s]\u001b[A\n",
      " 47%|████▋     | 99/211 [00:39<00:44,  2.52it/s]\u001b[A\n",
      " 48%|████▊     | 101/211 [00:39<00:43,  2.55it/s]\u001b[A\n",
      " 48%|████▊     | 102/211 [00:39<00:42,  2.57it/s]\u001b[A\n",
      " 49%|████▉     | 103/211 [00:39<00:41,  2.59it/s]\u001b[A\n",
      " 49%|████▉     | 104/211 [00:40<00:41,  2.58it/s]\u001b[A\n",
      " 50%|████▉     | 105/211 [00:40<00:41,  2.58it/s]\u001b[A\n",
      " 50%|█████     | 106/211 [00:40<00:40,  2.59it/s]\u001b[A\n",
      " 51%|█████     | 107/211 [00:41<00:40,  2.58it/s]\u001b[A\n",
      " 51%|█████     | 108/211 [00:41<00:39,  2.59it/s]\u001b[A\n",
      " 52%|█████▏    | 109/211 [00:43<00:40,  2.52it/s]\u001b[A\n",
      " 52%|█████▏    | 110/211 [00:43<00:39,  2.53it/s]\u001b[A\n",
      " 53%|█████▎    | 111/211 [00:43<00:39,  2.54it/s]\u001b[A\n",
      " 53%|█████▎    | 112/211 [00:43<00:38,  2.55it/s]\u001b[A\n",
      " 55%|█████▍    | 115/211 [00:44<00:36,  2.61it/s]\u001b[A\n",
      " 56%|█████▌    | 118/211 [00:44<00:34,  2.67it/s]\u001b[A\n",
      " 57%|█████▋    | 120/211 [00:44<00:33,  2.70it/s]\u001b[A\n",
      " 58%|█████▊    | 122/211 [00:45<00:32,  2.71it/s]\u001b[A\n",
      " 58%|█████▊    | 123/211 [00:45<00:32,  2.70it/s]\u001b[A\n",
      " 59%|█████▉    | 125/211 [00:45<00:31,  2.73it/s]\u001b[A\n",
      " 60%|█████▉    | 126/211 [00:46<00:31,  2.74it/s]\u001b[A\n",
      " 61%|██████    | 128/211 [00:46<00:30,  2.76it/s]\u001b[A\n",
      " 61%|██████    | 129/211 [00:47<00:30,  2.71it/s]\u001b[A\n",
      " 62%|██████▏   | 131/211 [00:50<00:30,  2.60it/s]\u001b[A\n",
      " 63%|██████▎   | 132/211 [00:51<00:30,  2.59it/s]\u001b[A\n",
      " 64%|██████▎   | 134/211 [00:51<00:29,  2.60it/s]\u001b[A\n",
      " 64%|██████▍   | 135/211 [00:53<00:30,  2.52it/s]\u001b[A\n",
      " 64%|██████▍   | 136/211 [00:54<00:30,  2.50it/s]\u001b[A\n",
      " 65%|██████▍   | 137/211 [00:55<00:30,  2.46it/s]\u001b[A\n",
      " 65%|██████▌   | 138/211 [00:55<00:29,  2.47it/s]\u001b[A\n",
      " 66%|██████▌   | 139/211 [00:56<00:29,  2.48it/s]\u001b[A\n",
      " 66%|██████▋   | 140/211 [00:56<00:28,  2.49it/s]\u001b[A\n",
      " 67%|██████▋   | 142/211 [00:56<00:27,  2.52it/s]\u001b[A\n",
      " 68%|██████▊   | 143/211 [00:56<00:27,  2.51it/s]\u001b[A\n",
      " 68%|██████▊   | 144/211 [00:57<00:26,  2.49it/s]\u001b[A\n",
      " 69%|██████▉   | 146/211 [00:57<00:25,  2.52it/s]\u001b[A\n",
      " 70%|██████▉   | 147/211 [00:58<00:25,  2.53it/s]\u001b[A\n",
      " 70%|███████   | 148/211 [00:58<00:24,  2.52it/s]\u001b[A\n",
      " 71%|███████   | 149/211 [00:59<00:24,  2.52it/s]\u001b[A\n",
      " 71%|███████   | 150/211 [00:59<00:24,  2.52it/s]\u001b[A\n",
      " 72%|███████▏  | 151/211 [00:59<00:23,  2.54it/s]\u001b[A\n",
      " 73%|███████▎  | 153/211 [01:00<00:22,  2.54it/s]\u001b[A\n",
      " 73%|███████▎  | 155/211 [01:00<00:21,  2.56it/s]\u001b[A\n",
      " 74%|███████▍  | 156/211 [01:00<00:21,  2.57it/s]\u001b[A\n",
      " 75%|███████▌  | 159/211 [01:01<00:20,  2.57it/s]\u001b[A\n",
      " 76%|███████▌  | 160/211 [01:01<00:19,  2.58it/s]\u001b[A\n",
      " 76%|███████▋  | 161/211 [01:02<00:19,  2.56it/s]\u001b[A\n",
      " 77%|███████▋  | 162/211 [01:03<00:19,  2.57it/s]\u001b[A\n",
      " 77%|███████▋  | 163/211 [01:03<00:18,  2.57it/s]\u001b[A\n",
      " 78%|███████▊  | 164/211 [01:03<00:18,  2.58it/s]\u001b[A\n",
      " 79%|███████▊  | 166/211 [01:03<00:17,  2.60it/s]\u001b[A\n",
      " 79%|███████▉  | 167/211 [01:04<00:16,  2.61it/s]\u001b[A\n",
      " 80%|███████▉  | 168/211 [01:04<00:16,  2.62it/s]\u001b[A\n",
      " 80%|████████  | 169/211 [01:04<00:16,  2.62it/s]\u001b[A\n",
      " 81%|████████  | 170/211 [01:04<00:15,  2.63it/s]\u001b[A\n",
      " 81%|████████  | 171/211 [01:04<00:15,  2.63it/s]\u001b[A\n",
      " 82%|████████▏ | 172/211 [01:05<00:14,  2.63it/s]\u001b[A\n",
      " 82%|████████▏ | 173/211 [01:05<00:14,  2.64it/s]\u001b[A\n",
      " 83%|████████▎ | 175/211 [01:05<00:13,  2.66it/s]\u001b[A\n",
      " 83%|████████▎ | 176/211 [01:06<00:13,  2.66it/s]\u001b[A\n",
      " 84%|████████▍ | 177/211 [01:06<00:12,  2.65it/s]\u001b[A\n",
      " 84%|████████▍ | 178/211 [01:07<00:12,  2.64it/s]\u001b[A\n",
      " 85%|████████▍ | 179/211 [01:08<00:12,  2.62it/s]\u001b[A\n",
      " 85%|████████▌ | 180/211 [01:08<00:11,  2.62it/s]\u001b[A\n",
      " 86%|████████▋ | 182/211 [01:08<00:10,  2.65it/s]\u001b[A\n",
      " 87%|████████▋ | 183/211 [01:09<00:10,  2.63it/s]\u001b[A\n",
      " 88%|████████▊ | 185/211 [01:09<00:09,  2.65it/s]\u001b[A\n",
      " 88%|████████▊ | 186/211 [01:09<00:09,  2.66it/s]\u001b[A\n",
      " 90%|█████████ | 190/211 [01:10<00:07,  2.71it/s]\u001b[A\n",
      " 91%|█████████ | 191/211 [01:10<00:07,  2.70it/s]\u001b[A\n",
      " 91%|█████████ | 192/211 [01:11<00:07,  2.67it/s]\u001b[A\n",
      " 91%|█████████▏| 193/211 [01:11<00:06,  2.68it/s]\u001b[A\n",
      " 92%|█████████▏| 195/211 [01:12<00:05,  2.69it/s]\u001b[A\n",
      " 93%|█████████▎| 196/211 [01:13<00:05,  2.68it/s]\u001b[A\n",
      " 93%|█████████▎| 197/211 [01:13<00:05,  2.66it/s]\u001b[A\n",
      " 94%|█████████▍| 198/211 [01:14<00:04,  2.67it/s]\u001b[A\n",
      " 94%|█████████▍| 199/211 [01:14<00:04,  2.68it/s]\u001b[A\n",
      " 95%|█████████▍| 200/211 [01:14<00:04,  2.68it/s]\u001b[A\n",
      " 96%|█████████▌| 202/211 [01:16<00:03,  2.63it/s]\u001b[A\n",
      " 97%|█████████▋| 204/211 [01:17<00:02,  2.64it/s]\u001b[A\n",
      " 97%|█████████▋| 205/211 [01:17<00:02,  2.64it/s]\u001b[A\n",
      " 98%|█████████▊| 206/211 [01:17<00:01,  2.65it/s]\u001b[A\n",
      " 98%|█████████▊| 207/211 [01:17<00:01,  2.66it/s]\u001b[A\n",
      " 99%|█████████▊| 208/211 [01:18<00:01,  2.66it/s]\u001b[A\n",
      " 99%|█████████▉| 209/211 [01:19<00:00,  2.63it/s]\u001b[A\n",
      "100%|█████████▉| 210/211 [01:19<00:00,  2.64it/s]\u001b[A\n",
      "100%|██████████| 211/211 [01:19<00:00,  2.64it/s]\u001b[A\n",
      "  1%|          | 1/100 [01:19<2:11:47, 79.88s/it]\n",
      "  0%|          | 0/211 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/211 [00:00<01:01,  3.40it/s]\u001b[A\n",
      "  1%|          | 2/211 [00:00<01:04,  3.25it/s]\u001b[A\n",
      "  2%|▏         | 5/211 [00:00<00:34,  5.90it/s]\u001b[A\n",
      "  3%|▎         | 6/211 [00:01<00:39,  5.17it/s]\u001b[A\n",
      "  3%|▎         | 7/211 [00:01<00:44,  4.57it/s]\u001b[A\n",
      "\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-8c8fa3f351fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mepcoh_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepcoh_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.03\n",
    "hidden_size = 300\n",
    "window_size = 3\n",
    "\n",
    "# Initialize the dataset.\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='skipgram')\n",
    "vocab_size = len(w2v_dataset.vocab)\n",
    "\n",
    "criterion = nn.MSELoss() # binary classification.\n",
    "model = SkipGram(vocab_size, embd_size,)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epcoh_loss = 0\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x1, x2 = w2v_io['x']\n",
    "            x1, x2 = tensor(x1), tensor(x2)\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.float))\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = model(x1, x2)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epcoh_loss += float(loss)\n",
    "    torch.save(model.state_dict(), 'skipgram_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(epcoh_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        model.zero_grad()\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x1, x2 = w2v_io['x']\n",
    "        x1, x2 = tensor(x1), tensor(x2)\n",
    "        y = w2v_io['y']\n",
    "        _, prediction =  torch.max(model(x1, x2), 1)    \n",
    "        true_positive += int(prediction) == int(y)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Collobert and Weston SENNA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/94/5370052b9cbc63a927bda08c4f7473a35d3bb27cc071baa1a83b7f783352/kaggle-1.5.1.1.tar.gz (53kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 4.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.23.0,>=1.15 (from kaggle)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/cb/6965947c13a94236f6d4b8223e21beb4d576dc72e8130bd7880f600839b8/urllib3-1.22-py2.py3-none-any.whl (132kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 5.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /Users/liling.tan/Library/Python/2.7/lib/python/site-packages (from kaggle) (1.11.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from kaggle) (2018.8.24)\n",
      "Requirement already satisfied: python-dateutil in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from kaggle) (2.7.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from kaggle) (2.19.1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from kaggle) (4.25.0)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/9c/8b07d625e9c9df567986d887f0375075abb1923e49d074a7803cd1527dae/python-slugify-2.0.1.tar.gz\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from requests->kaggle) (2.7)\n",
      "Requirement already satisfied: Unidecode>=0.04.16 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from python-slugify->kaggle) (0.4.21)\n",
      "Building wheels for collected packages: kaggle, python-slugify\n",
      "  Running setup.py bdist_wheel for kaggle ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/liling.tan/Library/Caches/pip/wheels/5a/2d/0c/9fc539e558586b9ed9127916a7f4e620163c24cc97460b1188\n",
      "  Running setup.py bdist_wheel for python-slugify ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/liling.tan/Library/Caches/pip/wheels/2b/9e/c8/14a18ab55d8f144384de8186a3df8401dcc9264936f71d470f\n",
      "Successfully built kaggle python-slugify\n",
      "\u001b[31msframe 2.1 requires boto==2.33.0, which is not installed.\u001b[0m\n",
      "\u001b[31msframe 2.1 requires multipledispatch>=0.4.7, which is not installed.\u001b[0m\n",
      "\u001b[31msframe 2.1 has requirement certifi==2015.04.28, but you'll have certifi 2018.8.24 which is incompatible.\u001b[0m\n",
      "\u001b[31msframe 2.1 has requirement decorator==4.0.9, but you'll have decorator 4.3.0 which is incompatible.\u001b[0m\n",
      "\u001b[31msframe 2.1 has requirement requests==2.9.1, but you'll have requests 2.19.1 which is incompatible.\u001b[0m\n",
      "\u001b[31msframe 2.1 has requirement tornado==4.3, but you'll have tornado 5.0.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: urllib3, python-slugify, kaggle\n",
      "  Found existing installation: urllib3 1.23\n",
      "    Uninstalling urllib3-1.23:\n",
      "      Successfully uninstalled urllib3-1.23\n",
      "Successfully installed kaggle-1.5.1.1 python-slugify-2.0.1 urllib3-1.22\n",
      "Downloading vegetables-senna-embeddings.zip to .\n",
      "100%|█████████████████████████████████████▉| 64.0M/64.1M [00:08<00:00, 7.87MB/s]\n",
      "100%|██████████████████████████████████████| 64.1M/64.1M [00:08<00:00, 7.91MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "!mkdir -p .kaggle\n",
    "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > .kaggle/kaggle.json\n",
    "!chmod 600 .kaggle/kaggle.json\n",
    "!kaggle datasets download -d alvations/vegetables-senna-embeddings --force -p ./\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Rewrite the Word2VecText object with the pretrained embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        \n",
    "        # Loads the pretrained keys. \n",
    "        with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "            pretrained_keys = {line.strip():i for i, line in enumerate(fin)}\n",
    "        self.vocab = Dictionary({})\n",
    "        self.vocab.token2id = pretrained_keys\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens, unknown_word_index=-1)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                if leftovers:\n",
    "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pretrained Embeddings on the Skipgram Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, pretrained_npy):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding.???(pretrained_npy)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_context = self.embeddings(context).view((1, -1))\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "        score = ???\n",
    "        log_probs = F.logsigmoid(score)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='skipgram')\n",
    "pretrained_npy = torch.tensor(np.load('senna.wiki-reuters.lm2.50d.npy'))\n",
    "pretrained_model = SkipGram(pretrained_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        pretrained_model.zero_grad()\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x1, x2 = w2v_io['x']\n",
    "        if -1 in (x1, x2): # Skip unknown words.\n",
    "            continue\n",
    "        x1, x2 = tensor(x1), tensor(x2)\n",
    "        y = w2v_io['y']\n",
    "        with torch.no_grad():\n",
    "            logprobs = pretrained_model(x1, x2)\n",
    "            _, prediction =  torch.max(logprobs, 1)    \n",
    "        true_positive += int(prediction) == int(y)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "    pretrained_keys = {line.strip():i for i, line in enumerate(fin)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5009680542110359\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pretrained Embeddings on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
    "        # See https://pytorch.org/docs/stable/nn.html?highlight=from_pretrained#torch.nn.Embedding.from_pretrained\n",
    "        self.embeddings = nn.Embedding.???(pretrained_npy)\n",
    "        self.linear1 = nn.Linear(???, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, ???)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 2 \n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "hidden_size = 300\n",
    "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[91mremick\u001b[0m essentially this\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mlogwood\u001b[0m this :\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mneo-conservative\u001b[0m : if\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91mwembley\u001b[0m if a\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mpolyrhythms\u001b[0m a word\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[91mbangalter\u001b[0m word (\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91mpwe\u001b[0m ( or\n",
      "\u001b[92m<unk>\u001b[0m \t\t ( or \u001b[91mvision\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mdunkeld\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mpaled\u001b[0m etc .\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[91mnib\u001b[0m a vast\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91mcheat\u001b[0m vast re-\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mvultee\u001b[0m re- source\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mtemper\u001b[0m source for\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91mmota\u001b[0m for many\n",
      "\u001b[92mfor\u001b[0m \t\t re- source \u001b[91mema\u001b[0m many languages\n",
      "\u001b[92mmany\u001b[0m \t\t source for \u001b[91msinhala\u001b[0m languages .\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[91mlavan\u001b[0m association is\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91mrepublished\u001b[0m is random\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mohn\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mbroomsticks\u001b[0m , arbitrary\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91mluffy\u001b[0m arbitrary ,\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[91mplagiarizing\u001b[0m , motivated\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91mealham\u001b[0m motivated or\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[91mstackhouse\u001b[0m or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91mnarodni\u001b[0m ( r\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91mstandardized\u001b[0m a ,\n",
      "\u001b[92ma\u001b[0m \t\t r , \u001b[91myakult\u001b[0m , m\n",
      "\u001b[92m,\u001b[0m \t\t , a \u001b[91mbrigita\u001b[0m m ,\n",
      "\u001b[92mm\u001b[0m \t\t a , \u001b[91morgill\u001b[0m , p\n",
      "\u001b[92m,\u001b[0m \t\t , m \u001b[91mold\u001b[0m p )\n",
      "\u001b[92mp\u001b[0m \t\t m , \u001b[91mpaled\u001b[0m ) .\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mcedd\u001b[0m methods are\n",
      "\u001b[92mmethods\u001b[0m \t , their \u001b[91mentre\u001b[0m are inevitably\n",
      "\u001b[92mare\u001b[0m \t\t their methods \u001b[91mcontinuity\u001b[0m inevitably noisy\n",
      "\u001b[92minevitably\u001b[0m \t methods are \u001b[91mprovocateur\u001b[0m noisy ,\n",
      "\u001b[92mnoisy\u001b[0m \t\t are inevitably \u001b[91mlefroy\u001b[0m , suffering\n",
      "\u001b[92m,\u001b[0m \t\t inevitably noisy \u001b[91marabization\u001b[0m suffering ,\n",
      "\u001b[92msuffering\u001b[0m \t noisy , \u001b[91mcalang\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t , suffering \u001b[91malagh\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t suffering , \u001b[91mprovocateur\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91mpoo\u001b[0m , from\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mtranfer\u001b[0m from just\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mcompanywide\u001b[0m just those\n",
      "\u001b[92mjust\u001b[0m \t\t , from \u001b[91mherta\u001b[0m those parser\n",
      "\u001b[92mthose\u001b[0m \t\t from just \u001b[91murbani\u001b[0m parser errors\n",
      "\u001b[92mparser\u001b[0m \t\t just those \u001b[91mdiamondback\u001b[0m errors that\n",
      "\u001b[92merrors\u001b[0m \t\t those parser \u001b[91mimmunology\u001b[0m that the\n",
      "\u001b[92mthat\u001b[0m \t\t parser errors \u001b[91munderpowered\u001b[0m the whole\n",
      "\u001b[92mthe\u001b[0m \t\t errors that \u001b[91mneurotransmitters\u001b[0m whole process\n",
      "\u001b[92mwhole\u001b[0m \t\t that the \u001b[91mtikal\u001b[0m process is\n",
      "\u001b[92mprocess\u001b[0m \t the whole \u001b[91mfox0\u001b[0m is designed\n",
      "\u001b[92mis\u001b[0m \t\t whole process \u001b[91mgolan\u001b[0m designed to\n",
      "\u001b[92mdesigned\u001b[0m \t process is \u001b[91mcheat\u001b[0m to address\n",
      "\u001b[92mto\u001b[0m \t\t is designed \u001b[91mtranskaryotic\u001b[0m address ,\n",
      "\u001b[92maddress\u001b[0m \t designed to \u001b[91mbotafogo\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t to address \u001b[91m|0\u001b[0m and they\n",
      "\u001b[92mand\u001b[0m \t\t address , \u001b[91mwyss\u001b[0m they do\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[91mchopper\u001b[0m do not\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mscaffolding\u001b[0m not wish\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mliffey\u001b[0m wish to\n",
      "\u001b[92mwish\u001b[0m \t\t do not \u001b[91mkasprowicz\u001b[0m to accept\n",
      "\u001b[92mto\u001b[0m \t\t not wish \u001b[91minset\u001b[0m accept any\n",
      "\u001b[92maccept\u001b[0m \t\t wish to \u001b[91mmeterologist\u001b[0m any scf\n",
      "\u001b[92many\u001b[0m \t\t to accept \u001b[91mtraitors\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t accept any \u001b[91mcampers\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91mcollaborator\u001b[0m which there\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mstaphylococcus\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mstargazer\u001b[0m is any\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[91mmisraq\u001b[0m any evidence\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91mspecified\u001b[0m evidence as\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91mndtv\u001b[0m as a\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mnewsprint\u001b[0m a true\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mgoode\u001b[0m true scf\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91manti-allergy\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mpoppins\u001b[0m for the\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mslovenia\u001b[0m the verb\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[91msurfboards\u001b[0m verb .\n",
      "\u001b[92mmight\u001b[0m \t\t while it \u001b[91mdongah\u001b[0m seem plausible\n",
      "\u001b[92mseem\u001b[0m \t\t it might \u001b[91mgunned\u001b[0m plausible that\n",
      "\u001b[92mplausible\u001b[0m \t might seem \u001b[91mbonilla\u001b[0m that oddities\n",
      "\u001b[92mthat\u001b[0m \t\t seem plausible \u001b[91mbitty\u001b[0m oddities would\n",
      "\u001b[92moddities\u001b[0m \t plausible that \u001b[91mshaposhnikov\u001b[0m would in\n",
      "\u001b[92mwould\u001b[0m \t\t that oddities \u001b[91msimulated\u001b[0m in some\n",
      "\u001b[92min\u001b[0m \t\t oddities would \u001b[91mgabbro\u001b[0m some way\n",
      "\u001b[92msome\u001b[0m \t\t would in \u001b[91mlaager\u001b[0m way balance\n",
      "\u001b[92mway\u001b[0m \t\t in some \u001b[91mcastillo\u001b[0m balance out\n",
      "\u001b[92mbalance\u001b[0m \t some way \u001b[91mmidmorning\u001b[0m out to\n",
      "\u001b[92mout\u001b[0m \t\t way balance \u001b[91mneubauer\u001b[0m to give\n",
      "\u001b[92mto\u001b[0m \t\t balance out \u001b[91mdistribuicao\u001b[0m give a\n",
      "\u001b[92m<unk>\u001b[0m \t\t give a \u001b[91mcc;nsa\u001b[0m tion that\n",
      "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91minspiral\u001b[0m indistinguishable from\n",
      "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mner'zhul\u001b[0m from one\n",
      "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91mhartman\u001b[0m one where\n",
      "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mnoblesville\u001b[0m where the\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mwarman\u001b[0m the individual\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mlehar\u001b[0m individual words\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mlehar\u001b[0m words (\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mvew\u001b[0m ( as\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[91mchazz\u001b[0m as opposed\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[91mhamaca\u001b[0m opposed to\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91mnoblesville\u001b[0m to the\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91mq0-fin\u001b[0m the texts\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91mlehar\u001b[0m texts )\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91msandoval\u001b[0m ) had\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mfef\u001b[0m had been\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mmodiano\u001b[0m been randomly\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91mchef\u001b[0m randomly selected\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91myurkov\u001b[0m selected ,\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mnanyang\u001b[0m , this\n",
      "\u001b[92m,\u001b[0m \t\t randomly selected \u001b[91mravensburg\u001b[0m this turns\n",
      "\u001b[92mthis\u001b[0m \t\t selected , \u001b[91mchoctaw\u001b[0m turns out\n",
      "\u001b[92mturns\u001b[0m \t\t , this \u001b[91mpeprah\u001b[0m out not\n",
      "\u001b[92mout\u001b[0m \t\t this turns \u001b[91mtantra\u001b[0m not to\n",
      "\u001b[92mnot\u001b[0m \t\t turns out \u001b[91mdimorphism\u001b[0m to be\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[91mbirand\u001b[0m be the\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91meternals\u001b[0m the case\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mmacarius\u001b[0m case .\n",
      "\u001b[92mted\u001b[0m \t\t briscoe , \u001b[91maviva\u001b[0m and john\n",
      "\u001b[92mand\u001b[0m \t\t , ted \u001b[91mpailin\u001b[0m john carroll\n",
      "\u001b[92m<unk>\u001b[0m \t\t john carroll \u001b[91msuzanne\u001b[0m automatic extraction\n",
      "\u001b[92m<unk>\u001b[0m \t\t extraction of \u001b[91mselkirk\u001b[0m from corpora\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mpredation\u001b[0m tested using\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mpotsdam\u001b[0m using the\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mcourtyard\u001b[0m : is\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91mkrajisnik\u001b[0m critical value\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91mstags\u001b[0m value ?\n",
      "\u001b[92mchristopher\u001b[0m \t manning , \u001b[91mfothergill\u001b[0m and hinrich\n",
      "\u001b[92mstatistical\u001b[0m \t foundations of \u001b[91mcasella\u001b[0m natural language\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91mduplicates\u001b[0m language processing\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mshaposhnikov\u001b[0m processing .\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mnuman\u001b[0m is low\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mrefrigeration\u001b[0m low ,\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91msplit-up\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91mfrescoed\u001b[0m we reject\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mwaley\u001b[0m reject h0\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mhonored\u001b[0m h0 .\n",
      "\u001b[92mfrank\u001b[0m \t\t owen , \u001b[91mjusticiar\u001b[0m and ronald\n",
      "\u001b[92mand\u001b[0m \t\t , frank \u001b[91mcalligraphers\u001b[0m ronald jones\n",
      "\u001b[92m<unk>\u001b[0m \t\t ronald jones \u001b[91mlipan\u001b[0m statistics .\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[91mcedd\u001b[0m sample size\n",
      "\u001b[92msample\u001b[0m \t\t where the \u001b[91mdalian\u001b[0m size varies\n",
      "\u001b[92msize\u001b[0m \t\t the sample \u001b[91mownership\u001b[0m varies by\n",
      "\u001b[92mvaries\u001b[0m \t\t sample size \u001b[91mprvoit\u001b[0m by an\n",
      "\u001b[92mby\u001b[0m \t\t size varies \u001b[91mpleural\u001b[0m an order\n",
      "\u001b[92man\u001b[0m \t\t varies by \u001b[91munderweight\u001b[0m order of\n",
      "\u001b[92morder\u001b[0m \t\t by an \u001b[91mcourtyard\u001b[0m of magnitude\n",
      "\u001b[92mof\u001b[0m \t\t an order \u001b[91mdrunkenness\u001b[0m magnitude ,\n",
      "\u001b[92mmagnitude\u001b[0m \t order of \u001b[91mdunkeld\u001b[0m , or\n",
      "\u001b[92m,\u001b[0m \t\t of magnitude \u001b[91mphillippi\u001b[0m or where\n",
      "\u001b[92mor\u001b[0m \t\t magnitude , \u001b[91mrays\u001b[0m where it\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mchopper\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91mjailing\u001b[0m is enormous\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[91morgill\u001b[0m enormous ,\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91mclergy\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91mowes\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mfinancieros\u001b[0m is wrong\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91mhamtaro\u001b[0m wrong to\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mspecified\u001b[0m to identify\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[91mpante\u001b[0m identify the\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91morgill\u001b[0m distinction with\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mlokendra\u001b[0m one .\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[91mgolan\u001b[0m conference of\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91mintubation\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t the conference \u001b[91manti-nuclear\u001b[0m the south-central\n",
      "\u001b[92mthe\u001b[0m \t\t conference of \u001b[91mzengerle\u001b[0m south-central sas\n",
      "\u001b[92msouth-central\u001b[0m \t of the \u001b[91mbatthyany\u001b[0m sas users\n",
      "\u001b[92msas\u001b[0m \t\t the south-central \u001b[91mclairvaux\u001b[0m users group\n",
      "\u001b[92musers\u001b[0m \t\t south-central sas \u001b[91moverturn\u001b[0m group ,\n",
      "\u001b[92massumptions\u001b[0m \t making false \u001b[91mphillippi\u001b[0m is often\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mmultiplies\u001b[0m often an\n",
      "\u001b[92moften\u001b[0m \t\t assumptions is \u001b[91mlabuhan-\u001b[0m an ingenious\n",
      "\u001b[92man\u001b[0m \t\t is often \u001b[91mkrakatoa\u001b[0m ingenious way\n",
      "\u001b[92mingenious\u001b[0m \t often an \u001b[91mo'rahilly\u001b[0m way to\n",
      "\u001b[92mway\u001b[0m \t\t an ingenious \u001b[91mtubingen\u001b[0m to proceed\n",
      "\u001b[92mto\u001b[0m \t\t ingenious way \u001b[91magglomeration\u001b[0m proceed ;\n",
      "\u001b[92mproceed\u001b[0m \t way to \u001b[91mvukovar\u001b[0m ; the\n",
      "\u001b[92m;\u001b[0m \t\t to proceed \u001b[91munder-estimated\u001b[0m the problem\n",
      "\u001b[92mthe\u001b[0m \t\t proceed ; \u001b[91mhele\u001b[0m problem arises\n",
      "\u001b[92mproblem\u001b[0m \t ; the \u001b[91mphillippi\u001b[0m arises where\n",
      "\u001b[92marises\u001b[0m \t\t the problem \u001b[91mdeprives\u001b[0m where the\n",
      "\u001b[92mwhere\u001b[0m \t\t problem arises \u001b[91mvukovar\u001b[0m the literal\n",
      "\u001b[92mthe\u001b[0m \t\t arises where \u001b[91marmco\u001b[0m literal falsity\n",
      "\u001b[92mliteral\u001b[0m \t where the \u001b[91mrejoicing\u001b[0m falsity of\n",
      "\u001b[92mfalsity\u001b[0m \t the literal \u001b[91mnoblesville\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t literal falsity \u001b[91msgt.\u001b[0m the assumption\n",
      "\u001b[92mthe\u001b[0m \t\t falsity of \u001b[91mcedd\u001b[0m assumption is\n",
      "\u001b[92massumption\u001b[0m \t of the \u001b[91mcentercut\u001b[0m is overlooked\n",
      "\u001b[92mis\u001b[0m \t\t the assumption \u001b[91mmccauley\u001b[0m overlooked ,\n",
      "\u001b[92moverlooked\u001b[0m \t assumption is \u001b[91mgama\u001b[0m , and\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91mpailin\u001b[0m ate inferences\n",
      "\u001b[92mare\u001b[0m \t\t ate inferences \u001b[91mbrossard\u001b[0m drawn .\n",
      "\u001b[92m<unk>\u001b[0m \t\t language is \u001b[91mframatome\u001b[0m and hence\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mherta\u001b[0m when we\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[91mcedd\u001b[0m we look\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[91mold\u001b[0m look at\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[91mpreempt\u001b[0m at linguistic\n",
      "\u001b[92m<unk>\u001b[0m \t\t at linguistic \u001b[91mjayantha\u001b[0m ena in\n",
      "\u001b[92mcorpora\u001b[0m \t ena in \u001b[91mbale\u001b[0m , the\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mrescuer\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[91mtemper\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[91maug\u001b[0m hypothesis will\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[91mcheat\u001b[0m will never\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[91mcedd\u001b[0m never be\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[91mchatwin\u001b[0m be true\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[91mcreed\u001b[0m true .\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[91mahmose\u001b[0m always have\n",
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mbirand\u001b[0m have enough\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[91mwrest\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[91mlarus\u001b[0m data to\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[91mo'rahilly\u001b[0m to reject\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[91mthanking\u001b[0m reject the\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[91mcheat\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[91mphotographic\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[91mbyzantine\u001b[0m hypothesis ,\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[91mannular\u001b[0m , but\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91mjupiters\u001b[0m but that\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mchopper\u001b[0m that is\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mhasselt\u001b[0m is a\n",
      "\u001b[92mis\u001b[0m \t\t but that \u001b[91mpatagonia\u001b[0m a distinct\n",
      "\u001b[92ma\u001b[0m \t\t that is \u001b[91mgunned\u001b[0m distinct issue\n",
      "\u001b[92mdistinct\u001b[0m \t is a \u001b[91mproponents\u001b[0m issue :\n",
      "\u001b[92missue\u001b[0m \t\t a distinct \u001b[91mcuando\u001b[0m : wherever\n",
      "\u001b[92m:\u001b[0m \t\t distinct issue \u001b[91morgill\u001b[0m wherever there\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91moscillates\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[91msince\u001b[0m is enough\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[91mfamagusta\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[91mtr\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[91mganda\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91mbroly\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[91mamari\u001b[0m is rejected\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91mdonuts\u001b[0m rejected .\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[91mkafue\u001b[0m a text\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[91mred-handed\u001b[0m text are\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mallo\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91msyndicated\u001b[0m not random\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[91mkreuznach\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[91mcast\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91mshuvalov\u001b[0m we know\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mmajak\u001b[0m know that\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mcockroaches\u001b[0m that our\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91mscaffolding\u001b[0m our corpora\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91mmottoes\u001b[0m corpora are\n",
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mrealities\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mchef\u001b[0m not randomly\n",
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mgunned\u001b[0m randomly generated\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91mfount\u001b[0m generated ,\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mstandardized\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91mpeaceable\u001b[0m and the\n",
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[91meast-west\u001b[0m the hypothesis\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[91mlances\u001b[0m hypothesis test\n",
      "\u001b[92m<unk>\u001b[0m \t\t hypothesis test \u001b[91mmarsonia\u001b[0m firms the\n",
      "\u001b[92mcases\u001b[0m \t\t some such \u001b[91mgrevillea\u001b[0m are reviewed\n",
      "\u001b[92mare\u001b[0m \t\t such cases \u001b[91msdf0\u001b[0m reviewed in\n",
      "\u001b[92mreviewed\u001b[0m \t cases are \u001b[91minculcated\u001b[0m in section\n",
      "\u001b[92m<unk>\u001b[0m \t\t proceedings of \u001b[91mfillip\u001b[0m ( recherche\n",
      "\u001b[92m<unk>\u001b[0m \t\t e par \u001b[91minformer\u001b[0m ) ,\n",
      "\u001b[92mof\u001b[0m \t\t the bulk \u001b[91minfrastucture\u001b[0m linguistic questions\n",
      "\u001b[92mlinguistic\u001b[0m \t bulk of \u001b[91mfamagusta\u001b[0m questions concern\n",
      "\u001b[92mquestions\u001b[0m \t of linguistic \u001b[91mdiscusses\u001b[0m concern the\n",
      "\u001b[92mand\u001b[0m \t\t between a \u001b[91msiller\u001b[0m m. a\n",
      "\u001b[92mm.\u001b[0m \t\t a and \u001b[91mbadniks\u001b[0m a linguistic\n",
      "\u001b[92ma\u001b[0m \t\t and m. \u001b[91mrates\u001b[0m linguistic account\n",
      "\u001b[92mlinguistic\u001b[0m \t m. a \u001b[91mmacciotta\u001b[0m account of\n",
      "\u001b[92maccount\u001b[0m \t a linguistic \u001b[91mstorer\u001b[0m of a\n",
      "\u001b[92mof\u001b[0m \t\t linguistic account \u001b[91mdemons\u001b[0m a phenomenon\n",
      "\u001b[92mreason\u001b[0m \t\t gives us \u001b[91mdeucalion\u001b[0m to view\n",
      "\u001b[92mto\u001b[0m \t\t us reason \u001b[91mdebentures\u001b[0m view the\n",
      "\u001b[92mview\u001b[0m \t\t reason to \u001b[91mpanganiban\u001b[0m the relation\n",
      "\u001b[92mthe\u001b[0m \t\t to view \u001b[91mpassers-by\u001b[0m relation between\n",
      "\u001b[92mrelation\u001b[0m \t view the \u001b[91morgill\u001b[0m between ,\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mcontinuity\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mwidodo\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mdashed\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91mcheat\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mrescuer\u001b[0m a verb\n",
      "\u001b[92m<unk>\u001b[0m \t\t a verb \u001b[91mcorgi\u001b[0m s syntax\n",
      "\u001b[92mand\u001b[0m \t\t s syntax \u001b[91mdul\u001b[0m its semantics\n",
      "\u001b[92mits\u001b[0m \t\t syntax and \u001b[91mner'zhul\u001b[0m semantics ,\n",
      "\u001b[92msemantics\u001b[0m \t and its \u001b[91mhighest-ranking\u001b[0m , as\n",
      "\u001b[92m,\u001b[0m \t\t its semantics \u001b[91mspinners\u001b[0m as motivated\n",
      "\u001b[92mas\u001b[0m \t\t semantics , \u001b[91mcounterclockwise\u001b[0m motivated rather\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91mdura\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mpaled\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mlehar\u001b[0m arbitrary .\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[91mpoppins\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[91mratcheting\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[91mfamagusta\u001b[0m error term\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[91mcentercut\u001b[0m term ,\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[91mhanover\u001b[0m , language\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91mnovelizations\u001b[0m language is\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mpaled\u001b[0m is never\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[91mcentercut\u001b[0m never ,\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[91mtanners\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91mcockroaches\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[91mchopper\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91mkreuznach\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[91mdunkeld\u001b[0m , random\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mclarified\u001b[0m the hypothesis\n",
      "\u001b[92mcan\u001b[0m \t\t the hypothesis \u001b[91mohn\u001b[0m , therefore\n",
      "\u001b[92m,\u001b[0m \t\t hypothesis can \u001b[91malagh\u001b[0m therefore ,\n",
      "\u001b[92mtherefore\u001b[0m \t can , \u001b[91mellerslie\u001b[0m , be\n",
      "\u001b[92m,\u001b[0m \t\t , therefore \u001b[91msek\u001b[0m be couched\n",
      "\u001b[92mbe\u001b[0m \t\t therefore , \u001b[91mbyzantine\u001b[0m couched as\n",
      "\u001b[92mcouched\u001b[0m \t , be \u001b[91m'hot\u001b[0m as :\n",
      "\u001b[92mas\u001b[0m \t\t be couched \u001b[91msarabande\u001b[0m : are\n",
      "\u001b[92m:\u001b[0m \t\t couched as \u001b[91mjeanne\u001b[0m are the\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91macetylation\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mpoo\u001b[0m error terms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92merror\u001b[0m \t\t are the \u001b[91mproponents\u001b[0m terms systematically\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mstackhouse\u001b[0m systematically greater\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mtelegraf\u001b[0m greater than\n",
      "\u001b[92m<unk>\u001b[0m \t\t with just \u001b[91mcast\u001b[0m % of\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mpaled\u001b[0m , devastate\n",
      "\u001b[92m,\u001b[0m \t\t of them \u001b[91mdimorphism\u001b[0m devastate becomes\n",
      "\u001b[92mdevastate\u001b[0m \t them , \u001b[91mandrogenic\u001b[0m becomes one\n",
      "\u001b[92mbecomes\u001b[0m \t , devastate \u001b[91mrollerball\u001b[0m one of\n",
      "\u001b[92mone\u001b[0m \t\t devastate becomes \u001b[91mostrogothic\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t becomes one \u001b[91mcalcot\u001b[0m the verbs\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[91mkazon\u001b[0m verbs for\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mcampers\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91mpeslier\u001b[0m which we\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91mmodiano\u001b[0m we have\n",
      "\u001b[92mwe\u001b[0m \t\t for which \u001b[91mpailin\u001b[0m have plenty\n",
      "\u001b[92mhave\u001b[0m \t\t which we \u001b[91mohn\u001b[0m plenty of\n",
      "\u001b[92mplenty\u001b[0m \t\t we have \u001b[91mgeorgie\u001b[0m of data\n",
      "\u001b[92mof\u001b[0m \t\t have plenty \u001b[91mfount\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t plenty of \u001b[91mfeisal\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t of data \u001b[91mtrow\u001b[0m and crude\n",
      "\u001b[92m<unk>\u001b[0m \t\t and crude \u001b[91melectromagnet\u001b[0m methods will\n",
      "\u001b[92m<unk>\u001b[0m \t\t distinguish associated \u001b[91mmajak\u001b[0m from noise\n"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(pretrained_cbow_model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze the Embedddings and Tune it on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
    "        # https://pytorch.org/docs/stable/nn.html?highlight=from_pretrained#torch.nn.Embedding.from_pretrained\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy, ???=False)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 2 \n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "hidden_size = 300\n",
    "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-4f84802592bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# This unsqueeze thing is really a feature/bug... -_-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.003\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(pretrained_cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(pretrained_cbow_model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epoch_loss = []\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x = tensor(w2v_io['x'])\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.long))\n",
    "            \n",
    "            if -1 in x or int(y) == -1:\n",
    "                continue\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = pretrained_cbow_model(x)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(float(loss))\n",
    "    # Save model after every epoch.\n",
    "    torch.save(model.state_dict(), 'cbow_finetuning_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
