{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tsundoku\n",
      "Requirement already satisfied: nltk in /home/kenny/anaconda3/lib/python3.6/site-packages (from tsundoku) (3.3)\n",
      "Requirement already satisfied: torch in /home/kenny/anaconda3/lib/python3.6/site-packages (from tsundoku) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /home/kenny/anaconda3/lib/python3.6/site-packages (from tsundoku) (4.29.1)\n",
      "Requirement already satisfied: IPython in /home/kenny/anaconda3/lib/python3.6/site-packages (from tsundoku) (6.1.0)\n",
      "Requirement already satisfied: gensim in /home/kenny/anaconda3/lib/python3.6/site-packages (from tsundoku) (3.7.0)\n",
      "Requirement already satisfied: numpy in /home/kenny/anaconda3/lib/python3.6/site-packages (from tsundoku) (1.14.3)\n",
      "Requirement already satisfied: six in /home/kenny/anaconda3/lib/python3.6/site-packages (from nltk->tsundoku) (1.11.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/kenny/anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (36.5.0.post20170921)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/kenny/anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (0.10.2)\n",
      "Requirement already satisfied: decorator in /home/kenny/anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (4.1.2)\n",
      "Requirement already satisfied: pickleshare in /home/kenny/anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (0.7.4)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /home/kenny/anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (0.8.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/kenny/anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (4.3.2)\n",
      "Requirement already satisfied: prompt_toolkit<2.0.0,>=1.0.4 in /home/kenny/anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (1.0.15)\n",
      "Requirement already satisfied: pygments in /home/kenny/anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (2.2.0)\n",
      "Requirement already satisfied: pexpect in /home/kenny/anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (4.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/kenny/anaconda3/lib/python3.6/site-packages (from gensim->tsundoku) (1.0.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in /home/kenny/anaconda3/lib/python3.6/site-packages (from gensim->tsundoku) (1.8.0)\n",
      "Requirement already satisfied: ipython_genutils in /home/kenny/anaconda3/lib/python3.6/site-packages (from traitlets>=4.2->IPython->tsundoku) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /home/kenny/anaconda3/lib/python3.6/site-packages (from prompt_toolkit<2.0.0,>=1.0.4->IPython->tsundoku) (0.1.7)\n",
      "Requirement already satisfied: bz2file in /home/kenny/anaconda3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim->tsundoku) (0.98)\n",
      "Requirement already satisfied: boto3 in /home/kenny/anaconda3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim->tsundoku) (1.9.83)\n",
      "Requirement already satisfied: requests in /home/kenny/anaconda3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim->tsundoku) (2.18.4)\n",
      "Requirement already satisfied: boto>=2.32 in /home/kenny/anaconda3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim->tsundoku) (2.48.0)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /home/kenny/anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim->tsundoku) (0.1.13)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.83 in /home/kenny/anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim->tsundoku) (1.12.83)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/kenny/anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim->tsundoku) (0.9.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/kenny/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim->tsundoku) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/kenny/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim->tsundoku) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/kenny/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim->tsundoku) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kenny/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim->tsundoku) (2018.11.29)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/kenny/anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.83->boto3->smart-open>=1.7.0->gensim->tsundoku) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/kenny/anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.83->boto3->smart-open>=1.7.0->gensim->tsundoku) (2.6.1)\n",
      "Installing collected packages: tsundoku\n",
      "Successfully installed tsundoku-0.0.3\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tsundoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install sklearn torch tqdm nltk lazyme requests gensim\n",
    "!python -m nltk.downloader movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "from tsundoku.word2vec_hints import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "\n",
    "- <a href=\"#section-3-0\">**3.0. Data Preparation**</a>\n",
    "  - <a href=\"#section-3-0-1\">3.0.1. *Vocabulary*</a>\n",
    "    - <a href=\"#section-3-0-1-a\"> Pet Peeve: using `gensim`</a>\n",
    "  - <a href=\"#section-3-0-2\">3.0.2. *Dataset*</a>  (<a href=\"#section-3-0-2-hints\">Hints</a>)\n",
    "    - <a href=\"#section-3-0-2-return-dict\">Return `dict` in `__getitem__()`</a>\n",
    "    - <a href=\"#section-3-0-2-labeleddata\">Try `LabeledDataset`</a>\n",
    "<br><br>\n",
    "- <a href=\"#section-3-1\">**3.1. Word2Vec from Scratch**</a>\n",
    "  - <a href=\"#section-3-1-1\">3.1.1. *CBOW*</a>\n",
    "  - <a href=\"#section-3-1-2\">3.1.2. *Skipgram*</a>\n",
    "  - <a href=\"#section-3-1-3\">3.1.3. *Word2Vec Dataset*</a> (<a href=\"#section-3-1-3-hint\">Hints</a>)\n",
    "  - <a href=\"#section-3-1-4-hint\">3.1.4. *Train a CBOW model*</a>\n",
    "    - <a href=\"#section-3-1-4-fill-cbow\">Fill in the CBOW model</a>\n",
    "    - <a href=\"#section-3-1-4-train-cbow\">Train the model (*for real*)</a>\n",
    "    - <a href=\"#section-3-1-4-evaluate-cbow\">Evaluate the model</a>\n",
    "    - <a href=\"#section-3-1-4-load-model\">Load model at specific epoch</a>\n",
    "  - <a href=\"#section-3-1-5\">3.1.5. *Train a Skipgram model*</a>\n",
    "    - <a href=\"#section-3-1-5-forward\">Take a closer look at `forward()`</a>\n",
    "    - <a href=\"#section-3-1-5-train\">Train the model (*for real*)</a>\n",
    "    - <a href=\"section-3-1-5-evaluate\">Evaluate the model</a>\n",
    "  - <a href=\"#section-3-1-6\">3.1.6. *Loading Pre-trained Embeddings*</a>\n",
    "    - <a href=\"#section-3-1-6-vocab\">Override the Embedding vocabulary</a>\n",
    "    - <a href=\"#section-3-1-6-pretrained\">Override the Embedding weights</a>\n",
    "    - <a href=\"#section-3-1-6-eval-skipgram\">Evaluate on the Skipgram task</a>\n",
    "    - <a href=\"#section-3-1-6-eval-cbow\">Evaluate on the CBOW task</a>\n",
    "    - <a href=\"#section-3-1-6-unfreeze-finetune\">Unfreeeze and finetune</a>\n",
    "    - <a href=\"#section-3-1-6-reval-cbow\">Re-evaluate on the CBOW task</a>\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0\"></a>\n",
    "# 3.0. Data Preparation\n",
    "\n",
    "Before we train our own embeddings, lets first understand how to read text data into pytorch.\n",
    "The native pytorch way to load datasets is to use the `torch.utils.data.Dataset` object.\n",
    "\n",
    "There are already several other libraries that help with loading text datasets, e.g. \n",
    "\n",
    " - FastAI https://docs.fast.ai/text.data.html\n",
    " - AllenNLP https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset.html\n",
    " - Torch Text https://github.com/pytorch/text#data\n",
    " - Texar https://texar.readthedocs.io/en/latest/code/data.html#id4 \n",
    " - SpaCy https://github.com/explosion/thinc\n",
    " \n",
    "\n",
    "But to truly understand and use it for the custom datasets you'll see at work, lets learn it the native way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-1\"></a>\n",
    "## 3.0.1  Vocabulary\n",
    "\n",
    "Given a text, the first thing to do is to build a vocabulary (i.e. a dictionary of unique words) and assign an index to each unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues.\n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    # See https://stackoverflow.com/a/25736515/610569\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    # Use the toktok tokenizer that requires no dependencies.\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "text = \"\"\"Language users never choose words randomly, and language is essentially\n",
    "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
    "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
    "the null hypothesis will never be true. Moreover, where there is enough\n",
    "data, we shall (almost) always be able to establish that it is not true. In\n",
    "corpus studies, we frequently do have enough data, so the fact that a relation \n",
    "between two phenomena is demonstrably non-random, does not support the inference \n",
    "that it is not arbitrary. We present experimental evidence\n",
    "of how arbitrary associations between word frequencies and corpora are\n",
    "systematically non-random. We review literature in which hypothesis testing \n",
    "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
    "\n",
    "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "\n",
    "uniq_tokens = set(chain(*tokenized_text))\n",
    "\n",
    "vocab = {}   # Assign indices to every word.\n",
    "idx2tok = {} # Also keep an dict of index to words.\n",
    "for i, token in enumerate(uniq_tokens):\n",
    "    vocab[token] = i\n",
    "    idx2tok[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(': 68,\n",
       " ')': 79,\n",
       " ',': 49,\n",
       " '.': 70,\n",
       " 'a': 8,\n",
       " 'able': 27,\n",
       " 'almost': 34,\n",
       " 'always': 47,\n",
       " 'and': 14,\n",
       " 'arbitrary': 2,\n",
       " 'are': 9,\n",
       " 'associations': 61,\n",
       " 'at': 69,\n",
       " 'be': 35,\n",
       " 'been': 73,\n",
       " 'between': 25,\n",
       " 'choose': 71,\n",
       " 'corpora': 16,\n",
       " 'corpus': 56,\n",
       " 'data': 86,\n",
       " 'demonstrably': 59,\n",
       " 'do': 13,\n",
       " 'does': 72,\n",
       " 'enough': 26,\n",
       " 'essentially': 10,\n",
       " 'establish': 85,\n",
       " 'evidence': 6,\n",
       " 'experimental': 78,\n",
       " 'fact': 17,\n",
       " 'frequencies': 3,\n",
       " 'frequently': 32,\n",
       " 'has': 12,\n",
       " 'have': 45,\n",
       " 'hence': 15,\n",
       " 'how': 29,\n",
       " 'hypothesis': 66,\n",
       " 'in': 76,\n",
       " 'inference': 64,\n",
       " 'is': 20,\n",
       " 'it': 37,\n",
       " 'language': 18,\n",
       " 'led': 28,\n",
       " 'linguistic': 23,\n",
       " 'literature': 31,\n",
       " 'look': 19,\n",
       " 'misleading': 0,\n",
       " 'moreover': 63,\n",
       " 'never': 1,\n",
       " 'non-random': 43,\n",
       " 'not': 22,\n",
       " 'null': 50,\n",
       " 'of': 39,\n",
       " 'often': 38,\n",
       " 'or': 62,\n",
       " 'phenomena': 55,\n",
       " 'posits': 60,\n",
       " 'present': 7,\n",
       " 'randomly': 30,\n",
       " 'randomness': 51,\n",
       " 'relation': 24,\n",
       " 'results': 52,\n",
       " 'review': 48,\n",
       " 'shall': 83,\n",
       " 'show': 42,\n",
       " 'so': 36,\n",
       " 'statistical': 21,\n",
       " 'studies': 54,\n",
       " 'support': 33,\n",
       " 'systematically': 77,\n",
       " 'testing': 67,\n",
       " 'that': 11,\n",
       " 'the': 44,\n",
       " 'there': 41,\n",
       " 'to': 80,\n",
       " 'true': 5,\n",
       " 'two': 57,\n",
       " 'unhelpful': 74,\n",
       " 'used': 58,\n",
       " 'users': 40,\n",
       " 'uses': 65,\n",
       " 'we': 81,\n",
       " 'when': 75,\n",
       " 'where': 84,\n",
       " 'which': 82,\n",
       " 'will': 53,\n",
       " 'word': 46,\n",
       " 'words': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the index of the word 'corpora'\n",
    "vocab['non-random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 40, 1, 71, 4, 30, 49, 14, 18, 20, 10, 43, 70]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The indexed representation of the first sentence.\n",
    "\n",
    "sent0 = tokenized_text[0]\n",
    "\n",
    "[vocab[token] for token in sent0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-1-a\"></a>\n",
    "\n",
    "### Pet Peeve (Gensim)\n",
    "\n",
    "I (Liling) don't really like to write my own vectorizer the `gensim` has functions that are optimized for such operations. In fact, I've written a [whole preprocessing pipeline library for me to use for language modelling and machine translation purposes](https://github.com/alvations/komorebi/blob/master/komorebi/text.py) =)\n",
    "\n",
    "Using `gensim`, I would have written the above as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "vocab = Dictionary(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ',',\n",
       " 1: '.',\n",
       " 2: 'and',\n",
       " 3: 'choose',\n",
       " 4: 'essentially',\n",
       " 5: 'is',\n",
       " 6: 'language',\n",
       " 7: 'never',\n",
       " 8: 'non-random',\n",
       " 9: 'randomly',\n",
       " 10: 'users',\n",
       " 11: 'words',\n",
       " 12: 'a',\n",
       " 13: 'hypothesis',\n",
       " 14: 'null',\n",
       " 15: 'posits',\n",
       " 16: 'randomness',\n",
       " 17: 'statistical',\n",
       " 18: 'testing',\n",
       " 19: 'uses',\n",
       " 20: 'which',\n",
       " 21: 'at',\n",
       " 22: 'be',\n",
       " 23: 'corpora',\n",
       " 24: 'hence',\n",
       " 25: 'in',\n",
       " 26: 'linguistic',\n",
       " 27: 'look',\n",
       " 28: 'phenomena',\n",
       " 29: 'the',\n",
       " 30: 'true',\n",
       " 31: 'we',\n",
       " 32: 'when',\n",
       " 33: 'will',\n",
       " 34: '(',\n",
       " 35: ')',\n",
       " 36: 'able',\n",
       " 37: 'almost',\n",
       " 38: 'always',\n",
       " 39: 'data',\n",
       " 40: 'enough',\n",
       " 41: 'establish',\n",
       " 42: 'it',\n",
       " 43: 'moreover',\n",
       " 44: 'not',\n",
       " 45: 'shall',\n",
       " 46: 'that',\n",
       " 47: 'there',\n",
       " 48: 'to',\n",
       " 49: 'where',\n",
       " 50: 'arbitrary',\n",
       " 51: 'between',\n",
       " 52: 'corpus',\n",
       " 53: 'demonstrably',\n",
       " 54: 'do',\n",
       " 55: 'does',\n",
       " 56: 'fact',\n",
       " 57: 'frequently',\n",
       " 58: 'have',\n",
       " 59: 'inference',\n",
       " 60: 'relation',\n",
       " 61: 'so',\n",
       " 62: 'studies',\n",
       " 63: 'support',\n",
       " 64: 'two',\n",
       " 65: 'are',\n",
       " 66: 'associations',\n",
       " 67: 'evidence',\n",
       " 68: 'experimental',\n",
       " 69: 'frequencies',\n",
       " 70: 'how',\n",
       " 71: 'of',\n",
       " 72: 'present',\n",
       " 73: 'systematically',\n",
       " 74: 'word',\n",
       " 75: 'been',\n",
       " 76: 'has',\n",
       " 77: 'led',\n",
       " 78: 'literature',\n",
       " 79: 'misleading',\n",
       " 80: 'often',\n",
       " 81: 'or',\n",
       " 82: 'results',\n",
       " 83: 'review',\n",
       " 84: 'show',\n",
       " 85: 'unhelpful',\n",
       " 86: 'used'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the key-value order is different of gensim from the native Python's\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token2id['corpora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.doc2idx(sent0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"indexed form\" of the tokens in the sentence forms the ***vectorized*** input to the `nn.Embedding` layer in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-2\"></a>\n",
    "\n",
    "# 3.0.2 Dataset\n",
    "\n",
    "Lets try creating a `torch.utils.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Text(Dataset):\n",
    "    def __init__(self, tokenized_texts):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self.vocab = Dictionary(tokenized_texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        # Hint: You want to return a vectorized sentence here.\n",
    "        return {'x': self.vectorize(self.sents[index])}\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-2-hints\"></a>\n",
    "## Hints to the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
    "##hint_dataset_vectorize()\n",
    "##code_text_dataset_vectorize()\n",
    "\n",
    "# Option 2: \"I give up just, run the code for me\" \n",
    "# Uncomment the next two lines, if you really gave up... \n",
    "#full_code_text_dataset_vectorize()\n",
    "##from tsundoku.word2vec import Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'present',\n",
       " 'experimental',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'how',\n",
       " 'arbitrary',\n",
       " 'associations',\n",
       " 'between',\n",
       " 'word',\n",
       " 'frequencies',\n",
       " 'and',\n",
       " 'corpora',\n",
       " 'are',\n",
       " 'systematically',\n",
       " 'non-random',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_dataset = Text(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': [31, 72, 68, 67, 71, 70, 50, 66, 51, 74, 69, 2, 23, 65, 73, 8, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset[5] # First sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-2-return-dict\"></a>\n",
    "\n",
    "### Return `dict` in `__getitem__()`\n",
    "\n",
    "This is nice if we're just representing sentences/documents by their indices but when we're doing machine learning, we usually have `X` and `Y`. \n",
    "\n",
    "If we have labels for the each sentence, we can also put it into to `__getitem__()` by having it return a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LabeledText(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self.labels = labels # Sentence level labels.\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        return {'X': self.vectorize(self.sents[index]), 'Y': self.labels[index]}\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-2-labeleddata\"></a>\n",
    "\n",
    "### Lets try the `LabeledDataset` on a movie review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:17<00:00, 111.99it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "for fileid in tqdm(movie_reviews.fileids()):\n",
    "    label = fileid.split('/')[0]\n",
    "    doc = word_tokenize(movie_reviews.open(fileid).read())\n",
    "    documents.append(doc)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " ':',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'church',\n",
       " 'party',\n",
       " ',',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'then',\n",
       " 'drive',\n",
       " '.',\n",
       " 'they',\n",
       " 'get',\n",
       " 'into',\n",
       " 'an',\n",
       " 'accident',\n",
       " '.',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'guys',\n",
       " 'dies',\n",
       " ',',\n",
       " 'but',\n",
       " 'his',\n",
       " 'girlfriend',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'see',\n",
       " 'him',\n",
       " 'in',\n",
       " 'her',\n",
       " 'life',\n",
       " ',',\n",
       " 'and',\n",
       " 'has',\n",
       " 'nightmares',\n",
       " '.',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'deal',\n",
       " '?',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'and',\n",
       " '``',\n",
       " 'sorta',\n",
       " '``',\n",
       " 'find',\n",
       " 'out',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'critique',\n",
       " ':',\n",
       " 'a',\n",
       " 'mind-fuck',\n",
       " 'movie',\n",
       " 'for',\n",
       " 'the',\n",
       " 'teen',\n",
       " 'generation',\n",
       " 'that',\n",
       " 'touches',\n",
       " 'on',\n",
       " 'a',\n",
       " 'very',\n",
       " 'cool',\n",
       " 'idea',\n",
       " ',',\n",
       " 'but',\n",
       " 'presents',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'bad',\n",
       " 'package',\n",
       " '.',\n",
       " 'which',\n",
       " 'is',\n",
       " 'what',\n",
       " 'makes',\n",
       " 'this',\n",
       " 'review',\n",
       " 'an',\n",
       " 'even',\n",
       " 'harder',\n",
       " 'one',\n",
       " 'to',\n",
       " 'write',\n",
       " ',',\n",
       " 'since',\n",
       " 'i',\n",
       " 'generally',\n",
       " 'applaud',\n",
       " 'films',\n",
       " 'which',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'break',\n",
       " 'the',\n",
       " 'mold',\n",
       " ',',\n",
       " 'mess',\n",
       " 'with',\n",
       " 'your',\n",
       " 'head',\n",
       " 'and',\n",
       " 'such',\n",
       " '(',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '&',\n",
       " 'memento',\n",
       " ')',\n",
       " ',',\n",
       " 'but',\n",
       " 'there',\n",
       " 'are',\n",
       " 'good',\n",
       " 'and',\n",
       " 'bad',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'making',\n",
       " 'all',\n",
       " 'types',\n",
       " 'of',\n",
       " 'films',\n",
       " ',',\n",
       " 'and',\n",
       " 'these',\n",
       " 'folks',\n",
       " 'just',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'snag',\n",
       " 'this',\n",
       " 'one',\n",
       " 'correctly',\n",
       " '.',\n",
       " 'they',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'this',\n",
       " 'pretty',\n",
       " 'neat',\n",
       " 'concept',\n",
       " ',',\n",
       " 'but',\n",
       " 'executed',\n",
       " 'it',\n",
       " 'terribly',\n",
       " '.',\n",
       " 'so',\n",
       " 'what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'problems',\n",
       " 'with',\n",
       " 'the',\n",
       " 'movie',\n",
       " '?',\n",
       " 'well',\n",
       " ',',\n",
       " 'its',\n",
       " 'main',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'simply',\n",
       " 'too',\n",
       " 'jumbled',\n",
       " '.',\n",
       " 'it',\n",
       " 'starts',\n",
       " 'off',\n",
       " '``',\n",
       " 'normal',\n",
       " '``',\n",
       " 'but',\n",
       " 'then',\n",
       " 'downshifts',\n",
       " 'into',\n",
       " 'this',\n",
       " '``',\n",
       " 'fantasy',\n",
       " '``',\n",
       " 'world',\n",
       " 'in',\n",
       " 'which',\n",
       " 'you',\n",
       " ',',\n",
       " 'as',\n",
       " 'an',\n",
       " 'audience',\n",
       " 'member',\n",
       " ',',\n",
       " 'have',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'going',\n",
       " 'on',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'dreams',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'characters',\n",
       " 'coming',\n",
       " 'back',\n",
       " 'from',\n",
       " 'the',\n",
       " 'dead',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'others',\n",
       " 'who',\n",
       " 'look',\n",
       " 'like',\n",
       " 'the',\n",
       " 'dead',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'strange',\n",
       " 'apparitions',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'disappearances',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'a',\n",
       " 'looooot',\n",
       " 'of',\n",
       " 'chase',\n",
       " 'scenes',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tons',\n",
       " 'of',\n",
       " 'weird',\n",
       " 'things',\n",
       " 'that',\n",
       " 'happen',\n",
       " ',',\n",
       " 'and',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'not',\n",
       " 'explained',\n",
       " '.',\n",
       " 'now',\n",
       " 'i',\n",
       " 'personally',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'mind',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'unravel',\n",
       " 'a',\n",
       " 'film',\n",
       " 'every',\n",
       " 'now',\n",
       " 'and',\n",
       " 'then',\n",
       " ',',\n",
       " 'but',\n",
       " 'when',\n",
       " 'all',\n",
       " 'it',\n",
       " 'does',\n",
       " 'is',\n",
       " 'give',\n",
       " 'me',\n",
       " 'the',\n",
       " 'same',\n",
       " 'clue',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " ',',\n",
       " 'i',\n",
       " 'get',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'fed',\n",
       " 'up',\n",
       " 'after',\n",
       " 'a',\n",
       " 'while',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'this',\n",
       " 'film',\n",
       " \"'s\",\n",
       " 'biggest',\n",
       " 'problem',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'obviously',\n",
       " 'got',\n",
       " 'this',\n",
       " 'big',\n",
       " 'secret',\n",
       " 'to',\n",
       " 'hide',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'want',\n",
       " 'to',\n",
       " 'hide',\n",
       " 'it',\n",
       " 'completely',\n",
       " 'until',\n",
       " 'its',\n",
       " 'final',\n",
       " 'five',\n",
       " 'minutes',\n",
       " '.',\n",
       " 'and',\n",
       " 'do',\n",
       " 'they',\n",
       " 'make',\n",
       " 'things',\n",
       " 'entertaining',\n",
       " ',',\n",
       " 'thrilling',\n",
       " 'or',\n",
       " 'even',\n",
       " 'engaging',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'meantime',\n",
       " '?',\n",
       " 'not',\n",
       " 'really',\n",
       " '.',\n",
       " 'the',\n",
       " 'sad',\n",
       " 'part',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'arrow',\n",
       " 'and',\n",
       " 'i',\n",
       " 'both',\n",
       " 'dig',\n",
       " 'on',\n",
       " 'flicks',\n",
       " 'like',\n",
       " 'this',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " 'actually',\n",
       " 'figured',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'out',\n",
       " 'by',\n",
       " 'the',\n",
       " 'half-way',\n",
       " 'point',\n",
       " ',',\n",
       " 'so',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'after',\n",
       " 'that',\n",
       " 'did',\n",
       " 'start',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'sense',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'still',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'the',\n",
       " 'make',\n",
       " 'the',\n",
       " 'film',\n",
       " 'all',\n",
       " 'that',\n",
       " 'more',\n",
       " 'entertaining',\n",
       " '.',\n",
       " 'i',\n",
       " 'guess',\n",
       " 'the',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'with',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'this',\n",
       " 'is',\n",
       " 'that',\n",
       " 'you',\n",
       " 'should',\n",
       " 'always',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'the',\n",
       " 'audience',\n",
       " 'is',\n",
       " '``',\n",
       " 'into',\n",
       " 'it',\n",
       " '``',\n",
       " 'even',\n",
       " 'before',\n",
       " 'they',\n",
       " 'are',\n",
       " 'given',\n",
       " 'the',\n",
       " 'secret',\n",
       " 'password',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'your',\n",
       " 'world',\n",
       " 'of',\n",
       " 'understanding',\n",
       " '.',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'showing',\n",
       " 'melissa',\n",
       " 'sagemiller',\n",
       " 'running',\n",
       " 'away',\n",
       " 'from',\n",
       " 'visions',\n",
       " 'for',\n",
       " 'about',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'just',\n",
       " 'plain',\n",
       " 'lazy',\n",
       " '!',\n",
       " '!',\n",
       " 'okay',\n",
       " ',',\n",
       " 'we',\n",
       " 'get',\n",
       " 'it',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'people',\n",
       " 'chasing',\n",
       " 'her',\n",
       " 'and',\n",
       " 'we',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'who',\n",
       " 'they',\n",
       " 'are',\n",
       " '.',\n",
       " 'do',\n",
       " 'we',\n",
       " 'really',\n",
       " 'need',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " '?',\n",
       " 'how',\n",
       " 'about',\n",
       " 'giving',\n",
       " 'us',\n",
       " 'different',\n",
       " 'scenes',\n",
       " 'offering',\n",
       " 'further',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'going',\n",
       " 'down',\n",
       " 'in',\n",
       " 'the',\n",
       " 'movie',\n",
       " '?',\n",
       " 'apparently',\n",
       " ',',\n",
       " 'the',\n",
       " 'studio',\n",
       " 'took',\n",
       " 'this',\n",
       " 'film',\n",
       " 'away',\n",
       " 'from',\n",
       " 'its',\n",
       " 'director',\n",
       " 'and',\n",
       " 'chopped',\n",
       " 'it',\n",
       " 'up',\n",
       " 'themselves',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'shows',\n",
       " '.',\n",
       " 'there',\n",
       " 'might',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'decent',\n",
       " 'teen',\n",
       " 'mind-fuck',\n",
       " 'movie',\n",
       " 'in',\n",
       " 'here',\n",
       " 'somewhere',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'guess',\n",
       " '``',\n",
       " 'the',\n",
       " 'suits',\n",
       " '``',\n",
       " 'decided',\n",
       " 'that',\n",
       " 'turning',\n",
       " 'it',\n",
       " 'into',\n",
       " 'a',\n",
       " 'music',\n",
       " 'video',\n",
       " 'with',\n",
       " 'little',\n",
       " 'edge',\n",
       " ',',\n",
       " 'would',\n",
       " 'make',\n",
       " 'more',\n",
       " 'sense',\n",
       " '.',\n",
       " 'the',\n",
       " 'actors',\n",
       " 'are',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'for',\n",
       " 'the',\n",
       " 'most',\n",
       " 'part',\n",
       " ',',\n",
       " 'although',\n",
       " 'wes',\n",
       " 'bentley',\n",
       " 'just',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'playing',\n",
       " 'the',\n",
       " 'exact',\n",
       " 'same',\n",
       " 'character',\n",
       " 'that',\n",
       " 'he',\n",
       " 'did',\n",
       " 'in',\n",
       " 'american',\n",
       " 'beauty',\n",
       " ',',\n",
       " 'only',\n",
       " 'in',\n",
       " 'a',\n",
       " 'new',\n",
       " 'neighborhood',\n",
       " '.',\n",
       " 'but',\n",
       " 'my',\n",
       " 'biggest',\n",
       " 'kudos',\n",
       " 'go',\n",
       " 'out',\n",
       " 'to',\n",
       " 'sagemiller',\n",
       " ',',\n",
       " 'who',\n",
       " 'holds',\n",
       " 'her',\n",
       " 'own',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'film',\n",
       " ',',\n",
       " 'and',\n",
       " 'actually',\n",
       " 'has',\n",
       " 'you',\n",
       " 'feeling',\n",
       " 'her',\n",
       " 'character',\n",
       " \"'s\",\n",
       " 'unraveling',\n",
       " '.',\n",
       " 'overall',\n",
       " ',',\n",
       " 'the',\n",
       " 'film',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'stick',\n",
       " 'because',\n",
       " 'it',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'entertain',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'confusing',\n",
       " ',',\n",
       " 'it',\n",
       " 'rarely',\n",
       " 'excites',\n",
       " 'and',\n",
       " 'it',\n",
       " 'feels',\n",
       " 'pretty',\n",
       " 'redundant',\n",
       " 'for',\n",
       " 'most',\n",
       " 'of',\n",
       " 'its',\n",
       " 'runtime',\n",
       " ',',\n",
       " 'despite',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'ending',\n",
       " 'and',\n",
       " 'explanation',\n",
       " 'to',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'craziness',\n",
       " 'that',\n",
       " 'came',\n",
       " 'before',\n",
       " 'it',\n",
       " '.',\n",
       " 'oh',\n",
       " ',',\n",
       " 'and',\n",
       " 'by',\n",
       " 'the',\n",
       " 'way',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'horror',\n",
       " 'or',\n",
       " 'teen',\n",
       " 'slasher',\n",
       " 'flick',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'just',\n",
       " 'packaged',\n",
       " 'to',\n",
       " 'look',\n",
       " 'that',\n",
       " 'way',\n",
       " 'because',\n",
       " 'someone',\n",
       " 'is',\n",
       " 'apparently',\n",
       " 'assuming',\n",
       " 'that',\n",
       " 'the',\n",
       " 'genre',\n",
       " 'is',\n",
       " 'still',\n",
       " 'hot',\n",
       " 'with',\n",
       " 'the',\n",
       " 'kids',\n",
       " '.',\n",
       " 'it',\n",
       " 'also',\n",
       " 'wrapped',\n",
       " 'production',\n",
       " 'two',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'has',\n",
       " 'been',\n",
       " 'sitting',\n",
       " 'on',\n",
       " 'the',\n",
       " 'shelves',\n",
       " 'ever',\n",
       " 'since',\n",
       " '.',\n",
       " 'whatever',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'skip',\n",
       " 'it',\n",
       " '!',\n",
       " 'where',\n",
       " \"'s\",\n",
       " 'joblo',\n",
       " 'coming',\n",
       " 'from',\n",
       " '?',\n",
       " 'a',\n",
       " 'nightmare',\n",
       " 'of',\n",
       " 'elm',\n",
       " 'street',\n",
       " '3',\n",
       " '(',\n",
       " '7/10',\n",
       " ')',\n",
       " '-',\n",
       " 'blair',\n",
       " 'witch',\n",
       " '2',\n",
       " '(',\n",
       " '7/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'crow',\n",
       " '(',\n",
       " '9/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'crow',\n",
       " ':',\n",
       " 'salvation',\n",
       " '(',\n",
       " '4/10',\n",
       " ')',\n",
       " '-',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '(',\n",
       " '10/10',\n",
       " ')',\n",
       " '-',\n",
       " 'memento',\n",
       " '(',\n",
       " '10/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'others',\n",
       " '(',\n",
       " '9/10',\n",
       " ')',\n",
       " '-',\n",
       " 'stir',\n",
       " 'of',\n",
       " 'echoes',\n",
       " '(',\n",
       " '8/10',\n",
       " ')']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_dataset = LabeledText(documents, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': [243,\n",
       "  17,\n",
       "  314,\n",
       "  294,\n",
       "  77,\n",
       "  140,\n",
       "  307,\n",
       "  20,\n",
       "  68,\n",
       "  237,\n",
       "  6,\n",
       "  97,\n",
       "  34,\n",
       "  299,\n",
       "  98,\n",
       "  8,\n",
       "  302,\n",
       "  135,\n",
       "  167,\n",
       "  33,\n",
       "  22,\n",
       "  8,\n",
       "  226,\n",
       "  220,\n",
       "  297,\n",
       "  145,\n",
       "  87,\n",
       "  6,\n",
       "  60,\n",
       "  158,\n",
       "  136,\n",
       "  74,\n",
       "  307,\n",
       "  262,\n",
       "  157,\n",
       "  165,\n",
       "  153,\n",
       "  179,\n",
       "  6,\n",
       "  34,\n",
       "  149,\n",
       "  214,\n",
       "  8,\n",
       "  333,\n",
       "  2,\n",
       "  297,\n",
       "  82,\n",
       "  18,\n",
       "  326,\n",
       "  297,\n",
       "  204,\n",
       "  34,\n",
       "  19,\n",
       "  280,\n",
       "  19,\n",
       "  124,\n",
       "  230,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  79,\n",
       "  17,\n",
       "  20,\n",
       "  199,\n",
       "  204,\n",
       "  129,\n",
       "  297,\n",
       "  294,\n",
       "  133,\n",
       "  296,\n",
       "  311,\n",
       "  225,\n",
       "  20,\n",
       "  322,\n",
       "  75,\n",
       "  164,\n",
       "  6,\n",
       "  60,\n",
       "  245,\n",
       "  169,\n",
       "  165,\n",
       "  20,\n",
       "  322,\n",
       "  46,\n",
       "  234,\n",
       "  8,\n",
       "  337,\n",
       "  168,\n",
       "  333,\n",
       "  188,\n",
       "  304,\n",
       "  253,\n",
       "  33,\n",
       "  108,\n",
       "  148,\n",
       "  226,\n",
       "  307,\n",
       "  345,\n",
       "  6,\n",
       "  272,\n",
       "  163,\n",
       "  132,\n",
       "  37,\n",
       "  122,\n",
       "  337,\n",
       "  42,\n",
       "  307,\n",
       "  59,\n",
       "  297,\n",
       "  201,\n",
       "  6,\n",
       "  196,\n",
       "  341,\n",
       "  348,\n",
       "  152,\n",
       "  34,\n",
       "  290,\n",
       "  4,\n",
       "  185,\n",
       "  156,\n",
       "  1,\n",
       "  195,\n",
       "  5,\n",
       "  6,\n",
       "  60,\n",
       "  300,\n",
       "  38,\n",
       "  142,\n",
       "  34,\n",
       "  46,\n",
       "  328,\n",
       "  220,\n",
       "  189,\n",
       "  28,\n",
       "  315,\n",
       "  220,\n",
       "  122,\n",
       "  6,\n",
       "  34,\n",
       "  301,\n",
       "  128,\n",
       "  173,\n",
       "  86,\n",
       "  208,\n",
       "  276,\n",
       "  304,\n",
       "  226,\n",
       "  76,\n",
       "  8,\n",
       "  302,\n",
       "  263,\n",
       "  307,\n",
       "  150,\n",
       "  293,\n",
       "  304,\n",
       "  246,\n",
       "  209,\n",
       "  72,\n",
       "  6,\n",
       "  60,\n",
       "  113,\n",
       "  169,\n",
       "  295,\n",
       "  8,\n",
       "  277,\n",
       "  333,\n",
       "  38,\n",
       "  297,\n",
       "  248,\n",
       "  341,\n",
       "  297,\n",
       "  204,\n",
       "  18,\n",
       "  331,\n",
       "  6,\n",
       "  170,\n",
       "  186,\n",
       "  247,\n",
       "  168,\n",
       "  296,\n",
       "  169,\n",
       "  2,\n",
       "  271,\n",
       "  309,\n",
       "  172,\n",
       "  8,\n",
       "  169,\n",
       "  282,\n",
       "  221,\n",
       "  19,\n",
       "  216,\n",
       "  19,\n",
       "  60,\n",
       "  299,\n",
       "  95,\n",
       "  167,\n",
       "  304,\n",
       "  19,\n",
       "  116,\n",
       "  19,\n",
       "  342,\n",
       "  165,\n",
       "  337,\n",
       "  347,\n",
       "  6,\n",
       "  40,\n",
       "  33,\n",
       "  43,\n",
       "  194,\n",
       "  6,\n",
       "  150,\n",
       "  215,\n",
       "  164,\n",
       "  333,\n",
       "  2,\n",
       "  141,\n",
       "  225,\n",
       "  8,\n",
       "  300,\n",
       "  38,\n",
       "  96,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  64,\n",
       "  70,\n",
       "  45,\n",
       "  130,\n",
       "  297,\n",
       "  81,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  229,\n",
       "  339,\n",
       "  183,\n",
       "  180,\n",
       "  297,\n",
       "  81,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  286,\n",
       "  36,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  91,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  20,\n",
       "  184,\n",
       "  220,\n",
       "  65,\n",
       "  260,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  308,\n",
       "  220,\n",
       "  330,\n",
       "  303,\n",
       "  296,\n",
       "  147,\n",
       "  6,\n",
       "  34,\n",
       "  203,\n",
       "  220,\n",
       "  169,\n",
       "  168,\n",
       "  271,\n",
       "  217,\n",
       "  114,\n",
       "  8,\n",
       "  218,\n",
       "  163,\n",
       "  240,\n",
       "  92,\n",
       "  208,\n",
       "  198,\n",
       "  312,\n",
       "  307,\n",
       "  317,\n",
       "  20,\n",
       "  121,\n",
       "  110,\n",
       "  218,\n",
       "  34,\n",
       "  299,\n",
       "  6,\n",
       "  60,\n",
       "  335,\n",
       "  28,\n",
       "  169,\n",
       "  93,\n",
       "  168,\n",
       "  137,\n",
       "  190,\n",
       "  297,\n",
       "  259,\n",
       "  69,\n",
       "  231,\n",
       "  34,\n",
       "  231,\n",
       "  26,\n",
       "  6,\n",
       "  163,\n",
       "  135,\n",
       "  175,\n",
       "  220,\n",
       "  117,\n",
       "  320,\n",
       "  25,\n",
       "  20,\n",
       "  338,\n",
       "  6,\n",
       "  337,\n",
       "  168,\n",
       "  304,\n",
       "  121,\n",
       "  2,\n",
       "  54,\n",
       "  247,\n",
       "  8,\n",
       "  169,\n",
       "  2,\n",
       "  219,\n",
       "  143,\n",
       "  304,\n",
       "  53,\n",
       "  261,\n",
       "  307,\n",
       "  155,\n",
       "  6,\n",
       "  60,\n",
       "  169,\n",
       "  265,\n",
       "  307,\n",
       "  325,\n",
       "  307,\n",
       "  155,\n",
       "  169,\n",
       "  71,\n",
       "  319,\n",
       "  170,\n",
       "  123,\n",
       "  125,\n",
       "  200,\n",
       "  8,\n",
       "  34,\n",
       "  92,\n",
       "  302,\n",
       "  187,\n",
       "  303,\n",
       "  106,\n",
       "  6,\n",
       "  305,\n",
       "  228,\n",
       "  108,\n",
       "  103,\n",
       "  6,\n",
       "  165,\n",
       "  297,\n",
       "  192,\n",
       "  18,\n",
       "  217,\n",
       "  251,\n",
       "  8,\n",
       "  297,\n",
       "  256,\n",
       "  236,\n",
       "  168,\n",
       "  296,\n",
       "  297,\n",
       "  39,\n",
       "  34,\n",
       "  163,\n",
       "  57,\n",
       "  89,\n",
       "  225,\n",
       "  127,\n",
       "  180,\n",
       "  304,\n",
       "  6,\n",
       "  277,\n",
       "  329,\n",
       "  24,\n",
       "  120,\n",
       "  203,\n",
       "  220,\n",
       "  169,\n",
       "  230,\n",
       "  61,\n",
       "  297,\n",
       "  146,\n",
       "  244,\n",
       "  6,\n",
       "  277,\n",
       "  28,\n",
       "  220,\n",
       "  297,\n",
       "  287,\n",
       "  25,\n",
       "  296,\n",
       "  86,\n",
       "  281,\n",
       "  307,\n",
       "  187,\n",
       "  20,\n",
       "  182,\n",
       "  55,\n",
       "  220,\n",
       "  266,\n",
       "  6,\n",
       "  60,\n",
       "  169,\n",
       "  284,\n",
       "  86,\n",
       "  208,\n",
       "  297,\n",
       "  187,\n",
       "  297,\n",
       "  121,\n",
       "  28,\n",
       "  296,\n",
       "  202,\n",
       "  106,\n",
       "  8,\n",
       "  163,\n",
       "  144,\n",
       "  297,\n",
       "  58,\n",
       "  181,\n",
       "  341,\n",
       "  205,\n",
       "  180,\n",
       "  304,\n",
       "  168,\n",
       "  296,\n",
       "  347,\n",
       "  268,\n",
       "  31,\n",
       "  187,\n",
       "  292,\n",
       "  296,\n",
       "  297,\n",
       "  43,\n",
       "  168,\n",
       "  19,\n",
       "  167,\n",
       "  169,\n",
       "  19,\n",
       "  108,\n",
       "  51,\n",
       "  302,\n",
       "  38,\n",
       "  138,\n",
       "  297,\n",
       "  261,\n",
       "  238,\n",
       "  307,\n",
       "  104,\n",
       "  348,\n",
       "  342,\n",
       "  220,\n",
       "  316,\n",
       "  8,\n",
       "  163,\n",
       "  191,\n",
       "  6,\n",
       "  269,\n",
       "  193,\n",
       "  257,\n",
       "  254,\n",
       "  44,\n",
       "  130,\n",
       "  324,\n",
       "  129,\n",
       "  21,\n",
       "  11,\n",
       "  200,\n",
       "  306,\n",
       "  297,\n",
       "  204,\n",
       "  168,\n",
       "  173,\n",
       "  241,\n",
       "  178,\n",
       "  0,\n",
       "  0,\n",
       "  224,\n",
       "  6,\n",
       "  329,\n",
       "  135,\n",
       "  169,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  300,\n",
       "  38,\n",
       "  239,\n",
       "  66,\n",
       "  153,\n",
       "  34,\n",
       "  329,\n",
       "  92,\n",
       "  208,\n",
       "  176,\n",
       "  339,\n",
       "  302,\n",
       "  38,\n",
       "  8,\n",
       "  92,\n",
       "  329,\n",
       "  251,\n",
       "  210,\n",
       "  307,\n",
       "  262,\n",
       "  169,\n",
       "  231,\n",
       "  34,\n",
       "  231,\n",
       "  26,\n",
       "  18,\n",
       "  162,\n",
       "  21,\n",
       "  139,\n",
       "  321,\n",
       "  88,\n",
       "  260,\n",
       "  222,\n",
       "  131,\n",
       "  166,\n",
       "  167,\n",
       "  28,\n",
       "  220,\n",
       "  297,\n",
       "  287,\n",
       "  141,\n",
       "  94,\n",
       "  165,\n",
       "  297,\n",
       "  204,\n",
       "  18,\n",
       "  35,\n",
       "  6,\n",
       "  297,\n",
       "  289,\n",
       "  310,\n",
       "  304,\n",
       "  121,\n",
       "  44,\n",
       "  130,\n",
       "  170,\n",
       "  90,\n",
       "  34,\n",
       "  67,\n",
       "  169,\n",
       "  320,\n",
       "  298,\n",
       "  6,\n",
       "  34,\n",
       "  169,\n",
       "  270,\n",
       "  8,\n",
       "  300,\n",
       "  197,\n",
       "  3,\n",
       "  50,\n",
       "  20,\n",
       "  246,\n",
       "  83,\n",
       "  294,\n",
       "  199,\n",
       "  204,\n",
       "  165,\n",
       "  154,\n",
       "  279,\n",
       "  6,\n",
       "  60,\n",
       "  163,\n",
       "  144,\n",
       "  19,\n",
       "  297,\n",
       "  291,\n",
       "  19,\n",
       "  84,\n",
       "  296,\n",
       "  313,\n",
       "  169,\n",
       "  167,\n",
       "  20,\n",
       "  206,\n",
       "  323,\n",
       "  341,\n",
       "  182,\n",
       "  100,\n",
       "  6,\n",
       "  343,\n",
       "  187,\n",
       "  202,\n",
       "  266,\n",
       "  8,\n",
       "  297,\n",
       "  23,\n",
       "  38,\n",
       "  246,\n",
       "  142,\n",
       "  129,\n",
       "  297,\n",
       "  203,\n",
       "  236,\n",
       "  6,\n",
       "  30,\n",
       "  332,\n",
       "  52,\n",
       "  173,\n",
       "  264,\n",
       "  307,\n",
       "  47,\n",
       "  242,\n",
       "  297,\n",
       "  111,\n",
       "  259,\n",
       "  63,\n",
       "  296,\n",
       "  151,\n",
       "  86,\n",
       "  165,\n",
       "  32,\n",
       "  48,\n",
       "  6,\n",
       "  227,\n",
       "  165,\n",
       "  20,\n",
       "  212,\n",
       "  211,\n",
       "  8,\n",
       "  60,\n",
       "  207,\n",
       "  54,\n",
       "  177,\n",
       "  140,\n",
       "  230,\n",
       "  307,\n",
       "  257,\n",
       "  6,\n",
       "  339,\n",
       "  159,\n",
       "  153,\n",
       "  233,\n",
       "  306,\n",
       "  297,\n",
       "  107,\n",
       "  121,\n",
       "  6,\n",
       "  34,\n",
       "  24,\n",
       "  149,\n",
       "  347,\n",
       "  118,\n",
       "  153,\n",
       "  63,\n",
       "  2,\n",
       "  318,\n",
       "  8,\n",
       "  232,\n",
       "  6,\n",
       "  297,\n",
       "  121,\n",
       "  93,\n",
       "  208,\n",
       "  283,\n",
       "  49,\n",
       "  169,\n",
       "  93,\n",
       "  208,\n",
       "  105,\n",
       "  6,\n",
       "  169,\n",
       "  2,\n",
       "  73,\n",
       "  6,\n",
       "  169,\n",
       "  250,\n",
       "  112,\n",
       "  34,\n",
       "  169,\n",
       "  119,\n",
       "  246,\n",
       "  252,\n",
       "  129,\n",
       "  203,\n",
       "  220,\n",
       "  170,\n",
       "  255,\n",
       "  6,\n",
       "  85,\n",
       "  20,\n",
       "  246,\n",
       "  75,\n",
       "  102,\n",
       "  34,\n",
       "  115,\n",
       "  307,\n",
       "  28,\n",
       "  220,\n",
       "  297,\n",
       "  78,\n",
       "  296,\n",
       "  62,\n",
       "  51,\n",
       "  169,\n",
       "  8,\n",
       "  223,\n",
       "  6,\n",
       "  34,\n",
       "  61,\n",
       "  297,\n",
       "  327,\n",
       "  6,\n",
       "  304,\n",
       "  168,\n",
       "  217,\n",
       "  20,\n",
       "  160,\n",
       "  228,\n",
       "  294,\n",
       "  275,\n",
       "  126,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  169,\n",
       "  2,\n",
       "  173,\n",
       "  235,\n",
       "  307,\n",
       "  183,\n",
       "  296,\n",
       "  327,\n",
       "  49,\n",
       "  278,\n",
       "  168,\n",
       "  35,\n",
       "  41,\n",
       "  296,\n",
       "  297,\n",
       "  134,\n",
       "  168,\n",
       "  284,\n",
       "  161,\n",
       "  341,\n",
       "  297,\n",
       "  174,\n",
       "  8,\n",
       "  169,\n",
       "  29,\n",
       "  344,\n",
       "  249,\n",
       "  314,\n",
       "  346,\n",
       "  27,\n",
       "  34,\n",
       "  149,\n",
       "  50,\n",
       "  273,\n",
       "  225,\n",
       "  297,\n",
       "  267,\n",
       "  109,\n",
       "  272,\n",
       "  8,\n",
       "  334,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  274,\n",
       "  169,\n",
       "  0,\n",
       "  336,\n",
       "  2,\n",
       "  171,\n",
       "  70,\n",
       "  130,\n",
       "  18,\n",
       "  20,\n",
       "  213,\n",
       "  220,\n",
       "  101,\n",
       "  288,\n",
       "  12,\n",
       "  4,\n",
       "  14,\n",
       "  5,\n",
       "  7,\n",
       "  56,\n",
       "  340,\n",
       "  10,\n",
       "  4,\n",
       "  14,\n",
       "  5,\n",
       "  7,\n",
       "  297,\n",
       "  80,\n",
       "  4,\n",
       "  16,\n",
       "  5,\n",
       "  7,\n",
       "  297,\n",
       "  80,\n",
       "  17,\n",
       "  258,\n",
       "  4,\n",
       "  13,\n",
       "  5,\n",
       "  7,\n",
       "  185,\n",
       "  156,\n",
       "  4,\n",
       "  9,\n",
       "  5,\n",
       "  7,\n",
       "  195,\n",
       "  4,\n",
       "  9,\n",
       "  5,\n",
       "  7,\n",
       "  297,\n",
       "  229,\n",
       "  4,\n",
       "  16,\n",
       "  5,\n",
       "  7,\n",
       "  285,\n",
       "  220,\n",
       "  99,\n",
       "  4,\n",
       "  15,\n",
       "  5],\n",
       " 'Y': 'neg'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset[0]  # First review in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[243,\n",
       " 17,\n",
       " 314,\n",
       " 294,\n",
       " 77,\n",
       " 140,\n",
       " 307,\n",
       " 20,\n",
       " 68,\n",
       " 237,\n",
       " 6,\n",
       " 97,\n",
       " 34,\n",
       " 299,\n",
       " 98,\n",
       " 8,\n",
       " 302,\n",
       " 135,\n",
       " 167,\n",
       " 33,\n",
       " 22,\n",
       " 8,\n",
       " 226,\n",
       " 220,\n",
       " 297,\n",
       " 145,\n",
       " 87,\n",
       " 6,\n",
       " 60,\n",
       " 158,\n",
       " 136,\n",
       " 74,\n",
       " 307,\n",
       " 262,\n",
       " 157,\n",
       " 165,\n",
       " 153,\n",
       " 179,\n",
       " 6,\n",
       " 34,\n",
       " 149,\n",
       " 214,\n",
       " 8,\n",
       " 333,\n",
       " 2,\n",
       " 297,\n",
       " 82,\n",
       " 18,\n",
       " 326,\n",
       " 297,\n",
       " 204,\n",
       " 34,\n",
       " 19,\n",
       " 280,\n",
       " 19,\n",
       " 124,\n",
       " 230,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 79,\n",
       " 17,\n",
       " 20,\n",
       " 199,\n",
       " 204,\n",
       " 129,\n",
       " 297,\n",
       " 294,\n",
       " 133,\n",
       " 296,\n",
       " 311,\n",
       " 225,\n",
       " 20,\n",
       " 322,\n",
       " 75,\n",
       " 164,\n",
       " 6,\n",
       " 60,\n",
       " 245,\n",
       " 169,\n",
       " 165,\n",
       " 20,\n",
       " 322,\n",
       " 46,\n",
       " 234,\n",
       " 8,\n",
       " 337,\n",
       " 168,\n",
       " 333,\n",
       " 188,\n",
       " 304,\n",
       " 253,\n",
       " 33,\n",
       " 108,\n",
       " 148,\n",
       " 226,\n",
       " 307,\n",
       " 345,\n",
       " 6,\n",
       " 272,\n",
       " 163,\n",
       " 132,\n",
       " 37,\n",
       " 122,\n",
       " 337,\n",
       " 42,\n",
       " 307,\n",
       " 59,\n",
       " 297,\n",
       " 201,\n",
       " 6,\n",
       " 196,\n",
       " 341,\n",
       " 348,\n",
       " 152,\n",
       " 34,\n",
       " 290,\n",
       " 4,\n",
       " 185,\n",
       " 156,\n",
       " 1,\n",
       " 195,\n",
       " 5,\n",
       " 6,\n",
       " 60,\n",
       " 300,\n",
       " 38,\n",
       " 142,\n",
       " 34,\n",
       " 46,\n",
       " 328,\n",
       " 220,\n",
       " 189,\n",
       " 28,\n",
       " 315,\n",
       " 220,\n",
       " 122,\n",
       " 6,\n",
       " 34,\n",
       " 301,\n",
       " 128,\n",
       " 173,\n",
       " 86,\n",
       " 208,\n",
       " 276,\n",
       " 304,\n",
       " 226,\n",
       " 76,\n",
       " 8,\n",
       " 302,\n",
       " 263,\n",
       " 307,\n",
       " 150,\n",
       " 293,\n",
       " 304,\n",
       " 246,\n",
       " 209,\n",
       " 72,\n",
       " 6,\n",
       " 60,\n",
       " 113,\n",
       " 169,\n",
       " 295,\n",
       " 8,\n",
       " 277,\n",
       " 333,\n",
       " 38,\n",
       " 297,\n",
       " 248,\n",
       " 341,\n",
       " 297,\n",
       " 204,\n",
       " 18,\n",
       " 331,\n",
       " 6,\n",
       " 170,\n",
       " 186,\n",
       " 247,\n",
       " 168,\n",
       " 296,\n",
       " 169,\n",
       " 2,\n",
       " 271,\n",
       " 309,\n",
       " 172,\n",
       " 8,\n",
       " 169,\n",
       " 282,\n",
       " 221,\n",
       " 19,\n",
       " 216,\n",
       " 19,\n",
       " 60,\n",
       " 299,\n",
       " 95,\n",
       " 167,\n",
       " 304,\n",
       " 19,\n",
       " 116,\n",
       " 19,\n",
       " 342,\n",
       " 165,\n",
       " 337,\n",
       " 347,\n",
       " 6,\n",
       " 40,\n",
       " 33,\n",
       " 43,\n",
       " 194,\n",
       " 6,\n",
       " 150,\n",
       " 215,\n",
       " 164,\n",
       " 333,\n",
       " 2,\n",
       " 141,\n",
       " 225,\n",
       " 8,\n",
       " 300,\n",
       " 38,\n",
       " 96,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 64,\n",
       " 70,\n",
       " 45,\n",
       " 130,\n",
       " 297,\n",
       " 81,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 229,\n",
       " 339,\n",
       " 183,\n",
       " 180,\n",
       " 297,\n",
       " 81,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 286,\n",
       " 36,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 91,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 20,\n",
       " 184,\n",
       " 220,\n",
       " 65,\n",
       " 260,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 308,\n",
       " 220,\n",
       " 330,\n",
       " 303,\n",
       " 296,\n",
       " 147,\n",
       " 6,\n",
       " 34,\n",
       " 203,\n",
       " 220,\n",
       " 169,\n",
       " 168,\n",
       " 271,\n",
       " 217,\n",
       " 114,\n",
       " 8,\n",
       " 218,\n",
       " 163,\n",
       " 240,\n",
       " 92,\n",
       " 208,\n",
       " 198,\n",
       " 312,\n",
       " 307,\n",
       " 317,\n",
       " 20,\n",
       " 121,\n",
       " 110,\n",
       " 218,\n",
       " 34,\n",
       " 299,\n",
       " 6,\n",
       " 60,\n",
       " 335,\n",
       " 28,\n",
       " 169,\n",
       " 93,\n",
       " 168,\n",
       " 137,\n",
       " 190,\n",
       " 297,\n",
       " 259,\n",
       " 69,\n",
       " 231,\n",
       " 34,\n",
       " 231,\n",
       " 26,\n",
       " 6,\n",
       " 163,\n",
       " 135,\n",
       " 175,\n",
       " 220,\n",
       " 117,\n",
       " 320,\n",
       " 25,\n",
       " 20,\n",
       " 338,\n",
       " 6,\n",
       " 337,\n",
       " 168,\n",
       " 304,\n",
       " 121,\n",
       " 2,\n",
       " 54,\n",
       " 247,\n",
       " 8,\n",
       " 169,\n",
       " 2,\n",
       " 219,\n",
       " 143,\n",
       " 304,\n",
       " 53,\n",
       " 261,\n",
       " 307,\n",
       " 155,\n",
       " 6,\n",
       " 60,\n",
       " 169,\n",
       " 265,\n",
       " 307,\n",
       " 325,\n",
       " 307,\n",
       " 155,\n",
       " 169,\n",
       " 71,\n",
       " 319,\n",
       " 170,\n",
       " 123,\n",
       " 125,\n",
       " 200,\n",
       " 8,\n",
       " 34,\n",
       " 92,\n",
       " 302,\n",
       " 187,\n",
       " 303,\n",
       " 106,\n",
       " 6,\n",
       " 305,\n",
       " 228,\n",
       " 108,\n",
       " 103,\n",
       " 6,\n",
       " 165,\n",
       " 297,\n",
       " 192,\n",
       " 18,\n",
       " 217,\n",
       " 251,\n",
       " 8,\n",
       " 297,\n",
       " 256,\n",
       " 236,\n",
       " 168,\n",
       " 296,\n",
       " 297,\n",
       " 39,\n",
       " 34,\n",
       " 163,\n",
       " 57,\n",
       " 89,\n",
       " 225,\n",
       " 127,\n",
       " 180,\n",
       " 304,\n",
       " 6,\n",
       " 277,\n",
       " 329,\n",
       " 24,\n",
       " 120,\n",
       " 203,\n",
       " 220,\n",
       " 169,\n",
       " 230,\n",
       " 61,\n",
       " 297,\n",
       " 146,\n",
       " 244,\n",
       " 6,\n",
       " 277,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 287,\n",
       " 25,\n",
       " 296,\n",
       " 86,\n",
       " 281,\n",
       " 307,\n",
       " 187,\n",
       " 20,\n",
       " 182,\n",
       " 55,\n",
       " 220,\n",
       " 266,\n",
       " 6,\n",
       " 60,\n",
       " 169,\n",
       " 284,\n",
       " 86,\n",
       " 208,\n",
       " 297,\n",
       " 187,\n",
       " 297,\n",
       " 121,\n",
       " 28,\n",
       " 296,\n",
       " 202,\n",
       " 106,\n",
       " 8,\n",
       " 163,\n",
       " 144,\n",
       " 297,\n",
       " 58,\n",
       " 181,\n",
       " 341,\n",
       " 205,\n",
       " 180,\n",
       " 304,\n",
       " 168,\n",
       " 296,\n",
       " 347,\n",
       " 268,\n",
       " 31,\n",
       " 187,\n",
       " 292,\n",
       " 296,\n",
       " 297,\n",
       " 43,\n",
       " 168,\n",
       " 19,\n",
       " 167,\n",
       " 169,\n",
       " 19,\n",
       " 108,\n",
       " 51,\n",
       " 302,\n",
       " 38,\n",
       " 138,\n",
       " 297,\n",
       " 261,\n",
       " 238,\n",
       " 307,\n",
       " 104,\n",
       " 348,\n",
       " 342,\n",
       " 220,\n",
       " 316,\n",
       " 8,\n",
       " 163,\n",
       " 191,\n",
       " 6,\n",
       " 269,\n",
       " 193,\n",
       " 257,\n",
       " 254,\n",
       " 44,\n",
       " 130,\n",
       " 324,\n",
       " 129,\n",
       " 21,\n",
       " 11,\n",
       " 200,\n",
       " 306,\n",
       " 297,\n",
       " 204,\n",
       " 168,\n",
       " 173,\n",
       " 241,\n",
       " 178,\n",
       " 0,\n",
       " 0,\n",
       " 224,\n",
       " 6,\n",
       " 329,\n",
       " 135,\n",
       " 169,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 300,\n",
       " 38,\n",
       " 239,\n",
       " 66,\n",
       " 153,\n",
       " 34,\n",
       " 329,\n",
       " 92,\n",
       " 208,\n",
       " 176,\n",
       " 339,\n",
       " 302,\n",
       " 38,\n",
       " 8,\n",
       " 92,\n",
       " 329,\n",
       " 251,\n",
       " 210,\n",
       " 307,\n",
       " 262,\n",
       " 169,\n",
       " 231,\n",
       " 34,\n",
       " 231,\n",
       " 26,\n",
       " 18,\n",
       " 162,\n",
       " 21,\n",
       " 139,\n",
       " 321,\n",
       " 88,\n",
       " 260,\n",
       " 222,\n",
       " 131,\n",
       " 166,\n",
       " 167,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 287,\n",
       " 141,\n",
       " 94,\n",
       " 165,\n",
       " 297,\n",
       " 204,\n",
       " 18,\n",
       " 35,\n",
       " 6,\n",
       " 297,\n",
       " 289,\n",
       " 310,\n",
       " 304,\n",
       " 121,\n",
       " 44,\n",
       " 130,\n",
       " 170,\n",
       " 90,\n",
       " 34,\n",
       " 67,\n",
       " 169,\n",
       " 320,\n",
       " 298,\n",
       " 6,\n",
       " 34,\n",
       " 169,\n",
       " 270,\n",
       " 8,\n",
       " 300,\n",
       " 197,\n",
       " 3,\n",
       " 50,\n",
       " 20,\n",
       " 246,\n",
       " 83,\n",
       " 294,\n",
       " 199,\n",
       " 204,\n",
       " 165,\n",
       " 154,\n",
       " 279,\n",
       " 6,\n",
       " 60,\n",
       " 163,\n",
       " 144,\n",
       " 19,\n",
       " 297,\n",
       " 291,\n",
       " 19,\n",
       " 84,\n",
       " 296,\n",
       " 313,\n",
       " 169,\n",
       " 167,\n",
       " 20,\n",
       " 206,\n",
       " 323,\n",
       " 341,\n",
       " 182,\n",
       " 100,\n",
       " 6,\n",
       " 343,\n",
       " 187,\n",
       " 202,\n",
       " 266,\n",
       " 8,\n",
       " 297,\n",
       " 23,\n",
       " 38,\n",
       " 246,\n",
       " 142,\n",
       " 129,\n",
       " 297,\n",
       " 203,\n",
       " 236,\n",
       " 6,\n",
       " 30,\n",
       " 332,\n",
       " 52,\n",
       " 173,\n",
       " 264,\n",
       " 307,\n",
       " 47,\n",
       " 242,\n",
       " 297,\n",
       " 111,\n",
       " 259,\n",
       " 63,\n",
       " 296,\n",
       " 151,\n",
       " 86,\n",
       " 165,\n",
       " 32,\n",
       " 48,\n",
       " 6,\n",
       " 227,\n",
       " 165,\n",
       " 20,\n",
       " 212,\n",
       " 211,\n",
       " 8,\n",
       " 60,\n",
       " 207,\n",
       " 54,\n",
       " 177,\n",
       " 140,\n",
       " 230,\n",
       " 307,\n",
       " 257,\n",
       " 6,\n",
       " 339,\n",
       " 159,\n",
       " 153,\n",
       " 233,\n",
       " 306,\n",
       " 297,\n",
       " 107,\n",
       " 121,\n",
       " 6,\n",
       " 34,\n",
       " 24,\n",
       " 149,\n",
       " 347,\n",
       " 118,\n",
       " 153,\n",
       " 63,\n",
       " 2,\n",
       " 318,\n",
       " 8,\n",
       " 232,\n",
       " 6,\n",
       " 297,\n",
       " 121,\n",
       " 93,\n",
       " 208,\n",
       " 283,\n",
       " 49,\n",
       " 169,\n",
       " 93,\n",
       " 208,\n",
       " 105,\n",
       " 6,\n",
       " 169,\n",
       " 2,\n",
       " 73,\n",
       " 6,\n",
       " 169,\n",
       " 250,\n",
       " 112,\n",
       " 34,\n",
       " 169,\n",
       " 119,\n",
       " 246,\n",
       " 252,\n",
       " 129,\n",
       " 203,\n",
       " 220,\n",
       " 170,\n",
       " 255,\n",
       " 6,\n",
       " 85,\n",
       " 20,\n",
       " 246,\n",
       " 75,\n",
       " 102,\n",
       " 34,\n",
       " 115,\n",
       " 307,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 78,\n",
       " 296,\n",
       " 62,\n",
       " 51,\n",
       " 169,\n",
       " 8,\n",
       " 223,\n",
       " 6,\n",
       " 34,\n",
       " 61,\n",
       " 297,\n",
       " 327,\n",
       " 6,\n",
       " 304,\n",
       " 168,\n",
       " 217,\n",
       " 20,\n",
       " 160,\n",
       " 228,\n",
       " 294,\n",
       " 275,\n",
       " 126,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 169,\n",
       " 2,\n",
       " 173,\n",
       " 235,\n",
       " 307,\n",
       " 183,\n",
       " 296,\n",
       " 327,\n",
       " 49,\n",
       " 278,\n",
       " 168,\n",
       " 35,\n",
       " 41,\n",
       " 296,\n",
       " 297,\n",
       " 134,\n",
       " 168,\n",
       " 284,\n",
       " 161,\n",
       " 341,\n",
       " 297,\n",
       " 174,\n",
       " 8,\n",
       " 169,\n",
       " 29,\n",
       " 344,\n",
       " 249,\n",
       " 314,\n",
       " 346,\n",
       " 27,\n",
       " 34,\n",
       " 149,\n",
       " 50,\n",
       " 273,\n",
       " 225,\n",
       " 297,\n",
       " 267,\n",
       " 109,\n",
       " 272,\n",
       " 8,\n",
       " 334,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 274,\n",
       " 169,\n",
       " 0,\n",
       " 336,\n",
       " 2,\n",
       " 171,\n",
       " 70,\n",
       " 130,\n",
       " 18,\n",
       " 20,\n",
       " 213,\n",
       " 220,\n",
       " 101,\n",
       " 288,\n",
       " 12,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 56,\n",
       " 340,\n",
       " 10,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 80,\n",
       " 4,\n",
       " 16,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 80,\n",
       " 17,\n",
       " 258,\n",
       " 4,\n",
       " 13,\n",
       " 5,\n",
       " 7,\n",
       " 185,\n",
       " 156,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 195,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 229,\n",
       " 4,\n",
       " 16,\n",
       " 5,\n",
       " 7,\n",
       " 285,\n",
       " 220,\n",
       " 99,\n",
       " 4,\n",
       " 15,\n",
       " 5]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset[0]['X']  # Label of the first review in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1\"></a>\n",
    "\n",
    "# 3.1 Word2Vec Training\n",
    "\n",
    "Word2Vec has two training variants:\n",
    "\n",
    " - **Continuous Bag of Words (CBOW)**: Predict center word from (bag of) context words.\n",
    " - **Skip-grams**: Predict context words given center word.\n",
    "  \n",
    "Visually, they look like this:\n",
    "\n",
    "<img src=\"https://ibin.co/4UIznsOEyH7t.png\" width=\"500\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-1\"></a>\n",
    "\n",
    "## 3.1.1. CBOW\n",
    "\n",
    "CBOW windows through the sentence and picks out the center word as the `Y` and the surrounding context words as the inputs `X`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (4, None, None)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lazyme import per_window, per_chunk\n",
    "\n",
    "xx =[1,2,3,4]\n",
    "list(per_window(xx, n=2))\n",
    "list(per_chunk(xx, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def per_window(sequence, n=1):\n",
    "    \"\"\"\n",
    "    From http://stackoverflow.com/q/42220614/610569\n",
    "        >>> list(per_window([1,2,3,4], n=2))\n",
    "        [(1, 2), (2, 3), (3, 4)]\n",
    "        >>> list(per_window([1,2,3,4], n=3))\n",
    "        [(1, 2, 3), (2, 3, 4)]\n",
    "    \"\"\"\n",
    "    start, stop = 0, n\n",
    "    seq = list(sequence)\n",
    "    while stop <= len(seq):\n",
    "        yield seq[start:stop]\n",
    "        start += 1\n",
    "        stop += 1\n",
    "\n",
    "def cbow_iterator(tokens, window_size):\n",
    "    n = window_size * 2 + 1\n",
    "    for window in per_window(tokens, n):\n",
    "        target = window.pop(window_size)\n",
    "        yield window, target   # X = window ; Y = target. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent0 = ['language', 'users', 'never', 'choose', 'words', 'randomly', ',', \n",
    "         'and', 'language', 'is', 'essentially', 'non-random', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['language', 'users', 'choose', 'words'], 'never'),\n",
       " (['users', 'never', 'words', 'randomly'], 'choose'),\n",
       " (['never', 'choose', 'randomly', ','], 'words'),\n",
       " (['choose', 'words', ',', 'and'], 'randomly'),\n",
       " (['words', 'randomly', 'and', 'language'], ','),\n",
       " (['randomly', ',', 'language', 'is'], 'and'),\n",
       " ([',', 'and', 'is', 'essentially'], 'language'),\n",
       " (['and', 'language', 'essentially', 'non-random'], 'is'),\n",
       " (['language', 'is', 'non-random', '.'], 'essentially')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_iterator(sent0, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['language', 'users', 'never', 'words', 'randomly', ','], 'choose'),\n",
       " (['users', 'never', 'choose', 'randomly', ',', 'and'], 'words'),\n",
       " (['never', 'choose', 'words', ',', 'and', 'language'], 'randomly'),\n",
       " (['choose', 'words', 'randomly', 'and', 'language', 'is'], ','),\n",
       " (['words', 'randomly', ',', 'language', 'is', 'essentially'], 'and'),\n",
       " (['randomly', ',', 'and', 'is', 'essentially', 'non-random'], 'language'),\n",
       " ([',', 'and', 'language', 'essentially', 'non-random', '.'], 'is')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_iterator(sent0, 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-2\"></a>\n",
    "\n",
    "## 3.1.2. Skipgram\n",
    "\n",
    "Skipgram training windows through the sentence and pictures out the center word as the input `X` and the context words as the outputs `Y`, additionally, it will randommly sample words not in the window as **negative samples**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skipgram_iterator(tokens, window_size):\n",
    "    n = window_size * 2 + 1 \n",
    "    for i, window in enumerate(per_window(tokens, n)):\n",
    "        target = window.pop(window_size)\n",
    "        # Generate positive samples.\n",
    "        for context_word in window:\n",
    "            yield target, context_word, 1\n",
    "        # Generate negative samples.\n",
    "        for _ in range(n-1):\n",
    "            leftovers = tokens[:i] + tokens[i+n:]\n",
    "            yield target, random.choice(leftovers), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('never', 'language', 1),\n",
       " ('never', 'users', 1),\n",
       " ('never', 'choose', 1),\n",
       " ('never', 'words', 1),\n",
       " ('never', 'is', 0),\n",
       " ('never', 'randomly', 0),\n",
       " ('never', 'is', 0),\n",
       " ('never', 'randomly', 0),\n",
       " ('choose', 'users', 1),\n",
       " ('choose', 'never', 1),\n",
       " ('choose', 'words', 1),\n",
       " ('choose', 'randomly', 1),\n",
       " ('choose', 'language', 0),\n",
       " ('choose', '.', 0),\n",
       " ('choose', 'language', 0),\n",
       " ('choose', 'essentially', 0),\n",
       " ('words', 'never', 1),\n",
       " ('words', 'choose', 1),\n",
       " ('words', 'randomly', 1),\n",
       " ('words', ',', 1),\n",
       " ('words', 'users', 0),\n",
       " ('words', 'is', 0),\n",
       " ('words', 'non-random', 0),\n",
       " ('words', 'and', 0),\n",
       " ('randomly', 'choose', 1),\n",
       " ('randomly', 'words', 1),\n",
       " ('randomly', ',', 1),\n",
       " ('randomly', 'and', 1),\n",
       " ('randomly', 'is', 0),\n",
       " ('randomly', 'language', 0),\n",
       " ('randomly', 'language', 0),\n",
       " ('randomly', '.', 0),\n",
       " (',', 'words', 1),\n",
       " (',', 'randomly', 1),\n",
       " (',', 'and', 1),\n",
       " (',', 'language', 1),\n",
       " (',', 'never', 0),\n",
       " (',', 'never', 0),\n",
       " (',', 'users', 0),\n",
       " (',', '.', 0),\n",
       " ('and', 'randomly', 1),\n",
       " ('and', ',', 1),\n",
       " ('and', 'language', 1),\n",
       " ('and', 'is', 1),\n",
       " ('and', 'never', 0),\n",
       " ('and', 'choose', 0),\n",
       " ('and', 'choose', 0),\n",
       " ('and', 'language', 0),\n",
       " ('language', ',', 1),\n",
       " ('language', 'and', 1),\n",
       " ('language', 'is', 1),\n",
       " ('language', 'essentially', 1),\n",
       " ('language', 'randomly', 0),\n",
       " ('language', 'words', 0),\n",
       " ('language', 'users', 0),\n",
       " ('language', 'words', 0),\n",
       " ('is', 'and', 1),\n",
       " ('is', 'language', 1),\n",
       " ('is', 'essentially', 1),\n",
       " ('is', 'non-random', 1),\n",
       " ('is', 'randomly', 0),\n",
       " ('is', 'never', 0),\n",
       " ('is', 'users', 0),\n",
       " ('is', 'words', 0),\n",
       " ('essentially', 'language', 1),\n",
       " ('essentially', 'is', 1),\n",
       " ('essentially', 'non-random', 1),\n",
       " ('essentially', '.', 1),\n",
       " ('essentially', 'and', 0),\n",
       " ('essentially', 'words', 0),\n",
       " ('essentially', 'choose', 0),\n",
       " ('essentially', 'words', 0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(skipgram_iterator(sent0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut-away: What is `partial`?\n",
    "\n",
    "The [`functools.partial`](https://docs.python.org/3.7/library/functools.html#functools.partial) function in Python is a mechanism to overload a function with preset arguments. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "# Generates bigrams\n",
    "list(ngrams('this is a sentence'.split(), n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# You can create a new function that \"preset\" the `n` argument, e.g.\n",
    "bigrams = partial(ngrams, n=2)\n",
    "trigrams = partial(ngrams, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is', 'a'), ('is', 'a', 'sentence')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigrams('this is a sentence'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams('this is a sentence'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-3\"></a>\n",
    "\n",
    "## 3.1.3 Word2Vec Dataset\n",
    "\n",
    "Now that we know what are the inputs `X` and outputs `Y` of the Word2Vec task. \n",
    "\n",
    "Lets put everything together and modify the `Dataset` so that `__getitem__` retrieves CBOW or Skipgram formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "\n",
    "    def cbow_iterator(self,tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield window, target   # X = window ; Y = target. \n",
    "\n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            target = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield target, context_word, 1\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                yield target, random.choice(leftovers), 0\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-3-hint\"></a>\n",
    "## Hints for the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
    "##hint_word2vec_dataset()\n",
    "\n",
    "# Option 2: \"I give up just, run the code for me\" \n",
    "# Uncomment the next two lines, if you really gave up... \n",
    "##full_code_word2vec_dataset()\n",
    "##from tsundoku.word2vec import Word2VecText\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-hint\"></a>\n",
    "\n",
    "## 3.1.4. Train a CBOW model\n",
    "\n",
    "### Lets Get Some Data\n",
    "\n",
    "Lets take Kilgarriff (2005) , \"Language is never ever, ever random\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import io #codecs\n",
    "\n",
    "\n",
    "# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
    "if os.path.isfile('language-never-random.txt'):\n",
    "    with io.open('language-never-random.txt', encoding='utf8') as fin:\n",
    "        text = fin.read()\n",
    "else:\n",
    "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
    "    text = requests.get(url).content.decode('utf8')\n",
    "    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
    "        fout.write(text)\n",
    "\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
    "window_size = 2\n",
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Language is never, ever, ever, random\n",
      "\n",
      "                                                               ADAM KILGARRIFF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Abstract\n",
      "Language users never choose words randomly, and language is essentially\n",
      "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
      "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
      "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
      "data, we shall (almost) always be able to establish that it is not true. In\n",
      "corpus studies, we frequently do have enough data, so the fact that a rela-\n",
      "tion between two phenomena is demonstrably non-random, does not sup-\n",
      "port the inference that it is not arbitrary. We present experimental evidence\n",
      "of how arbitrary associations between word frequencies and corpora are\n",
      "systematically non-random. We review literature in which hypothesis test-\n",
      "ing has been used, and show how it has often led to unhelpful or mislead-\n",
      "ing results.\n",
      "Keywords: 쎲쎲쎲\n",
      "\n",
      "1. Int\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check, lets take a look at the data.\n",
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bulk of linguistic questions concern the dis- tinction between a and m. a linguistic account of a phenomenon gen- erally gives us reason to view the relation between , for example , a verb ’ s syntax and its semantics , as motivated rather than arbitrary .\n",
      "\u001b[92mof\u001b[0m \t\t the bulk \u001b[91m______\u001b[0m linguistic questions\n",
      "\u001b[92mlinguistic\u001b[0m \t bulk of \u001b[91m______\u001b[0m questions concern\n",
      "\u001b[92mquestions\u001b[0m \t of linguistic \u001b[91m______\u001b[0m concern the\n",
      "\u001b[92mconcern\u001b[0m \t linguistic questions \u001b[91m______\u001b[0m the dis-\n",
      "\u001b[92mthe\u001b[0m \t\t questions concern \u001b[91m______\u001b[0m dis- tinction\n",
      "\u001b[92mdis-\u001b[0m \t\t concern the \u001b[91m______\u001b[0m tinction between\n",
      "\u001b[92mtinction\u001b[0m \t the dis- \u001b[91m______\u001b[0m between a\n",
      "\u001b[92mbetween\u001b[0m \t dis- tinction \u001b[91m______\u001b[0m a and\n",
      "\u001b[92ma\u001b[0m \t\t tinction between \u001b[91m______\u001b[0m and m.\n",
      "\u001b[92mand\u001b[0m \t\t between a \u001b[91m______\u001b[0m m. a\n",
      "\u001b[92mm.\u001b[0m \t\t a and \u001b[91m______\u001b[0m a linguistic\n",
      "\u001b[92ma\u001b[0m \t\t and m. \u001b[91m______\u001b[0m linguistic account\n",
      "\u001b[92mlinguistic\u001b[0m \t m. a \u001b[91m______\u001b[0m account of\n",
      "\u001b[92maccount\u001b[0m \t a linguistic \u001b[91m______\u001b[0m of a\n",
      "\u001b[92mof\u001b[0m \t\t linguistic account \u001b[91m______\u001b[0m a phenomenon\n",
      "\u001b[92ma\u001b[0m \t\t account of \u001b[91m______\u001b[0m phenomenon gen-\n",
      "\u001b[92mphenomenon\u001b[0m \t of a \u001b[91m______\u001b[0m gen- erally\n",
      "\u001b[92mgen-\u001b[0m \t\t a phenomenon \u001b[91m______\u001b[0m erally gives\n",
      "\u001b[92merally\u001b[0m \t\t phenomenon gen- \u001b[91m______\u001b[0m gives us\n",
      "\u001b[92mgives\u001b[0m \t\t gen- erally \u001b[91m______\u001b[0m us reason\n",
      "\u001b[92mus\u001b[0m \t\t erally gives \u001b[91m______\u001b[0m reason to\n",
      "\u001b[92mreason\u001b[0m \t\t gives us \u001b[91m______\u001b[0m to view\n",
      "\u001b[92mto\u001b[0m \t\t us reason \u001b[91m______\u001b[0m view the\n",
      "\u001b[92mview\u001b[0m \t\t reason to \u001b[91m______\u001b[0m the relation\n",
      "\u001b[92mthe\u001b[0m \t\t to view \u001b[91m______\u001b[0m relation between\n",
      "\u001b[92mrelation\u001b[0m \t view the \u001b[91m______\u001b[0m between ,\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91m______\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91m______\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91m______\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91m______\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91m______\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91m______\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[91m______\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[91m______\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
      "\u001b[92mand\u001b[0m \t\t s syntax \u001b[91m______\u001b[0m its semantics\n",
      "\u001b[92mits\u001b[0m \t\t syntax and \u001b[91m______\u001b[0m semantics ,\n",
      "\u001b[92msemantics\u001b[0m \t and its \u001b[91m______\u001b[0m , as\n",
      "\u001b[92m,\u001b[0m \t\t its semantics \u001b[91m______\u001b[0m as motivated\n",
      "\u001b[92mas\u001b[0m \t\t semantics , \u001b[91m______\u001b[0m motivated rather\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91m______\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91m______\u001b[0m arbitrary .\n"
     ]
    }
   ],
   "source": [
    "from lazyme import color_str\n",
    "\n",
    "def visualize_predictions(x, y, prediction, vocab, window_size, unk='<unk>'):\n",
    "    left = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[:window_size]])\n",
    "    right = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[window_size:]])\n",
    "    target = vocab.get(int(y), '<unk>')\n",
    "\n",
    "    if not prediction:\n",
    "        predicted_word = '______'\n",
    "    else:\n",
    "        predicted_word = vocab.get(int(prediction), '<unk>') \n",
    "    print(color_str(target, 'green'), '\\t' if len(target) > 6 else '\\t\\t', \n",
    "          left, color_str(predicted_word, 'green' if target == predicted_word else 'red'), right)\n",
    "    \n",
    "device='cpu'\n",
    "sent_idx = 10\n",
    "window_size = 2\n",
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')\n",
    "print(' '.join(w2v_dataset.sents[sent_idx]))\n",
    "for w2v_io in w2v_dataset[sent_idx]:\n",
    "    context, target = w2v_io\n",
    "    context, target = tensor(context).to(device), tensor(target).to(device)\n",
    "    visualize_predictions(context, target, None, w2v_dataset.vocab, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-cbow-model\"></a>\n",
    "\n",
    "## Fill-in the code for the CBOW Model\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=\"500\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Image from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets take a closer look from the inputs to the first `nn.Linear`\n",
    "\n",
    "Cos after it reach the first `nn.Linear` it's just the same as our multi-layered perceptron example =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10,  8,  0,  7])\n",
      "tensor(11)\n"
     ]
    }
   ],
   "source": [
    "# Lets take a look at the first output.\n",
    "x, y = w2v_dataset[0][0][0],  w2v_dataset[0][0][1], \n",
    "\n",
    "x = tensor(x)\n",
    "y = autograd.Variable(tensor(y, dtype=torch.long))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[ 0.7335,  1.5068, -0.5755, -1.6879,  0.9452],\n",
       "                      [ 0.9425,  0.4316,  1.0516,  1.1749,  0.3975],\n",
       "                      [-2.5105, -0.4638, -0.4089,  1.2706,  0.0488],\n",
       "                      ...,\n",
       "                      [-0.0080,  1.0538,  0.2862, -0.4308,  0.5077],\n",
       "                      [ 0.8857, -1.0650, -0.0785, -1.4148, -0.7635],\n",
       "                      [-0.5328,  0.1525, -2.1107, -0.2849,  0.3623]]))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_size = 5\n",
    "emb = nn.Embedding(len(w2v_dataset.vocab), embd_size)\n",
    "emb.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1388, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7335,  1.5068, -0.5755, -1.6879,  0.9452],\n",
       "        [ 0.9425,  0.4316,  1.0516,  1.1749,  0.3975],\n",
       "        [-2.5105, -0.4638, -0.4089,  1.2706,  0.0488],\n",
       "        ...,\n",
       "        [-0.0080,  1.0538,  0.2862, -0.4308,  0.5077],\n",
       "        [ 0.8857, -1.0650, -0.0785, -1.4148, -0.7635],\n",
       "        [-0.5328,  0.1525, -2.1107, -0.2849,  0.3623]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(emb.state_dict()['weight'].shape)\n",
    "emb.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n",
      "tensor([[-1.9476, -0.2496, -0.8896, -0.7262,  1.0757],\n",
      "        [ 0.0027,  0.4123, -0.5250,  1.1259, -0.3427],\n",
      "        [ 0.7335,  1.5068, -0.5755, -1.6879,  0.9452],\n",
      "        [-2.0829, -0.0792,  0.3238,  0.4219,  1.2362]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(emb(x).shape)\n",
    "print(emb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9476, -0.2496, -0.8896, -0.7262,  1.0757,  0.0027,  0.4123, -0.5250,\n",
       "          1.1259, -0.3427,  0.7335,  1.5068, -0.5755, -1.6879,  0.9452, -2.0829,\n",
       "         -0.0792,  0.3238,  0.4219,  1.2362]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(emb(x).view(1, -1).shape)\n",
    "emb(x).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.1841,  0.0522,  0.2199,  ..., -0.1687, -0.1509,  0.0999],\n",
      "        [-0.0486,  0.0334,  0.0295,  ...,  0.1646,  0.0800,  0.1874],\n",
      "        [-0.1359, -0.1871,  0.1793,  ...,  0.1134, -0.1641,  0.0620],\n",
      "        ...,\n",
      "        [ 0.0740,  0.1689,  0.2059,  ..., -0.0074, -0.2086, -0.1302],\n",
      "        [ 0.0827, -0.1457, -0.0466,  ..., -0.0417,  0.0796, -0.0143],\n",
      "        [ 0.0882, -0.0164, -0.1430,  ...,  0.1532, -0.1963, -0.0777]])), ('bias', tensor([ 0.1782, -0.1608, -0.0011, -0.0666, -0.0324, -0.2057,  0.1821, -0.1774,\n",
      "        -0.1428,  0.2019, -0.1390, -0.0183,  0.0534, -0.1038, -0.1622,  0.0726,\n",
      "         0.0967, -0.2143, -0.1682, -0.0557, -0.2070,  0.1639,  0.1866,  0.1446,\n",
      "        -0.2112,  0.0257,  0.2067,  0.1014,  0.2035,  0.2028, -0.1373, -0.1241,\n",
      "        -0.2136,  0.2006,  0.1042, -0.0249,  0.1957,  0.1049,  0.0790, -0.1374,\n",
      "        -0.0625,  0.2042,  0.2078, -0.1721,  0.1658,  0.0389,  0.1277, -0.0313,\n",
      "        -0.2058, -0.1762,  0.1194, -0.1901,  0.0342,  0.2091, -0.0574, -0.2091,\n",
      "         0.0771, -0.0022,  0.1579,  0.0241, -0.1942, -0.1523,  0.0475, -0.1456,\n",
      "        -0.1762,  0.1815,  0.1513, -0.2236,  0.1971, -0.0630, -0.0890, -0.1913,\n",
      "        -0.1025,  0.0130, -0.0201,  0.2095, -0.1792, -0.0203,  0.1215, -0.1261,\n",
      "        -0.0601,  0.1380, -0.0982, -0.1652, -0.1055,  0.2007, -0.0226,  0.2058,\n",
      "        -0.1162, -0.1283, -0.1980,  0.1189, -0.1902, -0.0103,  0.0655, -0.1373,\n",
      "        -0.1186, -0.1777, -0.2087, -0.0143]))])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "lin1 = nn.Linear(len(x)*embd_size, hidden_size)\n",
    "print(lin1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 20])\n",
      "tensor([[-0.1841,  0.0522,  0.2199,  ..., -0.1687, -0.1509,  0.0999],\n",
      "        [-0.0486,  0.0334,  0.0295,  ...,  0.1646,  0.0800,  0.1874],\n",
      "        [-0.1359, -0.1871,  0.1793,  ...,  0.1134, -0.1641,  0.0620],\n",
      "        ...,\n",
      "        [ 0.0740,  0.1689,  0.2059,  ..., -0.0074, -0.2086, -0.1302],\n",
      "        [ 0.0827, -0.1457, -0.0466,  ..., -0.0417,  0.0796, -0.0143],\n",
      "        [ 0.0882, -0.0164, -0.1430,  ...,  0.1532, -0.1963, -0.0777]])\n"
     ]
    }
   ],
   "source": [
    "print(lin1.state_dict()['weight'].shape)\n",
    "print(lin1.state_dict()['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0888, -1.0088,  0.0965,  0.2756,  0.1797, -0.5882,  0.3243, -0.3393,\n",
       "         -1.1270,  0.0161, -0.3290,  0.1747, -0.3472, -0.0795, -0.1349, -0.1686,\n",
       "          0.5079, -0.3324, -0.2411, -0.7249, -0.8540, -0.7165,  0.1019,  0.4911,\n",
       "         -0.5670,  0.4936, -0.6383,  0.3734,  0.1678, -0.1616, -0.5569, -0.9486,\n",
       "         -0.0304,  0.6370,  0.1391,  0.4529,  0.1172,  0.3384, -0.4796,  0.2601,\n",
       "         -0.6335, -1.6057, -0.4283,  0.2449,  0.6225, -0.8162, -0.2052, -0.1767,\n",
       "          0.9683,  1.1005, -0.6824,  0.4383,  0.3045, -0.8560,  0.7744, -0.8682,\n",
       "          0.9671, -0.1826,  0.0504, -0.0621,  0.3957,  0.4055,  0.1122,  0.0508,\n",
       "         -0.5659, -0.2446, -0.2561,  0.1710,  0.8264,  0.9718, -0.1567, -0.8874,\n",
       "          0.1698, -1.0425, -0.7192,  0.5400, -0.5012,  0.1647, -0.7856, -0.5713,\n",
       "         -0.1210, -0.7652, -0.7464,  0.4228, -0.3050,  1.3325, -0.0095,  0.5470,\n",
       "         -0.6679,  0.4492, -0.5300, -0.2526, -0.6127, -1.3180, -0.4840,  0.5257,\n",
       "         -0.5755, -0.2315,  0.1337, -0.1486]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lin1(emb(x).view(1, -1)).shape)\n",
    "lin1(emb(x).view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0888, 0.0000, 0.0965, 0.2756, 0.1797, 0.0000, 0.3243, 0.0000, 0.0000,\n",
       "         0.0161, 0.0000, 0.1747, 0.0000, 0.0000, 0.0000, 0.0000, 0.5079, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.1019, 0.4911, 0.0000, 0.4936, 0.0000,\n",
       "         0.3734, 0.1678, 0.0000, 0.0000, 0.0000, 0.0000, 0.6370, 0.1391, 0.4529,\n",
       "         0.1172, 0.3384, 0.0000, 0.2601, 0.0000, 0.0000, 0.0000, 0.2449, 0.6225,\n",
       "         0.0000, 0.0000, 0.0000, 0.9683, 1.1005, 0.0000, 0.4383, 0.3045, 0.0000,\n",
       "         0.7744, 0.0000, 0.9671, 0.0000, 0.0504, 0.0000, 0.3957, 0.4055, 0.1122,\n",
       "         0.0508, 0.0000, 0.0000, 0.0000, 0.1710, 0.8264, 0.9718, 0.0000, 0.0000,\n",
       "         0.1698, 0.0000, 0.0000, 0.5400, 0.0000, 0.1647, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.4228, 0.0000, 1.3325, 0.0000, 0.5470, 0.0000, 0.4492,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5257, 0.0000, 0.0000, 0.1337,\n",
       "         0.0000]], grad_fn=<ThresholdBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = nn.ReLU()\n",
    "print(relu(lin1(emb(x).view(1, -1))).shape)\n",
    "relu(lin1(emb(x).view(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1388, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0093, -0.0500, -0.0182,  ...,  0.0210, -0.0662, -0.0196],\n",
       "        [ 0.0894,  0.0349, -0.0460,  ..., -0.0203,  0.0277, -0.0457],\n",
       "        [ 0.0996,  0.0642,  0.0327,  ...,  0.0923,  0.0946,  0.0245],\n",
       "        ...,\n",
       "        [-0.0965, -0.0808, -0.0214,  ...,  0.0966, -0.0496, -0.0584],\n",
       "        [ 0.0265,  0.0851, -0.0604,  ..., -0.0978,  0.0749,  0.0669],\n",
       "        [-0.0632,  0.0624, -0.0602,  ...,  0.0833,  0.0041, -0.0210]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin2 = nn.Linear(hidden_size, len(w2v_dataset.vocab))\n",
    "print(lin2.state_dict()['weight'].shape)\n",
    "lin2.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1388])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2339,  0.3535, -0.2208,  ...,  0.0834,  0.0487,  0.1954]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_x = relu(lin1(emb(x).view(1, -1)))\n",
    "print(lin2(h_x).shape)\n",
    "lin2(h_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-7.026632308959961,\n",
       "  -6.907124996185303,\n",
       "  -7.4813947677612305,\n",
       "  -7.3909196853637695,\n",
       "  -7.239814281463623,\n",
       "  -7.540319442749023,\n",
       "  -7.456295967102051,\n",
       "  -7.424745559692383,\n",
       "  -7.198200702667236,\n",
       "  -7.306450366973877,\n",
       "  -7.160186767578125,\n",
       "  -7.522230625152588,\n",
       "  -7.079560279846191,\n",
       "  -7.514706611633301,\n",
       "  -7.365232467651367,\n",
       "  -7.693469524383545,\n",
       "  -7.305044174194336,\n",
       "  -7.411624431610107,\n",
       "  -7.21847677230835,\n",
       "  -7.394266128540039,\n",
       "  -7.056638240814209,\n",
       "  -7.264041423797607,\n",
       "  -7.241987228393555,\n",
       "  -7.237301826477051,\n",
       "  -7.260227203369141,\n",
       "  -7.019230365753174,\n",
       "  -7.0644917488098145,\n",
       "  -7.718108177185059,\n",
       "  -7.329542636871338,\n",
       "  -7.173532485961914,\n",
       "  -7.417546272277832,\n",
       "  -7.222652912139893,\n",
       "  -7.128057479858398,\n",
       "  -7.118430137634277,\n",
       "  -7.19818115234375,\n",
       "  -7.476899147033691,\n",
       "  -7.150943756103516,\n",
       "  -7.399081707000732,\n",
       "  -7.225815773010254,\n",
       "  -7.49507474899292,\n",
       "  -7.415511608123779,\n",
       "  -7.4867448806762695,\n",
       "  -7.331450462341309,\n",
       "  -7.36598014831543,\n",
       "  -7.291879177093506,\n",
       "  -7.4501214027404785,\n",
       "  -7.408347129821777,\n",
       "  -7.153469085693359,\n",
       "  -6.9428887367248535,\n",
       "  -7.252878189086914,\n",
       "  -7.412710189819336,\n",
       "  -7.230688571929932,\n",
       "  -7.450641632080078,\n",
       "  -6.9693803787231445,\n",
       "  -7.321620941162109,\n",
       "  -7.18576192855835,\n",
       "  -7.084728717803955,\n",
       "  -7.611446380615234,\n",
       "  -7.383947372436523,\n",
       "  -7.1829118728637695,\n",
       "  -6.940511226654053,\n",
       "  -7.035731315612793,\n",
       "  -7.246544361114502,\n",
       "  -7.051834583282471,\n",
       "  -7.616437911987305,\n",
       "  -6.867559909820557,\n",
       "  -7.180415153503418,\n",
       "  -7.476165294647217,\n",
       "  -7.185801982879639,\n",
       "  -6.954156398773193,\n",
       "  -6.7937726974487305,\n",
       "  -7.081499099731445,\n",
       "  -7.063412189483643,\n",
       "  -7.491771697998047,\n",
       "  -7.157018661499023,\n",
       "  -7.298053741455078,\n",
       "  -7.354499340057373,\n",
       "  -7.32025671005249,\n",
       "  -7.479083061218262,\n",
       "  -6.958202362060547,\n",
       "  -7.036192417144775,\n",
       "  -7.218421459197998,\n",
       "  -7.392274856567383,\n",
       "  -7.379844665527344,\n",
       "  -7.631824493408203,\n",
       "  -6.987310886383057,\n",
       "  -7.062452793121338,\n",
       "  -7.235650539398193,\n",
       "  -7.236778259277344,\n",
       "  -7.223626613616943,\n",
       "  -7.49542760848999,\n",
       "  -7.295275688171387,\n",
       "  -7.38722562789917,\n",
       "  -7.416998863220215,\n",
       "  -7.4632768630981445,\n",
       "  -7.318082332611084,\n",
       "  -7.021761894226074,\n",
       "  -7.358774185180664,\n",
       "  -7.552628040313721,\n",
       "  -7.159389972686768,\n",
       "  -7.036665439605713,\n",
       "  -7.513729095458984,\n",
       "  -7.054421424865723,\n",
       "  -7.439234256744385,\n",
       "  -7.497610569000244,\n",
       "  -7.700373649597168,\n",
       "  -7.3076043128967285,\n",
       "  -7.149777412414551,\n",
       "  -7.275280952453613,\n",
       "  -6.751932144165039,\n",
       "  -7.391146659851074,\n",
       "  -6.976258277893066,\n",
       "  -7.392076015472412,\n",
       "  -7.2612152099609375,\n",
       "  -7.805944919586182,\n",
       "  -7.266787528991699,\n",
       "  -7.615403652191162,\n",
       "  -7.629860877990723,\n",
       "  -7.108407020568848,\n",
       "  -7.0028276443481445,\n",
       "  -7.0407609939575195,\n",
       "  -7.059507369995117,\n",
       "  -7.02193021774292,\n",
       "  -7.002498626708984,\n",
       "  -7.095276832580566,\n",
       "  -7.165083885192871,\n",
       "  -7.190350532531738,\n",
       "  -6.940236568450928,\n",
       "  -6.992949485778809,\n",
       "  -6.973491191864014,\n",
       "  -7.159219264984131,\n",
       "  -7.123521327972412,\n",
       "  -7.387359142303467,\n",
       "  -7.352746486663818,\n",
       "  -7.248182773590088,\n",
       "  -7.088106155395508,\n",
       "  -7.095323085784912,\n",
       "  -7.089377403259277,\n",
       "  -7.114541053771973,\n",
       "  -7.309491157531738,\n",
       "  -7.079226493835449,\n",
       "  -7.142462253570557,\n",
       "  -6.977821350097656,\n",
       "  -7.242265701293945,\n",
       "  -7.605428218841553,\n",
       "  -7.3282790184021,\n",
       "  -7.288390159606934,\n",
       "  -7.165751934051514,\n",
       "  -7.137239456176758,\n",
       "  -7.131405353546143,\n",
       "  -7.613722801208496,\n",
       "  -7.050846576690674,\n",
       "  -7.385867595672607,\n",
       "  -7.15763521194458,\n",
       "  -7.3025383949279785,\n",
       "  -7.4725189208984375,\n",
       "  -7.065178394317627,\n",
       "  -7.231518268585205,\n",
       "  -6.839097499847412,\n",
       "  -7.246969699859619,\n",
       "  -6.8069963455200195,\n",
       "  -7.269848823547363,\n",
       "  -7.36909294128418,\n",
       "  -7.096888542175293,\n",
       "  -7.3347859382629395,\n",
       "  -7.578384876251221,\n",
       "  -7.204959869384766,\n",
       "  -7.12246036529541,\n",
       "  -6.787686824798584,\n",
       "  -7.3505706787109375,\n",
       "  -7.520174980163574,\n",
       "  -7.691651344299316,\n",
       "  -7.308344841003418,\n",
       "  -7.138230323791504,\n",
       "  -6.880875587463379,\n",
       "  -7.341044902801514,\n",
       "  -7.303077220916748,\n",
       "  -7.163924217224121,\n",
       "  -7.142710208892822,\n",
       "  -7.086395263671875,\n",
       "  -7.275447845458984,\n",
       "  -7.203489780426025,\n",
       "  -7.510344505310059,\n",
       "  -7.343838214874268,\n",
       "  -7.490536689758301,\n",
       "  -7.233268737792969,\n",
       "  -7.279354572296143,\n",
       "  -6.948267936706543,\n",
       "  -7.375680446624756,\n",
       "  -7.088876247406006,\n",
       "  -7.317090034484863,\n",
       "  -7.275096893310547,\n",
       "  -7.090513706207275,\n",
       "  -7.08235502243042,\n",
       "  -7.170477867126465,\n",
       "  -7.138206481933594,\n",
       "  -7.229911804199219,\n",
       "  -7.605103492736816,\n",
       "  -7.371068477630615,\n",
       "  -7.1977338790893555,\n",
       "  -7.470440864562988,\n",
       "  -7.160211086273193,\n",
       "  -7.030702590942383,\n",
       "  -7.172897815704346,\n",
       "  -6.9859185218811035,\n",
       "  -7.397062301635742,\n",
       "  -7.131474018096924,\n",
       "  -7.163399696350098,\n",
       "  -7.183882713317871,\n",
       "  -7.332397937774658,\n",
       "  -6.948509693145752,\n",
       "  -7.15344762802124,\n",
       "  -7.444509983062744,\n",
       "  -7.276329517364502,\n",
       "  -7.152207851409912,\n",
       "  -6.906001091003418,\n",
       "  -7.344515800476074,\n",
       "  -6.934169769287109,\n",
       "  -7.419843673706055,\n",
       "  -7.0609660148620605,\n",
       "  -7.252878189086914,\n",
       "  -7.146534442901611,\n",
       "  -7.225045680999756,\n",
       "  -7.349762439727783,\n",
       "  -7.45332670211792,\n",
       "  -7.418032169342041,\n",
       "  -7.708375453948975,\n",
       "  -7.449377536773682,\n",
       "  -6.992384433746338,\n",
       "  -7.715566635131836,\n",
       "  -7.174504280090332,\n",
       "  -7.2955641746521,\n",
       "  -7.350031852722168,\n",
       "  -7.501272201538086,\n",
       "  -7.0172624588012695,\n",
       "  -7.109353065490723,\n",
       "  -7.2571821212768555,\n",
       "  -7.129148006439209,\n",
       "  -7.126041889190674,\n",
       "  -7.298159122467041,\n",
       "  -7.318983554840088,\n",
       "  -7.365400791168213,\n",
       "  -7.2361531257629395,\n",
       "  -7.037802219390869,\n",
       "  -7.099120616912842,\n",
       "  -7.600142002105713,\n",
       "  -7.419136047363281,\n",
       "  -7.277111053466797,\n",
       "  -7.358712196350098,\n",
       "  -6.811097145080566,\n",
       "  -7.180905818939209,\n",
       "  -7.179861545562744,\n",
       "  -7.362267971038818,\n",
       "  -7.725729465484619,\n",
       "  -7.026488780975342,\n",
       "  -6.895756244659424,\n",
       "  -7.251706600189209,\n",
       "  -7.05031681060791,\n",
       "  -7.638610363006592,\n",
       "  -7.371417045593262,\n",
       "  -7.205596446990967,\n",
       "  -6.860536575317383,\n",
       "  -7.148355007171631,\n",
       "  -7.1648454666137695,\n",
       "  -7.260303974151611,\n",
       "  -7.243159770965576,\n",
       "  -7.498661518096924,\n",
       "  -6.86133337020874,\n",
       "  -7.278422832489014,\n",
       "  -7.310127258300781,\n",
       "  -7.252641677856445,\n",
       "  -7.2591142654418945,\n",
       "  -7.134693622589111,\n",
       "  -7.635846138000488,\n",
       "  -7.013103008270264,\n",
       "  -7.044005870819092,\n",
       "  -7.313253402709961,\n",
       "  -7.327666759490967,\n",
       "  -7.225832462310791,\n",
       "  -7.2032294273376465,\n",
       "  -7.245553970336914,\n",
       "  -7.143030166625977,\n",
       "  -7.105511665344238,\n",
       "  -7.4911627769470215,\n",
       "  -7.331769943237305,\n",
       "  -7.267956256866455,\n",
       "  -7.007958889007568,\n",
       "  -7.472223281860352,\n",
       "  -6.916428565979004,\n",
       "  -7.3646111488342285,\n",
       "  -7.3447651863098145,\n",
       "  -6.651465892791748,\n",
       "  -7.614923477172852,\n",
       "  -7.355011940002441,\n",
       "  -7.626663684844971,\n",
       "  -7.428736209869385,\n",
       "  -7.5824127197265625,\n",
       "  -7.108640193939209,\n",
       "  -7.287755966186523,\n",
       "  -6.899563789367676,\n",
       "  -7.049969673156738,\n",
       "  -6.896721839904785,\n",
       "  -7.327912330627441,\n",
       "  -7.322720527648926,\n",
       "  -6.904088497161865,\n",
       "  -7.658978462219238,\n",
       "  -7.240240097045898,\n",
       "  -7.441587448120117,\n",
       "  -7.276730060577393,\n",
       "  -7.446441173553467,\n",
       "  -7.331225872039795,\n",
       "  -7.470972537994385,\n",
       "  -7.622131824493408,\n",
       "  -7.260782241821289,\n",
       "  -7.030794143676758,\n",
       "  -7.433017253875732,\n",
       "  -7.160493850708008,\n",
       "  -7.245795726776123,\n",
       "  -7.264305591583252,\n",
       "  -7.322071552276611,\n",
       "  -7.072441577911377,\n",
       "  -6.870544910430908,\n",
       "  -6.954579830169678,\n",
       "  -7.1947784423828125,\n",
       "  -7.170607089996338,\n",
       "  -7.710254192352295,\n",
       "  -6.8633198738098145,\n",
       "  -7.5865983963012695,\n",
       "  -7.213120460510254,\n",
       "  -7.4878411293029785,\n",
       "  -7.243922233581543,\n",
       "  -7.223473072052002,\n",
       "  -7.308620929718018,\n",
       "  -7.174033164978027,\n",
       "  -7.1156005859375,\n",
       "  -7.218954563140869,\n",
       "  -7.33223295211792,\n",
       "  -7.342102527618408,\n",
       "  -7.19484806060791,\n",
       "  -7.212551116943359,\n",
       "  -7.345004558563232,\n",
       "  -7.330190181732178,\n",
       "  -7.520107269287109,\n",
       "  -7.220156669616699,\n",
       "  -7.3240461349487305,\n",
       "  -7.018653392791748,\n",
       "  -7.085709571838379,\n",
       "  -7.446810245513916,\n",
       "  -6.8392510414123535,\n",
       "  -7.594158172607422,\n",
       "  -6.881353855133057,\n",
       "  -7.094038009643555,\n",
       "  -6.799812316894531,\n",
       "  -7.2172160148620605,\n",
       "  -7.155668258666992,\n",
       "  -7.155559539794922,\n",
       "  -6.960511684417725,\n",
       "  -7.080467700958252,\n",
       "  -7.235867500305176,\n",
       "  -7.758676528930664,\n",
       "  -6.905130863189697,\n",
       "  -7.214045524597168,\n",
       "  -6.825980186462402,\n",
       "  -7.104270935058594,\n",
       "  -7.148382663726807,\n",
       "  -7.200598239898682,\n",
       "  -7.676844596862793,\n",
       "  -7.711111068725586,\n",
       "  -7.32901668548584,\n",
       "  -7.062483787536621,\n",
       "  -7.452307224273682,\n",
       "  -7.460933685302734,\n",
       "  -7.488216400146484,\n",
       "  -7.584674835205078,\n",
       "  -7.220744609832764,\n",
       "  -7.2059526443481445,\n",
       "  -6.7005295753479,\n",
       "  -7.483287811279297,\n",
       "  -7.2235331535339355,\n",
       "  -6.900755882263184,\n",
       "  -7.410041809082031,\n",
       "  -6.783103942871094,\n",
       "  -6.964832305908203,\n",
       "  -7.28420877456665,\n",
       "  -7.195919036865234,\n",
       "  -7.294503211975098,\n",
       "  -6.843315124511719,\n",
       "  -7.358775615692139,\n",
       "  -7.251833438873291,\n",
       "  -7.204257965087891,\n",
       "  -7.356752872467041,\n",
       "  -7.108847618103027,\n",
       "  -7.295232772827148,\n",
       "  -6.928206920623779,\n",
       "  -7.076173305511475,\n",
       "  -7.255738735198975,\n",
       "  -7.089733600616455,\n",
       "  -7.013730525970459,\n",
       "  -7.523929595947266,\n",
       "  -7.170836448669434,\n",
       "  -7.122270584106445,\n",
       "  -7.498308181762695,\n",
       "  -7.394528388977051,\n",
       "  -7.264378070831299,\n",
       "  -6.949496746063232,\n",
       "  -7.409923553466797,\n",
       "  -7.3724045753479,\n",
       "  -7.085265159606934,\n",
       "  -7.170036315917969,\n",
       "  -7.139884948730469,\n",
       "  -7.422072887420654,\n",
       "  -7.484200954437256,\n",
       "  -7.446052074432373,\n",
       "  -7.30455207824707,\n",
       "  -7.427527904510498,\n",
       "  -7.207287788391113,\n",
       "  -7.349334239959717,\n",
       "  -7.524272441864014,\n",
       "  -7.346651077270508,\n",
       "  -7.264131546020508,\n",
       "  -7.404611587524414,\n",
       "  -7.106387138366699,\n",
       "  -7.102144241333008,\n",
       "  -7.461264610290527,\n",
       "  -7.1064863204956055,\n",
       "  -7.4455037117004395,\n",
       "  -6.918097972869873,\n",
       "  -7.208919048309326,\n",
       "  -7.112027168273926,\n",
       "  -7.241739749908447,\n",
       "  -7.133418560028076,\n",
       "  -7.3931565284729,\n",
       "  -7.05752420425415,\n",
       "  -7.13381290435791,\n",
       "  -7.195865154266357,\n",
       "  -7.165806293487549,\n",
       "  -7.288533687591553,\n",
       "  -7.0404839515686035,\n",
       "  -7.22562837600708,\n",
       "  -7.091878414154053,\n",
       "  -7.192401885986328,\n",
       "  -7.118844032287598,\n",
       "  -7.396537780761719,\n",
       "  -7.196606636047363,\n",
       "  -6.6556525230407715,\n",
       "  -7.36494255065918,\n",
       "  -7.577099323272705,\n",
       "  -7.293577671051025,\n",
       "  -7.607857704162598,\n",
       "  -7.280890464782715,\n",
       "  -7.418797016143799,\n",
       "  -7.423347473144531,\n",
       "  -6.926289081573486,\n",
       "  -7.567205429077148,\n",
       "  -6.921732425689697,\n",
       "  -7.313319683074951,\n",
       "  -7.15174674987793,\n",
       "  -7.036425590515137,\n",
       "  -7.308507442474365,\n",
       "  -7.622293472290039,\n",
       "  -7.464636325836182,\n",
       "  -7.045578956604004,\n",
       "  -7.643455505371094,\n",
       "  -7.3577704429626465,\n",
       "  -7.1588897705078125,\n",
       "  -7.405746936798096,\n",
       "  -7.540650367736816,\n",
       "  -7.291934967041016,\n",
       "  -7.1381707191467285,\n",
       "  -7.5083417892456055,\n",
       "  -7.371061325073242,\n",
       "  -7.415656566619873,\n",
       "  -7.312498092651367,\n",
       "  -7.728673934936523,\n",
       "  -7.366708755493164,\n",
       "  -6.842895984649658,\n",
       "  -7.260918617248535,\n",
       "  -7.24386739730835,\n",
       "  -7.423617839813232,\n",
       "  -7.128383159637451,\n",
       "  -7.375665664672852,\n",
       "  -7.360020637512207,\n",
       "  -6.924428462982178,\n",
       "  -7.227920055389404,\n",
       "  -7.185912132263184,\n",
       "  -7.309560298919678,\n",
       "  -7.278670787811279,\n",
       "  -7.098551273345947,\n",
       "  -6.978446006774902,\n",
       "  -7.034008026123047,\n",
       "  -7.401645660400391,\n",
       "  -7.428108215332031,\n",
       "  -7.225152015686035,\n",
       "  -6.987658977508545,\n",
       "  -7.250363349914551,\n",
       "  -7.013880252838135,\n",
       "  -7.432135581970215,\n",
       "  -7.495203495025635,\n",
       "  -7.437432289123535,\n",
       "  -7.6607537269592285,\n",
       "  -7.535689353942871,\n",
       "  -7.008119583129883,\n",
       "  -7.235016345977783,\n",
       "  -7.0733771324157715,\n",
       "  -7.503575325012207,\n",
       "  -7.0968918800354,\n",
       "  -7.003355979919434,\n",
       "  -6.992273330688477,\n",
       "  -7.059150218963623,\n",
       "  -7.271511554718018,\n",
       "  -7.172077655792236,\n",
       "  -7.394398212432861,\n",
       "  -7.386453628540039,\n",
       "  -7.524303913116455,\n",
       "  -7.199219226837158,\n",
       "  -7.477310657501221,\n",
       "  -7.123281478881836,\n",
       "  -7.056666851043701,\n",
       "  -7.236987590789795,\n",
       "  -7.257155418395996,\n",
       "  -7.202383041381836,\n",
       "  -7.032574653625488,\n",
       "  -7.13664436340332,\n",
       "  -7.335511207580566,\n",
       "  -7.452840328216553,\n",
       "  -7.323960304260254,\n",
       "  -7.527449607849121,\n",
       "  -7.4967780113220215,\n",
       "  -7.325864791870117,\n",
       "  -7.0398478507995605,\n",
       "  -7.4100775718688965,\n",
       "  -7.015844821929932,\n",
       "  -7.008803844451904,\n",
       "  -7.352635383605957,\n",
       "  -6.972321033477783,\n",
       "  -6.943093776702881,\n",
       "  -7.189321517944336,\n",
       "  -7.418688774108887,\n",
       "  -7.356063365936279,\n",
       "  -7.218985080718994,\n",
       "  -7.409236907958984,\n",
       "  -7.0735392570495605,\n",
       "  -7.020015716552734,\n",
       "  -7.330461502075195,\n",
       "  -7.386878490447998,\n",
       "  -7.407071113586426,\n",
       "  -7.084856986999512,\n",
       "  -7.20134973526001,\n",
       "  -7.357419490814209,\n",
       "  -7.1932268142700195,\n",
       "  -7.139797210693359,\n",
       "  -7.0985426902771,\n",
       "  -7.461699485778809,\n",
       "  -7.020185470581055,\n",
       "  -7.043531894683838,\n",
       "  -7.735710620880127,\n",
       "  -7.031522274017334,\n",
       "  -7.5445966720581055,\n",
       "  -7.833362102508545,\n",
       "  -7.277371406555176,\n",
       "  -7.400688171386719,\n",
       "  -7.381545066833496,\n",
       "  -7.3293938636779785,\n",
       "  -7.185413360595703,\n",
       "  -6.786348819732666,\n",
       "  -7.134806156158447,\n",
       "  -7.351681709289551,\n",
       "  -7.677427291870117,\n",
       "  -7.3516845703125,\n",
       "  -7.479551315307617,\n",
       "  -7.29403829574585,\n",
       "  -7.025954723358154,\n",
       "  -6.947612285614014,\n",
       "  -7.582969665527344,\n",
       "  -7.2410736083984375,\n",
       "  -7.4024858474731445,\n",
       "  -7.236425399780273,\n",
       "  -7.01045036315918,\n",
       "  -7.141972541809082,\n",
       "  -7.30496072769165,\n",
       "  -7.076226234436035,\n",
       "  -7.356064319610596,\n",
       "  -6.983067989349365,\n",
       "  -7.34001350402832,\n",
       "  -7.087943077087402,\n",
       "  -7.335561275482178,\n",
       "  -6.882660388946533,\n",
       "  -7.284486770629883,\n",
       "  -7.179574489593506,\n",
       "  -7.823006629943848,\n",
       "  -6.921296119689941,\n",
       "  -7.399470329284668,\n",
       "  -6.990865707397461,\n",
       "  -7.125630855560303,\n",
       "  -7.005828857421875,\n",
       "  -7.368725776672363,\n",
       "  -7.281115531921387,\n",
       "  -7.216180324554443,\n",
       "  -7.3484649658203125,\n",
       "  -7.262959957122803,\n",
       "  -7.449732303619385,\n",
       "  -7.003993034362793,\n",
       "  -7.493220329284668,\n",
       "  -7.062230587005615,\n",
       "  -7.22193717956543,\n",
       "  -7.035979270935059,\n",
       "  -7.345901012420654,\n",
       "  -7.278379440307617,\n",
       "  -7.169912815093994,\n",
       "  -7.669895648956299,\n",
       "  -7.12703800201416,\n",
       "  -7.236288547515869,\n",
       "  -7.427118301391602,\n",
       "  -7.539922714233398,\n",
       "  -7.252152442932129,\n",
       "  -7.498621463775635,\n",
       "  -7.511430740356445,\n",
       "  -7.195030212402344,\n",
       "  -7.078521728515625,\n",
       "  -7.2390265464782715,\n",
       "  -7.498377799987793,\n",
       "  -7.357778072357178,\n",
       "  -7.525160789489746,\n",
       "  -6.759367942810059,\n",
       "  -7.319282531738281,\n",
       "  -7.120555400848389,\n",
       "  -7.420724868774414,\n",
       "  -7.351895809173584,\n",
       "  -6.8326215744018555,\n",
       "  -7.0373029708862305,\n",
       "  -7.438601016998291,\n",
       "  -7.329860687255859,\n",
       "  -7.411979675292969,\n",
       "  -7.1840715408325195,\n",
       "  -7.431294918060303,\n",
       "  -7.097255229949951,\n",
       "  -7.305974006652832,\n",
       "  -7.414275646209717,\n",
       "  -7.079245567321777,\n",
       "  -7.029892444610596,\n",
       "  -7.396251201629639,\n",
       "  -7.178050518035889,\n",
       "  -7.203548908233643,\n",
       "  -7.5869598388671875,\n",
       "  -7.546645164489746,\n",
       "  -7.000970363616943,\n",
       "  -7.326857566833496,\n",
       "  -7.271718978881836,\n",
       "  -7.445953845977783,\n",
       "  -7.295588970184326,\n",
       "  -7.441403388977051,\n",
       "  -7.3396992683410645,\n",
       "  -7.378527641296387,\n",
       "  -7.641468048095703,\n",
       "  -7.034397125244141,\n",
       "  -6.927710056304932,\n",
       "  -7.33842658996582,\n",
       "  -7.354536056518555,\n",
       "  -7.632081031799316,\n",
       "  -7.309967041015625,\n",
       "  -7.110193729400635,\n",
       "  -7.414278030395508,\n",
       "  -7.403775215148926,\n",
       "  -7.136842250823975,\n",
       "  -7.28069543838501,\n",
       "  -7.11262845993042,\n",
       "  -7.128815650939941,\n",
       "  -7.230787754058838,\n",
       "  -7.304895877838135,\n",
       "  -7.032186508178711,\n",
       "  -7.627646446228027,\n",
       "  -6.918437957763672,\n",
       "  -7.438157081604004,\n",
       "  -7.152191638946533,\n",
       "  -7.114207744598389,\n",
       "  -6.966510772705078,\n",
       "  -7.2778401374816895,\n",
       "  -7.241884708404541,\n",
       "  -7.134880065917969,\n",
       "  -7.088510513305664,\n",
       "  -7.292980194091797,\n",
       "  -7.526577949523926,\n",
       "  -7.427859783172607,\n",
       "  -7.149242877960205,\n",
       "  -6.777839660644531,\n",
       "  -7.304835796356201,\n",
       "  -7.109708309173584,\n",
       "  -7.312902450561523,\n",
       "  -7.270473003387451,\n",
       "  -7.5493621826171875,\n",
       "  -7.415818691253662,\n",
       "  -7.385583877563477,\n",
       "  -6.824288845062256,\n",
       "  -7.402397632598877,\n",
       "  -7.328231334686279,\n",
       "  -6.844458103179932,\n",
       "  -7.400697231292725,\n",
       "  -7.295133113861084,\n",
       "  -7.530788421630859,\n",
       "  -7.434386253356934,\n",
       "  -7.180737018585205,\n",
       "  -7.0430216789245605,\n",
       "  -7.391988277435303,\n",
       "  -7.5049052238464355,\n",
       "  -7.245298385620117,\n",
       "  -7.158644676208496,\n",
       "  -6.853377342224121,\n",
       "  -7.348366737365723,\n",
       "  -6.976221561431885,\n",
       "  -7.252176761627197,\n",
       "  -7.165712356567383,\n",
       "  -6.897287368774414,\n",
       "  -7.354807376861572,\n",
       "  -7.151982307434082,\n",
       "  -7.283552169799805,\n",
       "  -7.060410976409912,\n",
       "  -7.3028244972229,\n",
       "  -7.575455188751221,\n",
       "  -6.728687286376953,\n",
       "  -7.115723133087158,\n",
       "  -7.326900959014893,\n",
       "  -7.080549240112305,\n",
       "  -7.097685813903809,\n",
       "  -7.233916282653809,\n",
       "  -7.194461345672607,\n",
       "  -7.237947940826416,\n",
       "  -6.961482524871826,\n",
       "  -7.679908275604248,\n",
       "  -7.162742614746094,\n",
       "  -7.2483062744140625,\n",
       "  -7.1933135986328125,\n",
       "  -6.950172424316406,\n",
       "  -7.535391330718994,\n",
       "  -7.099305152893066,\n",
       "  -7.163791656494141,\n",
       "  -7.232967853546143,\n",
       "  -7.3930583000183105,\n",
       "  -7.454919338226318,\n",
       "  -7.290860652923584,\n",
       "  -7.183833122253418,\n",
       "  -7.40606164932251,\n",
       "  -7.409912586212158,\n",
       "  -7.343212127685547,\n",
       "  -7.511725902557373,\n",
       "  -7.589091777801514,\n",
       "  -7.0851731300354,\n",
       "  -7.570873260498047,\n",
       "  -7.2416090965271,\n",
       "  -7.376411437988281,\n",
       "  -7.0033440589904785,\n",
       "  -7.308862209320068,\n",
       "  -7.249584674835205,\n",
       "  -7.666335582733154,\n",
       "  -6.9986114501953125,\n",
       "  -7.5943803787231445,\n",
       "  -7.322219371795654,\n",
       "  -6.957415580749512,\n",
       "  -7.335965633392334,\n",
       "  -6.884741306304932,\n",
       "  -7.533960342407227,\n",
       "  -6.800055027008057,\n",
       "  -7.145383834838867,\n",
       "  -7.148357391357422,\n",
       "  -7.575084209442139,\n",
       "  -7.328136444091797,\n",
       "  -7.375488758087158,\n",
       "  -7.056188583374023,\n",
       "  -7.266322135925293,\n",
       "  -7.204972743988037,\n",
       "  -7.265562534332275,\n",
       "  -7.422573089599609,\n",
       "  -7.614680290222168,\n",
       "  -6.755813121795654,\n",
       "  -7.2979230880737305,\n",
       "  -6.882676601409912,\n",
       "  -7.2678399085998535,\n",
       "  -7.497635841369629,\n",
       "  -7.540746212005615,\n",
       "  -7.536545276641846,\n",
       "  -7.453169345855713,\n",
       "  -7.660639762878418,\n",
       "  -6.988923072814941,\n",
       "  -7.286722660064697,\n",
       "  -7.266038417816162,\n",
       "  -7.025489330291748,\n",
       "  -7.340279579162598,\n",
       "  -7.164204120635986,\n",
       "  -7.410148620605469,\n",
       "  -7.303271293640137,\n",
       "  -7.1335344314575195,\n",
       "  -7.223201751708984,\n",
       "  -7.584224224090576,\n",
       "  -7.261157512664795,\n",
       "  -7.142287254333496,\n",
       "  -7.299991130828857,\n",
       "  -7.409460544586182,\n",
       "  -7.416086673736572,\n",
       "  -7.544672012329102,\n",
       "  -7.540752410888672,\n",
       "  -7.701408863067627,\n",
       "  -7.23697566986084,\n",
       "  -7.4870381355285645,\n",
       "  -6.897495746612549,\n",
       "  -7.591802597045898,\n",
       "  -6.956332683563232,\n",
       "  -7.206474781036377,\n",
       "  -7.06087589263916,\n",
       "  -7.45338249206543,\n",
       "  -7.138777732849121,\n",
       "  -7.371273517608643,\n",
       "  -6.9770355224609375,\n",
       "  -7.577913761138916,\n",
       "  -7.309845924377441,\n",
       "  -7.157501220703125,\n",
       "  -7.435267448425293,\n",
       "  -7.483099460601807,\n",
       "  -7.354067802429199,\n",
       "  -7.578904628753662,\n",
       "  -7.065323829650879,\n",
       "  -7.083621501922607,\n",
       "  -7.068502902984619,\n",
       "  -7.3155670166015625,\n",
       "  -6.979966163635254,\n",
       "  -7.485980033874512,\n",
       "  -7.803559303283691,\n",
       "  -6.970181941986084,\n",
       "  -7.311140537261963,\n",
       "  -7.496665954589844,\n",
       "  -7.137378692626953,\n",
       "  -7.397022247314453,\n",
       "  -6.920180320739746,\n",
       "  -7.151406288146973,\n",
       "  -7.333001613616943,\n",
       "  -7.082779884338379,\n",
       "  -7.103575706481934,\n",
       "  -7.402109146118164,\n",
       "  -7.366697311401367,\n",
       "  -7.5674147605896,\n",
       "  -7.37465238571167,\n",
       "  -7.2412590980529785,\n",
       "  -7.349380016326904,\n",
       "  -7.372189998626709,\n",
       "  -7.100532054901123,\n",
       "  -7.301393032073975,\n",
       "  -6.796290874481201,\n",
       "  -7.197281360626221,\n",
       "  -7.443607330322266,\n",
       "  -7.135289192199707,\n",
       "  -7.360068321228027,\n",
       "  -7.408964157104492,\n",
       "  -7.205573558807373,\n",
       "  -7.454974174499512,\n",
       "  -7.4174981117248535,\n",
       "  -6.831664562225342,\n",
       "  -7.212850570678711,\n",
       "  -7.612602710723877,\n",
       "  -7.174962043762207,\n",
       "  -7.59566593170166,\n",
       "  -6.9602508544921875,\n",
       "  -7.249380588531494,\n",
       "  -7.1571245193481445,\n",
       "  -7.680871486663818,\n",
       "  -7.406078338623047,\n",
       "  -7.4496660232543945,\n",
       "  -7.568469524383545,\n",
       "  -7.464822292327881,\n",
       "  -7.366678237915039,\n",
       "  -7.1298065185546875,\n",
       "  -7.0814127922058105,\n",
       "  -6.989192962646484,\n",
       "  -6.936014652252197,\n",
       "  -7.221454620361328,\n",
       "  -7.470003128051758,\n",
       "  -7.015712738037109,\n",
       "  -7.425386428833008,\n",
       "  -6.948650360107422,\n",
       "  -7.257612228393555,\n",
       "  -7.399301052093506,\n",
       "  -7.100870609283447,\n",
       "  -7.7551589012146,\n",
       "  -7.081037521362305,\n",
       "  -7.374449253082275,\n",
       "  -7.052831649780273,\n",
       "  -6.946985244750977,\n",
       "  -7.309845447540283,\n",
       "  -7.1254963874816895,\n",
       "  -7.15842866897583,\n",
       "  -7.287840366363525,\n",
       "  -7.300775527954102,\n",
       "  -7.73068904876709,\n",
       "  -7.545932769775391,\n",
       "  -7.181588649749756,\n",
       "  -7.052502632141113,\n",
       "  -7.48550271987915,\n",
       "  -7.430411338806152,\n",
       "  -7.465792179107666,\n",
       "  -7.189488887786865,\n",
       "  -6.763614177703857,\n",
       "  -7.426180839538574,\n",
       "  -7.320562839508057,\n",
       "  -7.39733362197876,\n",
       "  -7.269693374633789,\n",
       "  -7.260161876678467,\n",
       "  -6.9358954429626465,\n",
       "  -6.801784992218018,\n",
       "  -7.245985984802246,\n",
       "  -7.709222793579102,\n",
       "  -7.615157604217529,\n",
       "  -6.966521263122559,\n",
       "  -7.285010814666748,\n",
       "  -7.021390438079834,\n",
       "  -7.33533239364624,\n",
       "  -7.315413951873779,\n",
       "  -7.210580348968506,\n",
       "  -7.3428263664245605,\n",
       "  -7.486448764801025,\n",
       "  -7.254660129547119,\n",
       "  -7.342303276062012,\n",
       "  -7.157780647277832,\n",
       "  -7.1164751052856445,\n",
       "  -7.2452006340026855,\n",
       "  -7.257828712463379,\n",
       "  -7.471445083618164,\n",
       "  -7.239429950714111,\n",
       "  -7.119592189788818,\n",
       "  -7.118971824645996,\n",
       "  -7.469045639038086,\n",
       "  -7.308967590332031,\n",
       "  -6.828397750854492,\n",
       "  -7.02565336227417,\n",
       "  -7.252005100250244,\n",
       "  -7.619792461395264,\n",
       "  -7.559312343597412,\n",
       "  -6.964035511016846,\n",
       "  -7.083888053894043,\n",
       "  -6.752946853637695,\n",
       "  -7.418388843536377,\n",
       "  -7.019731521606445,\n",
       "  -7.199170112609863,\n",
       "  -6.820239543914795,\n",
       "  -7.178019046783447,\n",
       "  -7.340604305267334,\n",
       "  -7.303576946258545,\n",
       "  -7.548152923583984,\n",
       "  -7.071664810180664,\n",
       "  -7.44514799118042,\n",
       "  -7.458370208740234,\n",
       "  -7.188562393188477,\n",
       "  -7.53788948059082,\n",
       "  -7.566684246063232,\n",
       "  -7.3766608238220215,\n",
       "  -7.1463823318481445,\n",
       "  -7.06557559967041,\n",
       "  -7.444659233093262,\n",
       "  -7.033060550689697,\n",
       "  -7.227713584899902,\n",
       "  -7.412405014038086,\n",
       "  -7.47772741317749,\n",
       "  -7.091013431549072,\n",
       "  -6.925886631011963,\n",
       "  -7.334589004516602,\n",
       "  -7.4047417640686035,\n",
       "  -6.9877448081970215,\n",
       "  -7.523900985717773,\n",
       "  -7.091930866241455,\n",
       "  -7.321393966674805,\n",
       "  -7.304620265960693,\n",
       "  -7.090681076049805,\n",
       "  -7.326874256134033,\n",
       "  -7.1330695152282715,\n",
       "  -7.227787494659424,\n",
       "  -7.160442352294922,\n",
       "  -7.201498985290527,\n",
       "  -7.341880798339844,\n",
       "  -7.609852313995361,\n",
       "  -7.144032955169678,\n",
       "  -7.399837493896484,\n",
       "  -7.059479236602783,\n",
       "  -6.999563217163086,\n",
       "  -7.300602912902832,\n",
       "  -7.33209753036499,\n",
       "  -6.894585609436035,\n",
       "  -7.040382385253906,\n",
       "  -7.2053303718566895,\n",
       "  -7.528674602508545,\n",
       "  -7.153548717498779,\n",
       "  -7.547086238861084,\n",
       "  -7.333837985992432,\n",
       "  -7.07753849029541,\n",
       "  -6.990224361419678,\n",
       "  -6.996987819671631,\n",
       "  -6.795766830444336,\n",
       "  -7.488516807556152,\n",
       "  -7.541502475738525,\n",
       "  -7.106418132781982,\n",
       "  -7.390179634094238,\n",
       "  -7.192805290222168,\n",
       "  -7.205826759338379,\n",
       "  -7.126266002655029,\n",
       "  -7.095795154571533,\n",
       "  ...]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.LogSoftmax(dim=1)\n",
    "softmax(lin2(h_x)).detach().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-6.6515], grad_fn=<MaxBackward0>), tensor([291]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the index with highest softmax probabilities\n",
    "torch.max(softmax(lin2(h_x)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-train-cbow\"></a>\n",
    "\n",
    "# Now, we train the CBOW model for real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211, 24)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we split the data into training and testing.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenized_text_train, tokenized_text_test = train_test_split(tokenized_text, test_size=0.1, random_state=42)\n",
    "len(tokenized_text_train), len(tokenized_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [10:42<00:00,  6.54s/it]\n"
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.003\n",
    "hidden_size = 100\n",
    "window_size = 2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the dataset.\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "vocab_size = len(w2v_dataset.vocab)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, embd_size, window_size, hidden_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epoch_loss = []\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x, y = w2v_io\n",
    "            x = tensor(x).to(device)\n",
    "            y = autograd.Variable(tensor(y, dtype=torch.long)).to(device)\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = model(x)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(float(loss))\n",
    "    # Save model after every epoch.\n",
    "    torch.save(model.state_dict(), 'cbow_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArkAAAHSCAYAAADohdOwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAA2A4hFwAAALbz/wHok5DveiGpQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-evaluate-cbow\"></a>\n",
    "\n",
    "# Apply and Evaluate the CBOW Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(w2v_io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[91m______\u001b[0m essentially this\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mnot\u001b[0m this :\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mrandom\u001b[0m : if\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91mused\u001b[0m if a\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mof\u001b[0m a word\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[92ma\u001b[0m word (\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91mweb\u001b[0m ( or\n",
      "\u001b[92m(\u001b[0m \t\t a word \u001b[91mreason\u001b[0m or bigram\n",
      "\u001b[92mor\u001b[0m \t\t word ( \u001b[91mand\u001b[0m bigram ,\n",
      "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91mother\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mplan\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mnot\u001b[0m etc .\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[91mfor\u001b[0m a vast\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91mnot\u001b[0m vast re-\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mtheoretical\u001b[0m re- source\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mone\u001b[0m source for\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91mnot\u001b[0m for many\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91mprobability\u001b[0m is random\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mbetween\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mnot\u001b[0m , arbitrary\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91mnon-\u001b[0m arbitrary ,\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[92marbitrary\u001b[0m , motivated\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91m______\u001b[0m motivated or\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[92mmotivated\u001b[0m or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91mp\u001b[0m ( r\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91mand\u001b[0m a ,\n",
      "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mhere\u001b[0m , p\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mand\u001b[0m methods are\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91massociated\u001b[0m from just\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mso\u001b[0m just those\n",
      "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91ma\u001b[0m errors that\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[91mit\u001b[0m do not\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mare\u001b[0m not wish\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mis\u001b[0m wish to\n",
      "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mreject\u001b[0m any scf\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91mcomparing\u001b[0m which there\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mfiltering\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mtask\u001b[0m is any\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[91mnot\u001b[0m any evidence\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91mbe\u001b[0m evidence as\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91mhave\u001b[0m as a\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mthe\u001b[0m a true\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mis\u001b[0m true scf\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91mand\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mreveals\u001b[0m for the\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mgiven\u001b[0m the verb\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
      "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mhave\u001b[0m out to\n",
      "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mare\u001b[0m indistinguishable from\n",
      "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mhave\u001b[0m from one\n",
      "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91mthe\u001b[0m one where\n",
      "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mcontributing\u001b[0m where the\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mof\u001b[0m the individual\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mthey\u001b[0m individual words\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mcommon\u001b[0m words (\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mword\u001b[0m ( as\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[91m______\u001b[0m as opposed\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91mfor\u001b[0m the texts\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91msee\u001b[0m texts )\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mmarked\u001b[0m ) had\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mis\u001b[0m had been\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91mwords\u001b[0m randomly selected\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mlanguage\u001b[0m selected ,\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mpeople\u001b[0m , this\n",
      "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91mand\u001b[0m out not\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[91mcan\u001b[0m be the\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91min\u001b[0m the case\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mis\u001b[0m case .\n",
      "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mthe\u001b[0m carroll 1997\n",
      "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91macquisition\u001b[0m of subcategorization\n",
      "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mand\u001b[0m corpora .\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mbeing\u001b[0m tested using\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mpurchases\u001b[0m using the\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mframework\u001b[0m : is\n",
      "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91m______\u001b[0m ⫺ 0.5\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mχ2\u001b[0m greater than\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91m0.5\u001b[0m critical value\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91mtwo\u001b[0m value ?\n",
      "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mterms\u001b[0m of statistical\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91mrandom\u001b[0m language processing\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[92mlanguage\u001b[0m processing .\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mprobabilities\u001b[0m is low\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91m4\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91m______\u001b[0m we reject\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mor\u001b[0m reject h0\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mlook\u001b[0m h0 .\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
      "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mand\u001b[0m by an\n",
      "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mpopulation\u001b[0m , or\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91mthere\u001b[0m is enormous\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91mnot\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mthere\u001b[0m is wrong\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mnot\u001b[0m to identify\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mverb\u001b[0m distinction with\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mnon-technical\u001b[0m one .\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91mprobability\u001b[0m of the\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mrandom\u001b[0m often an\n",
      "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mon\u001b[0m way to\n",
      "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91m:\u001b[0m ; the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91m______\u001b[0m where the\n",
      "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mdata\u001b[0m is overlooked\n",
      "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[91mand\u001b[0m 16 (\n",
      "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m19\u001b[0m ( 1\n",
      "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
      "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[91m1995\u001b[0m ) ,\n",
      "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91mthat\u001b[0m non-random and\n",
      "\u001b[92mnon-random\u001b[0m \t language is \u001b[91m______\u001b[0m and hence\n",
      "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91menormous\u001b[0m hence ,\n",
      "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mrandomly\u001b[0m , when\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mhodges\u001b[0m when we\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[92mlook\u001b[0m at linguistic\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91m______\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[92mnot\u001b[0m always have\n",
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mwe\u001b[0m have enough\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[92mhave\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[92menough\u001b[0m data to\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[91m______\u001b[0m reject the\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91mto\u001b[0m but that\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mthe\u001b[0m that is\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mthere\u001b[0m is a\n",
      "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mrole\u001b[0m issue :\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mwhere\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[91mh0\u001b[0m is rejected\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91mwith\u001b[0m rejected .\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[92min\u001b[0m a text\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[91mgeneral\u001b[0m text are\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mhelp\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91mof\u001b[0m not random\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[92mnot\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[91mare\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91mbecause\u001b[0m we know\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91m1995\u001b[0m that our\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91min\u001b[0m our corpora\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91mlarge\u001b[0m corpora are\n",
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mwe\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91m______\u001b[0m not randomly\n",
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mbeen\u001b[0m randomly generated\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91mone\u001b[0m generated ,\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mconsiderable\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91mpurchases\u001b[0m and the\n",
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[92mand\u001b[0m the hypothesis\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[92mthe\u001b[0m hypothesis test\n",
      "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mdoes\u001b[0m the fact\n",
      "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mdifferent\u001b[0m in section\n",
      "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mcells\u001b[0m concern the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91mdifferences\u001b[0m between a\n",
      "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mroger\u001b[0m a linguistic\n",
      "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91msubset\u001b[0m of a\n",
      "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mmodel\u001b[0m the relation\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mx\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mlanguage\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mor\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mfor\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91mwe\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mwords\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[92m’\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91mdefinition\u001b[0m and its\n",
      "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91mfalse\u001b[0m , as\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91mwords\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mand\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mthe\u001b[0m arbitrary .\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[91msame\u001b[0m term ,\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[91mmodel\u001b[0m , language\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91mthis\u001b[0m language is\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mit\u001b[0m is never\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mthere\u001b[0m is then\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mby\u001b[0m the hypothesis\n",
      "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91mrandomness\u001b[0m , be\n",
      "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mwe\u001b[0m as :\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91mfor\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mhigh\u001b[0m error terms\n",
      "\u001b[92merror\u001b[0m \t\t are the \u001b[92merror\u001b[0m terms systematically\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mterm\u001b[0m systematically greater\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mfar\u001b[0m greater than\n",
      "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[92mgreater\u001b[0m than 0.5\n",
      "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
      "\u001b[92m1\u001b[0m \t\t with just \u001b[91m0.5\u001b[0m % of\n",
      "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91mway\u001b[0m of them\n",
      "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91mis\u001b[0m them ,\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91msalience\u001b[0m , devastate\n",
      "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mthe\u001b[0m one of\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[91mor\u001b[0m verbs for\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91massociation\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91mon\u001b[0m which we\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91mthe\u001b[0m we have\n",
      "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91mset\u001b[0m of data\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91mtrue\u001b[0m thresholding methods\n",
      "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mfor\u001b[0m distinguish associated\n",
      "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91m______\u001b[0m associated scfs\n",
      "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mthe\u001b[0m scfs from\n",
      "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91m1993\u001b[0m from noise\n",
      "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91mfollowing\u001b[0m noise .\n"
     ]
    }
   ],
   "source": [
    "from lazyme import color_str\n",
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io[0])\n",
    "        y = tensor(w2v_io[1])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2553191489361702\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-load-model\"></a>\n",
    "\n",
    "# Go back to the 10th Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CBOW(\n",
       "    (embeddings): Embedding(1303, 100)\n",
       "    (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=1303, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = CBOW(vocab_size, embd_size, window_size, hidden_size)\n",
    "model_10 = torch.nn.DataParallel(model_10)\n",
    "model_10.load_state_dict(torch.load('cbow_checkpoint_10.pt'))\n",
    "model_10.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[91m______\u001b[0m essentially this\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mnot\u001b[0m this :\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mrandom\u001b[0m : if\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91mfor\u001b[0m if a\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mof\u001b[0m a word\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[92ma\u001b[0m word (\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91m2\u001b[0m ( or\n",
      "\u001b[92m(\u001b[0m \t\t a word \u001b[92m(\u001b[0m or bigram\n",
      "\u001b[92mor\u001b[0m \t\t word ( \u001b[91m1\u001b[0m bigram ,\n",
      "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mby\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mwere\u001b[0m etc .\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[91mof\u001b[0m a vast\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91mnot\u001b[0m vast re-\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mlanguage\u001b[0m re- source\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mrandom\u001b[0m source for\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91mcorpus\u001b[0m for many\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91mprobability\u001b[0m is random\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mbetween\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mnot\u001b[0m , arbitrary\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91mlanguage\u001b[0m arbitrary ,\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[92marbitrary\u001b[0m , motivated\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91m______\u001b[0m motivated or\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[91mas\u001b[0m or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91mp\u001b[0m ( r\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91mand\u001b[0m a ,\n",
      "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mwhich\u001b[0m , p\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mand\u001b[0m methods are\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91m______\u001b[0m from just\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mas\u001b[0m just those\n",
      "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91ma\u001b[0m errors that\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[91ma\u001b[0m do not\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mare\u001b[0m not wish\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mhypothesis\u001b[0m wish to\n",
      "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mreject\u001b[0m any scf\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91m)\u001b[0m which there\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mword\u001b[0m is any\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91mbe\u001b[0m evidence as\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91mdefeated\u001b[0m as a\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mthe\u001b[0m a true\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mis\u001b[0m true scf\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91mand\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mb\u001b[0m for the\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91msupport\u001b[0m the verb\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
      "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mhave\u001b[0m out to\n",
      "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mare\u001b[0m indistinguishable from\n",
      "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mhave\u001b[0m from one\n",
      "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91mthe\u001b[0m one where\n",
      "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mis\u001b[0m where the\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mof\u001b[0m the individual\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mthey\u001b[0m individual words\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mcommon\u001b[0m words (\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mfrequency\u001b[0m ( as\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[91m______\u001b[0m as opposed\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91mfor\u001b[0m the texts\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91msee\u001b[0m texts )\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mtwo\u001b[0m ) had\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mis\u001b[0m had been\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91mmore\u001b[0m randomly selected\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mlanguage\u001b[0m selected ,\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mpeople\u001b[0m , this\n",
      "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91mand\u001b[0m out not\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[91mcan\u001b[0m be the\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91min\u001b[0m the case\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mis\u001b[0m case .\n",
      "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mthe\u001b[0m carroll 1997\n",
      "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91macquisition\u001b[0m of subcategorization\n",
      "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mand\u001b[0m corpora .\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mand\u001b[0m tested using\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mthen\u001b[0m using the\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mtwo\u001b[0m : is\n",
      "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91m______\u001b[0m ⫺ 0.5\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91m’\u001b[0m greater than\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91mas\u001b[0m critical value\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91mtwo\u001b[0m value ?\n",
      "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91meach\u001b[0m of statistical\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91mrandom\u001b[0m language processing\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mwith\u001b[0m processing .\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mrandom\u001b[0m is low\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91m4\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91m______\u001b[0m we reject\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[92mwe\u001b[0m reject h0\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
      "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91m(\u001b[0m by an\n",
      "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mpopulation\u001b[0m , or\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mthis\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91mas\u001b[0m is enormous\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91mnot\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91mthat\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mthere\u001b[0m is wrong\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mnot\u001b[0m to identify\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91msamples\u001b[0m distinction with\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mnon-technical\u001b[0m one .\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91mprobability\u001b[0m of the\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mrandom\u001b[0m often an\n",
      "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mto\u001b[0m way to\n",
      "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91m:\u001b[0m ; the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91m______\u001b[0m where the\n",
      "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mdata\u001b[0m is overlooked\n",
      "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[91mand\u001b[0m 16 (\n",
      "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m19\u001b[0m ( 1\n",
      "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
      "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[92m1\u001b[0m ) ,\n",
      "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91mthat\u001b[0m non-random and\n",
      "\u001b[92mnon-random\u001b[0m \t language is \u001b[91m______\u001b[0m and hence\n",
      "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91mby\u001b[0m hence ,\n",
      "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mshoe-polish\u001b[0m , when\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91ma\u001b[0m when we\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[91mcan\u001b[0m at linguistic\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91m______\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[91mthe\u001b[0m always have\n",
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mwe\u001b[0m have enough\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[92mhave\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[92menough\u001b[0m data to\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[91m______\u001b[0m reject the\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91mto\u001b[0m but that\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mthe\u001b[0m that is\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mthere\u001b[0m is a\n",
      "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mto\u001b[0m issue :\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mwhere\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[91mh0\u001b[0m is rejected\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91mwith\u001b[0m rejected .\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[92min\u001b[0m a text\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[92ma\u001b[0m text are\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mrandom\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91mof\u001b[0m not random\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[92mnot\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[91mare\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91m______\u001b[0m we know\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mfrequently\u001b[0m that our\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91min\u001b[0m our corpora\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91mtwo\u001b[0m corpora are\n",
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91m______\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91m______\u001b[0m not randomly\n",
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mbeen\u001b[0m randomly generated\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91m(\u001b[0m generated ,\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mleech\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91mlinguistics\u001b[0m and the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[92mand\u001b[0m the hypothesis\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[92mthe\u001b[0m hypothesis test\n",
      "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n",
      "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mof\u001b[0m the fact\n",
      "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mall\u001b[0m in section\n",
      "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mcorpora\u001b[0m concern the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91mis\u001b[0m between a\n",
      "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mfor\u001b[0m a linguistic\n",
      "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91msubset\u001b[0m of a\n",
      "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mmodel\u001b[0m the relation\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mx\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mlanguage\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mwas\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mfor\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91mwe\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mwhere\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[92m’\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91mdefinition\u001b[0m and its\n",
      "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91marbitrary\u001b[0m , as\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91mwords\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mand\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mthe\u001b[0m arbitrary .\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[91msame\u001b[0m term ,\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[92mterm\u001b[0m , language\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91mthis\u001b[0m language is\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mh0\u001b[0m is never\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mrandom\u001b[0m is then\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mby\u001b[0m the hypothesis\n",
      "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91mrandomness\u001b[0m , be\n",
      "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mwe\u001b[0m as :\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91mfor\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mhigh\u001b[0m error terms\n",
      "\u001b[92merror\u001b[0m \t\t are the \u001b[91msame\u001b[0m terms systematically\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mterm\u001b[0m systematically greater\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mfar\u001b[0m greater than\n",
      "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[92mgreater\u001b[0m than 0.5\n",
      "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
      "\u001b[92m1\u001b[0m \t\t with just \u001b[91mthe\u001b[0m % of\n",
      "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91mnumber\u001b[0m of them\n",
      "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91mis\u001b[0m them ,\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mrandom\u001b[0m , devastate\n",
      "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mthe\u001b[0m one of\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[92mthe\u001b[0m verbs for\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mjoint\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91mof\u001b[0m which we\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91mthe\u001b[0m we have\n",
      "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91mlanguage\u001b[0m of data\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91mincreases\u001b[0m thresholding methods\n",
      "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91ma\u001b[0m distinguish associated\n",
      "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91mfor\u001b[0m associated scfs\n",
      "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mthe\u001b[0m scfs from\n",
      "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91mby\u001b[0m from noise\n",
      "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91mas\u001b[0m noise .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io[0])\n",
    "        y = tensor(w2v_io[1])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(model_10(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2680851063829787\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [optional] How to Handle Unknown Words? \n",
    "\n",
    "This is not the best way to handle unknown words, but we can simply assign an index for unknown words.\n",
    "\n",
    "**Hint:** Ensure that you have `gensim` version 3.7.0 first. Otherwise this part of the code won't work. Try `python -m pip install -U pip` and then `python -m pip install -U gensim==3.7.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a', 1: 'bar', 2: 'foo', 3: 'is', 4: 'sentence', 5: 'this'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: 'a',\n",
       " 7: 'bar',\n",
       " 2: 'foo',\n",
       " 3: 'is',\n",
       " 4: 'sentence',\n",
       " 5: 'this',\n",
       " 0: '<pad>',\n",
       " 1: '<unk>'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.patch_with_special_tokens\n",
    "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
    "\n",
    "try:\n",
    "    special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "    vocab.patch_with_special_tokens(special_tokens)\n",
    "except: # If gensim is not 3.7.0\n",
    "    pass\n",
    "    \n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [optional] Lets Rewrite the `Word2VecText` Object\n",
    "\n",
    "Now with the (i) unknown word patch in the vocabulary as well as (ii) `skipgram_iterator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        \n",
    "        # Add the unknown word patch here.\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "        try:\n",
    "            special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "            self.vocab.patch_with_special_tokens(special_tokens)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = self.cbow_iterator\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = self.skipgram_iterator\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent, self.window_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens, unknown_word_index=1)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                if leftovers:\n",
    "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-5\"></a>\n",
    "\n",
    "# Lets try the skipgram task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_context = self.embeddings(context).view((1, -1))\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "        score = torch.mm(embed_focus, torch.t(embed_context))\n",
    "        log_probs = F.logsigmoid(score)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-5-foward\"></a>\n",
    "\n",
    "# Take a closer look at what's in the `forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx1 = torch.rand(1,20)\n",
    "xx2 = torch.rand(1,20)\n",
    "\n",
    "xx1_numpy = xx1.detach().numpy()\n",
    "xx2_numpy = xx2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "(20, 1)\n",
      "[[3.1778643]]\n"
     ]
    }
   ],
   "source": [
    "print(xx1_numpy.shape)\n",
    "print(xx2_numpy.T.shape)\n",
    "print(np.dot(xx1_numpy, xx2_numpy.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "torch.Size([20, 1])\n",
      "tensor([[3.1779]])\n"
     ]
    }
   ],
   "source": [
    "print(xx1.shape)\n",
    "print(torch.t(xx2).shape) \n",
    "\n",
    "print(torch.mm(xx1, torch.t(xx2))) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-5-train\"></a>\n",
    "\n",
    "# Train a Skipgram model (for real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-35cae10f640a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# This unsqueeze thing is really a feature/bug... -_-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mepcoh_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "embd_size = 100\n",
    "learning_rate = 0.03\n",
    "hidden_size = 300\n",
    "window_size = 3\n",
    "\n",
    "# Initialize the dataset.\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=3, variant='skipgram')\n",
    "vocab_size = len(w2v_dataset.vocab)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model = SkipGram(vocab_size, embd_size,).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epcoh_loss = 0\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x1, x2 = w2v_io['x']\n",
    "            x1, x2 = tensor(x1).to(device), tensor(x2).to(device)\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.float)).to(device)\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = model(x1, x2)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epcoh_loss += float(loss)\n",
    "    torch.save(model.state_dict(), 'skipgram_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(epcoh_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-5-evaluate\"></a>\n",
    "\n",
    "# Evaluate the model on the skipgram task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "skipgram_iterator() missing 1 required positional argument: 'window_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-687508993e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Extract all the CBOW contexts (X) and targets (Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mw2v_io\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Retrieve the inputs and outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: skipgram_iterator() missing 1 required positional argument: 'window_size'"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        model.zero_grad()\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x1, x2 = w2v_io['x']\n",
    "        x1, x2 = tensor(x1), tensor(x2)\n",
    "        y = w2v_io['y']\n",
    "        _, prediction =  torch.max(model(x1, x2), 1)    \n",
    "        true_positive += int(prediction) == int(y)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-a1fbc309b646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_positive\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Collobert and Weston SENNA Embeddings\n",
    "\n",
    "\n",
    "If you're on a Mac or Linux, you can use the `!` bang commands in the next cell to get the data.\n",
    "\n",
    "```\n",
    "!pip install kaggle\n",
    "!mkdir -p .kaggle\n",
    "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > .kaggle/kaggle.json\n",
    "!chmod 600 .kaggle/kaggle.json\n",
    "!kaggle datasets download -d alvations/vegetables-senna-embeddings --force -p ./\n",
    "```\n",
    "\n",
    "If you're on windows go to https://www.kaggle.com/alvations/vegetables-senna-embeddings and download the data files. \n",
    "\n",
    "What's most important are the \n",
    " - `.txt` file that contains the vocabulary list\n",
    " - `.npy` file that contains the binarized numpy array\n",
    " \n",
    "The rows of the numpy array corresponds to the vocabulary in the order from the `.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-vocab\"></a>\n",
    "\n",
    "\n",
    "## 3.1.6. Loading Pre-trained Embeddings\n",
    "\n",
    "Lets overwrite the `Word2VecText` object with the pretrained embeddings. \n",
    "\n",
    "Most important thing is the overwrite the `Dictionary` from `gensim` with the vocabulary of the pre-trained embeddings, as such:\n",
    "\n",
    "```python\n",
    "        # Loads the pretrained keys. \n",
    "        with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "            pretrained_keys = {line.strip():i for i, line in enumerate(fin)}\n",
    "        self.vocab = Dictionary({})\n",
    "        self.vocab.token2id = pretrained_keys\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        \n",
    "        # Loads the pretrained keys. \n",
    "        with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "            pretrained_keys = {line.strip():i for i, line in enumerate(fin)}\n",
    "        self.vocab = Dictionary({})\n",
    "        self.vocab.token2id = pretrained_keys\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens, unknown_word_index=-1)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                if leftovers:\n",
    "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-pretrained\"></a>\n",
    "\n",
    "## Override the embeddings layer with the pre-trained weights.\n",
    "\n",
    "In PyTorch, the weights of the `nn.Embedding` object can be easily overwritten with `from_pretrained` function, see https://pytorch.org/docs/stable/nn.html#embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, pretrained_npy):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_context = self.embeddings(context).view((1, -1))\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "        score = torch.mm(embed_focus, torch.t(embed_context))\n",
    "        log_probs = F.logsigmoid(score)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_text_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-7a6e8133caa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2VecText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skipgram'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpretrained_npy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'senna.wiki-reuters.lm2.50d.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkipGram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_npy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_text_train' is not defined"
     ]
    }
   ],
   "source": [
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='skipgram')\n",
    "pretrained_npy = torch.tensor(np.load('senna.wiki-reuters.lm2.50d.npy'))\n",
    "pretrained_model = SkipGram(pretrained_npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-eval-skipgram\"></a>\n",
    "## Test Pretrained Embeddings on the Skipgram Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_text_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-470936aca002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Iterate through the test sentences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Extract all the CBOW contexts (X) and targets (Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw2v_io\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_text_test' is not defined"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        pretrained_model.zero_grad()\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x1, x2 = w2v_io['x']\n",
    "        if -1 in (x1, x2): # Skip unknown words.\n",
    "            continue\n",
    "        x1, x2 = tensor(x1), tensor(x2)\n",
    "        y = w2v_io['y']\n",
    "        with torch.no_grad():\n",
    "            logprobs = pretrained_model(x1, x2)\n",
    "            _, prediction =  torch.max(logprobs, 1)    \n",
    "        true_positive += int(prediction) == int(y)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "    pretrained_keys = {line.strip():i for i, line in enumerate(fin)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-a1fbc309b646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_positive\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-eval-cbow\"></a>\n",
    "## Test Pretrained Embeddings on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_text_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-59ff07c52046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2VecText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cbow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpretrained_cbow_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_npy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_text_train' is not defined"
     ]
    }
   ],
   "source": [
    "window_size = 5\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "hidden_size = 300\n",
    "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_text_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-078498ff5ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Iterate through the test sentences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Extract all the CBOW contexts (X) and targets (Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw2v_io\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_text_test' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(pretrained_cbow_model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-a1fbc309b646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_positive\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-unfreeze-finetune\"></a>\n",
    "## Unfreeze the Embedddings and Tune it on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy, freeze=False)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_text_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-985992808650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2VecText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cbow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpretrained_cbow_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_npy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_text_train' is not defined"
     ]
    }
   ],
   "source": [
    "window_size = 2 \n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "hidden_size = 300\n",
    "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_cbow_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-4f84802592bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.003\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_cbow_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained_cbow_model' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.003\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(pretrained_cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(pretrained_cbow_model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epoch_loss = []\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x = tensor(w2v_io['x'])\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.long))\n",
    "            \n",
    "            if -1 in x or int(y) == -1:\n",
    "                continue\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = pretrained_cbow_model(x)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(float(loss))\n",
    "    # Save model after every epoch.\n",
    "    torch.save(model.state_dict(), 'cbow_finetuning_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-reval-cbow\"></a>\n",
    "\n",
    "## Re-Test Pretrained Embeddings on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(pretrained_cbow_model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-a1fbc309b646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_positive\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
