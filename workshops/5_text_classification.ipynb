{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> table {float:left} </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style> table {float:left} </style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch tqdm lazyme nltk gensim\n",
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues.\n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    # See https://stackoverflow.com/a/25736515/610569\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    # Use the toktok tokenizer that requires no dependencies.\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification \n",
    "\n",
    "Text Categorization (textcat) is a common task in NLP. As long as, we have labelled data and we want to assign a discrete label to every input data point, it's a classification problem. E.g. \n",
    "\n",
    "| Tasks | Possible Labels | \n",
    "|:-|:-|\n",
    "| Sentiment analysis | Positive, Negative, Neutral | \n",
    "| Tweetstorm detection | True, False |\n",
    "| Author profiling | Author1, Author2, ... | \n",
    "| Language Identification | EN, ZH, DE, JA, FR, ...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various datasets for sentiment classification, previously we looked at the movie reviews dataset in `nltk`. There's also this other popular IMDB movie reviews dataset from Stanford. Lets use that.\n",
    "\n",
    "Download the dataset from http://ai.stanford.edu/~amaas/data/sentiment/ and put it in the same directory as where you're running this jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Munge the data!\n",
    "\n",
    "As always we have to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12500it [00:32, 379.57it/s]\n",
      "12500it [00:34, 359.02it/s]\n",
      "12500it [00:35, 347.69it/s]\n",
      "12500it [00:33, 367.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from lazyme import find_files\n",
    "\n",
    "def tokenize_data(path_to_dir, file_ext):\n",
    "    for filename in tqdm(find_files(path_to_dir, file_ext)):\n",
    "        with open(filename) as fin:\n",
    "            yield word_tokenize(fin.read())\n",
    "        \n",
    "X_train_pos = list(tokenize_data('aclIMDB/train/pos/', '*.txt'))\n",
    "X_train_neg = list(tokenize_data('aclIMDB/train/neg/', '*.txt'))\n",
    "X_test_pos = list(tokenize_data('aclIMDB/test/pos/', '*.txt'))\n",
    "X_test_neg = list(tokenize_data('aclIMDB/test/neg/', '*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_pos + X_train_neg\n",
    "X_test = X_test_pos + X_test_neg\n",
    "\n",
    "y_train = ['pos'] * len(X_train_pos) + ['neg'] * len(X_train_neg)\n",
    "y_test = ['pos'] * len(X_test_pos) + ['neg'] * len(X_test_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create our IMDB PyTorch Dataset \n",
    "\n",
    "Although we have a binary class problem, we will demonstrate a multi-class solution issue that can be also used on binary classification. \n",
    "\n",
    "\n",
    "First trick is to convert the \"human\" labels to a one-hot encoding.\n",
    "\n",
    "For example, if we have \n",
    "\n",
    "| Text Index | Label |\n",
    "|:-|:-|\n",
    "|0 | pos|\n",
    "|1 |neg|\n",
    "|2 |pos|\n",
    "|3 |neu|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the \n",
    "\n",
    " - first position of the label vector to represent negative  \n",
    " - second to represent positive\n",
    " - third to represent neutral\n",
    "\n",
    "\n",
    "we should represent the labels as such:\n",
    "\n",
    "| Text Index | Label | One-hot |\n",
    "|:-|:-|:-|\n",
    "|0 | 1|[0, 1, 0]|\n",
    "|1 | 0|[1, 0, 0]|\n",
    "|2 | 1|[0, 1, 0]|\n",
    "|3 | 2|[0, 0, 1]|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the one-hot encoding:\n",
    "labels = [1, 0, 1, 2]\n",
    "torch.eye(max(labels)+1)[labels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'one_hot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-263aedd91a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# In PyTorch version 1.0.1, simply use this:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'one_hot'"
     ]
    }
   ],
   "source": [
    "# In PyTorch version 1.0.1, simply use this:\n",
    "labels = [1, 0, 1, 2]\n",
    "torch.one_hot([1, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.vocab = Dictionary(texts)\n",
    "        # Vectorize labels\n",
    "        label_set = {'neg':0, 'pos':1}\n",
    "        labels = [label_set[l] for l in labels]\n",
    "        self.labels = torch.tensor(labels).long()\n",
    "        # Keep track of how many data points.\n",
    "        self._len = len(texts)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        vectorized_sent = self.vectorize(self.texts[index])\n",
    "        return {'x':vectorized_sent, \n",
    "                'y':self.labels[index], \n",
    "                'x_len':len(vectorized_sent)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        # Lets just cast list of indices into torch tensors directly =)\n",
    "        return torch.tensor(self.vocab.doc2idx(tokens))\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = IMDBDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([ 4, 17, 33, 43, 27, 34, 38, 44, 42, 21, 17, 31, 35, 32, 37, 30, 24, 45,\n",
      "        26,  2,  6, 17, 33, 46,  7, 10, 28, 19, 25,  0,  8, 13, 28, 17, 39, 41,\n",
      "         2, 14,  9, 23, 28, 20, 18, 40,  2, 15, 24,  3, 16, 14, 12,  1,  5, 29,\n",
      "        22, 17, 36, 11,  2]), 'y': tensor(1), 'x_len': 59}\n"
     ]
    }
   ],
   "source": [
    "print(imdb_data[0]) # First data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch DataLoader\n",
    "\n",
    "The [`torch.utils.data.DataLoader` object](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)  will help us easily create batches from the `torch.utils.data.Dataset` so that we can do mini-batch SGD and fully utilize GPU/CPU computation during gradient optimization.\n",
    "\n",
    "The `DataLoader` requires the following function to be implemented in the `Dataset`:\n",
    "\n",
    " - `__getitem__`: Return the dictionary of inputs \n",
    " - `__len__`: Return the no. of indices that `__getitem__` can fetch\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = DataLoader(dataset=imdb_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[ 1072,    53, 20332,    53,  2573,     2,  5698, 12440,  3131,    43,\n",
      "           722,   149,    33,    35,   149,  2299,     2,   608,    28,  7202,\n",
      "            17,  2778,    46,   149,    33,   409,   372,   332,   270,   535,\n",
      "           267,   149, 25242,    35,   149,  2299,     2,    14,   949,   183,\n",
      "            35,   149,    33,   194,   149,  2689,    53,   990,     2,    14,\n",
      "          4449,   183,   194,   149,  1072,   366, 13299,    35,   149,   477,\n",
      "           447,  5192,   162,  4215,  3505, 32134,     2,    14,  4352,    28,\n",
      "          2867,    84, 83691,     2,  1815,    53,  6976,    45,    53,   149,\n",
      "           477,  1115,   155,  1236,   267,  2223,  2367,   152,   269,    17,\n",
      "          1245,  1560,   162,    17,  2906,  1284,     2,   237,   194,  3213,\n",
      "            43,   523,  2136,   341, 55701,    44,   194,  3098,    17,  1268,\n",
      "            35, 80433, 22509,  6791,   257,   149,  2136,   455,  1216,   341,\n",
      "           149,  5128,   139, 14451, 55701,   341,   149, 43205,  3476,     2,\n",
      "           234,   454,   332,   266,   407,    35,  3609,    53,  1582,   152,\n",
      "          8187,    45,    33,     2]]), 'y': tensor([[1., 0.]]), 'x_len': tensor([144])}\n"
     ]
    }
   ],
   "source": [
    "for data_dict in dataloader:\n",
    "    # Sort indices of data in batch by lengths.\n",
    "    sorted_indices = np.array(data_dict['x_len']).argsort()[::-1].tolist()\n",
    "    data_batch = {name:_tensor[sorted_indices]\n",
    "                  for name, _tensor in data_dict.items()}\n",
    "    print(data_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets try batch of size > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "dataloader = DataLoader(dataset=imdb_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for data_dict in dataloader:\n",
    "    # Sort indices of data in batch by lengths.\n",
    "    sorted_indices = np.array(data_dict['x_len']).argsort()[::-1].tolist()\n",
    "    data_batch = {name:_tensor[sorted_indices]\n",
    "                  for name, _tensor in data_dict.items()}\n",
    "    print(data_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gotcha! Everything should be a fixed-size tensor\n",
    "\n",
    "To use the `DataLoader` to generate batches, one thing that we need to keep consistent is the size of the tensors for our inputs and outputs. \n",
    "\n",
    "For the outputs (`y`), it shouldn't be much of a problem since they are already in fixed size one-hot encoding.\n",
    "\n",
    "It's the inputs (`x`), that has variable length and we need to somehow fix it. \n",
    "\n",
    "There are a couple of ways to accomplish the fixed-size inputs:\n",
    "\n",
    " - Set the size of `x` tensors to a certain size and cut-off extra words after that\n",
    " - Set the size of `x` tensors to the max length seen in the train data and pad the other data points with lower length with a special `<pad>` symbol. \n",
    " \n",
    " \n",
    "Lets do both:\n",
    "\n",
    " - Set a max size limit\n",
    " - For sentences that has length > max, we cut the rest of the sentence off\n",
    " - For sentences that has length < max, we pad till we reach the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "tensor([-1.2528,  0.1755, -0.4251,  0.9689,  1.3231, -0.1331,  0.5136,  2.1149,\n",
      "         1.2246, -0.7345])\n"
     ]
    }
   ],
   "source": [
    "# Here's a clean way to pad 1-Dimensional tensors in PyTorch\n",
    "a = torch.randn(10)\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15])\n",
      "tensor([-1.2528,  0.1755, -0.4251,  0.9689,  1.3231, -0.1331,  0.5136,  2.1149,\n",
      "         1.2246, -0.7345,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "max_len = 15\n",
    "pad_left = 0\n",
    "pad_right = max_len - len(a)\n",
    "b = F.pad(a, (pad_left, pad_right), 'constant')\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to rewrite the `IMDBDataset` to account for fixed-length `x` tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        \n",
    "        # Remember the `patch_with_special_tokens` from gensim?\n",
    "        # Now we can put it into good use.\n",
    "        special_tokens = {'<pad>': 0, '<unk>':1}\n",
    "        self.vocab = Dictionary(texts)\n",
    "        self.vocab.patch_with_special_tokens(special_tokens)\n",
    "        # Keep track of vocab size.\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Vectorize labels\n",
    "        label_set = {'neg':0, 'pos':1}\n",
    "        labels = [label_set[l] for l in labels]\n",
    "        # Keep track of num of labels.\n",
    "        self.num_labels = max(labels)+1\n",
    "        self.labels_onehot = torch.eye(self.num_labels)[labels].long()\n",
    "        self.labels = torch.tensor(labels).long()\n",
    "        \n",
    "        # Keep track of how many data points.\n",
    "        self._len = len(texts)\n",
    "        \n",
    "        # Find the longest text in the data.\n",
    "        self.max_len = max(len(txt) for txt in texts)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        vectorized_sent = self.vectorize(self.texts[index])\n",
    "        # To pad the sentence:\n",
    "        # Pad left = 0; Pad right = max_len - len of sent.\n",
    "        pad_dim = (0, self.max_len - len(vectorized_sent))\n",
    "        vectorized_sent = F.pad(vectorized_sent, pad_dim, 'constant')\n",
    "        return {'x':vectorized_sent, \n",
    "                'y':self.labels[index], \n",
    "                'x_len':len(vectorized_sent)}\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        # Lets just cast list of indices into torch tensors directly =)\n",
    "        return torch.tensor(self.vocab.doc2idx(tokens, unknown_word_index=1))\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = IMDBDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.vocab.token2id['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([ 4, 17, 33,  ...,  0,  0,  0]), 'y': tensor(1), 'x_len': 2818}"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[  165,  1771,   625,  ...,     0,     0,     0],\n",
      "        [  163,   156,  1202,  ...,     0,     0,     0],\n",
      "        [   14,   174,    28,  ...,     0,     0,     0],\n",
      "        [  667, 30633,    84,  ...,     0,     0,     0],\n",
      "        [  608,   777,  1197,  ...,     0,     0,     0]]), 'y': tensor([1, 1, 0, 1, 0]), 'x_len': tensor([2818, 2818, 2818, 2818, 2818])}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "dataloader = DataLoader(dataset=imdb_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for data_dict in dataloader:\n",
    "    # Sort indices of data in batch by lengths.\n",
    "    sorted_indices = np.array(data_dict['x_len']).argsort()[::-1].tolist()\n",
    "    data_batch = {name:_tensor[sorted_indices]\n",
    "                  for name, _tensor in data_dict.items()}\n",
    "    print(data_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model with Feed-Forward Net\n",
    "\n",
    "Now that we have everything about the data in place, we can make use of all the knowledge we've gained thus far:\n",
    "\n",
    " - **Multi-Layered Perceptron**, aka. **Feed-Forward Network** that we've learnt from the previous XOR examples\n",
    "   - *Linear* layers\n",
    "   - *Activation function*, which?\n",
    "   - *Criterion* which?\n",
    "   - *Optimizer*, Adam vs SGD\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNet(nn.Module):\n",
    "    def __init__(self, max_len, num_labels, vocab_size, embedding_size, hidden_dim):\n",
    "        super(FFNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                       embedding_dim=embedding_size, \n",
    "                                       padding_idx=0)\n",
    "        # The no. of inputs to the linear layer is the \n",
    "        # no. of tokens in each input * embedding_size\n",
    "        self.linear1 = nn.Linear(embedding_size*max_len, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_labels)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # We want to flatten the inputs so that we get the matrix of shape.\n",
    "        # batch_size x no. of tokens in each input * embedding_size\n",
    "        batch_size, max_len = inputs.shape\n",
    "        embedded = self.embeddings(inputs).view(batch_size, -1)\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        return F.sigmoid(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6863493323326111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "embedding_size = 100\n",
    "learning_rate = 0.003\n",
    "hidden_size = 100\n",
    "\n",
    "# Initialize the dataset.\n",
    "batch_size = 5\n",
    "imdb_data = IMDBDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset=imdb_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Hint: the CBOW model object you've created.\n",
    "model = FFNet(imdb_data.max_len, \n",
    "              imdb_data.num_labels, \n",
    "              imdb_data.vocab_size, \n",
    "              embedding_size=embedding_size, \n",
    "              hidden_dim=hidden_size)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#model = nn.DataParallel(model)\n",
    "\n",
    "losses = []\n",
    "num_epochs = 100\n",
    "for _e in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        x = batch['x'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        # Zero gradient.\n",
    "        optimizer.zero_grad()\n",
    "        # Feed forward.\n",
    "        predictions = model(x)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(float(loss))\n",
    "        break\n",
    "    print(sum(epoch_loss)/len(epoch_loss))\n",
    "    break\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5063, 0.5023, 0.5161, 0.5047, 0.5059], grad_fn=<MaxBackward0>),\n",
       " tensor([1, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(predictions, 1)  # Predictions of the last batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Based', 'on', 'an', 'actual', 'story', ',', 'John', 'Boorman', 'shows', 'the', 'struggle', 'of', 'an', 'American', 'doctor', ',', 'whose', 'husband', 'and', 'son', 'were', 'murdered', 'and', 'she', 'was', 'continually', 'plagued', 'with', 'her', 'loss', '.', 'A', 'holiday', 'to', 'Burma', 'with', 'her', 'sister', 'seemed', 'like', 'a', 'good', 'idea', 'to', 'get', 'away', 'from', 'it', 'all', ',', 'but', 'when', 'her', 'passport', 'was', 'stolen', 'in', 'Rangoon', ',', 'she', 'could', 'not', 'leave', 'the', 'country', 'with', 'her', 'sister', ',', 'and', 'was', 'forced', 'to', 'stay', 'back', 'until', 'she', 'could', 'get', 'I.D', '.', 'papers', 'from', 'the', 'American', 'embassy', '.', 'To', 'fill', 'in', 'a', 'day', 'before', 'she', 'could', 'fly', 'out', ',', 'she', 'took', 'a', 'trip', 'into', 'the', 'countryside', 'with', 'a', 'tour', 'guide', '.', '``', 'I', 'tried', 'finding', 'something', 'in', 'those', 'stone', 'statues', ',', 'but', 'nothing', 'stirred', 'in', 'me', '.', 'I', 'was', 'stone', 'myself', '.', \"''\", '<', 'br', '/', '>', '<', 'br', '/', '>', 'Suddenly', 'all', 'hell', 'broke', 'loose', 'and', 'she', 'was', 'caught', 'in', 'a', 'political', 'revolt', '.', 'Just', 'when', 'it', 'looked', 'like', 'she', 'had', 'escaped', 'and', 'safely', 'boarded', 'a', 'train', ',', 'she', 'saw', 'her', 'tour', 'guide', 'get', 'beaten', 'and', 'shot', '.', 'In', 'a', 'split', 'second', 'she', 'decided', 'to', 'jump', 'from', 'the', 'moving', 'train', 'and', 'try', 'to', 'rescue', 'him', ',', 'with', 'no', 'thought', 'of', 'herself', '.', 'Continually', 'her', 'life', 'was', 'in', 'danger', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Here', 'is', 'a', 'woman', 'who', 'demonstrated', 'spontaneous', ',', 'selfless', 'charity', ',', 'risking', 'her', 'life', 'to', 'save', 'another', '.', 'Patricia', 'Arquette', 'is', 'beautiful', ',', 'and', 'not', 'just', 'to', 'look', 'at', ';', 'she', 'has', 'a', 'beautiful', 'heart', '.', 'This', 'is', 'an', 'unforgettable', 'story', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', \"''\", 'We', 'are', 'taught', 'that', 'suffering', 'is', 'the', 'one', 'promise', 'that', 'life', 'always', 'keeps', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[0]) # First test review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor: tensor([[ 56, 132,  20,  ...,   0,   0,   0]])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "def vectorize_test_inputs(inputs):\n",
    "    # Process the input text in the same way as you did with the training data.\n",
    "    vectorized_sent = imdb_data.vectorize(inputs)\n",
    "    pad_dim = (0, imdb_data.max_len - len(vectorized_sent))\n",
    "    return F.pad(vectorized_sent, pad_dim, 'constant').unsqueeze(0)\n",
    "\n",
    "print('Input tensor:', vectorize_test_inputs(X_test[0]))\n",
    "label_set = {'neg':0, 'pos':1}\n",
    "print('Label:', label_set[y_test[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5093, 0.5479])\n",
      "tensor([0.4903, 0.5097])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Apply the model to the inputs.\n",
    "with torch.no_grad():\n",
    "    predictions = model(vectorize_test_inputs(X_test[0]))[0]\n",
    "    print(predictions)\n",
    "    print(F.softmax(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: Classifying Toxic Comments\n",
    "\n",
    "Lets apply what we learnt in a realistic task and **fight cyber-abuse with NLP**!\n",
    "\n",
    "From https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/\n",
    "\n",
    "> *The threat of abuse and harassment online means that many people stop <br>*\n",
    "> *expressing themselves and give up on seeking different opinions. <br>*\n",
    "> *Platforms struggle to effectively facilitate conversations, leading many <br>*\n",
    "> *communities to limit or completely shut down user comments.*\n",
    "\n",
    "\n",
    "The goal of the task is to build a model to detect different types of of toxicity:\n",
    "\n",
    " - toxic\n",
    " - severe toxic\n",
    " - threats\n",
    " - obscenity\n",
    " - insults\n",
    " - identity-based hate\n",
    " \n",
    "In this part, you'll be munging the data as how I would be doing it at work. \n",
    "\n",
    "Your task is to train a feed-forward network on the toxic comments given the skills we have accomplished thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digging into the data...\n",
    "\n",
    "If you're using linux/Mac you can use these bang commands in the notebook:\n",
    "\n",
    "```\n",
    "!pip3 install kaggle\n",
    "!mkdir -p /content/.kaggle/\n",
    "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > /content/.kaggle/kaggle.json\n",
    "!chmod 600 /content/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
    "!unzip /content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/*\n",
    "```\n",
    "\n",
    "Otherwise, download the data from https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/train.csv.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-416-3d89fd8583f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train.csv.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzipfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/train.csv.zip'"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = '/content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/'\n",
    "\n",
    "with ZipFile(data_dir+'train.csv.zip', 'r') as zipfin:\n",
    "    train_csv = zipfin.open('train.csv')\n",
    "    df_train = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-412-92be9593ab31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This is how the training data looks like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "# This is how the training data looks like.\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ZipFile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-413-87db96d114ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'test.csv.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzipfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtest_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ZipFile' is not defined"
     ]
    }
   ],
   "source": [
    "with ZipFile(data_dir+'test.csv.zip', 'r') as zipfin:\n",
    "    test_csv = zipfin.open('test.csv')\n",
    "    df_test = pd.read_csv(test_csv, index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-414-e9d65f7a0979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This is how the test data looks like w/o the labels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "# This is how the test data looks like w/o the labels.\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ZipFile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-415-6f6c4b53db9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'test_labels.csv.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzipfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_labels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf_test_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ZipFile' is not defined"
     ]
    }
   ],
   "source": [
    "with ZipFile(data_dir+'test_labels.csv.zip', 'r') as zipfin:\n",
    "    test_labels = zipfin.open('test_labels.csv')\n",
    "    df_test_labels = pd.read_csv(test_labels, index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets add them to the *df_test*\n",
    "df_test = df_test.join(df_test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
